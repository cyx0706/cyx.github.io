<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ctwo&#39;s Blog</title>
  <icon>http://cyx0706.github.io/icon.png</icon>
  
  <link href="http://cyx0706.github.io/atom.xml" rel="self"/>
  
  <link href="http://cyx0706.github.io/"/>
  <updated>2021-07-09T09:57:04.074Z</updated>
  <id>http://cyx0706.github.io/</id>
  
  <author>
    <name>Ctwo</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>中兴捧月竞赛</title>
    <link href="http://cyx0706.github.io/2021/07/07/zte-contest/"/>
    <id>http://cyx0706.github.io/2021/07/07/zte-contest/</id>
    <published>2021-07-07T09:43:47.000Z</published>
    <updated>2021-07-09T09:57:04.074Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第十一届中兴捧月大赛"><a href="#第十一届中兴捧月大赛" class="headerlink" title="第十一届中兴捧月大赛"></a>第十一届中兴捧月大赛</h1><p>菜菜子，比特派，美羊大学，江湖名号 Ctwo</p><p>不太可以具体透露题目，只能随便讲讲了。<del>不会有人是来看题解的吧</del><br><a id="more"></a></p><h2 id="初赛"><a href="#初赛" class="headerlink" title="初赛"></a>初赛</h2><p>那时候正想着有啥算法比赛可以参加的，刚好看见宣传，直接冲了。</p><p>比特派初赛是 ACM 赛制，时间有点久了，记不清具体的题目了，两场任意一场晋级就行，第一场题目简单，第二场我印象深，我 Python dp 做一道题，疯狂超时，结束了下来拿 C++ 写了一手，过了，我当场裂开。</p><ul><li>C++ 真就是王道呗</li></ul><h2 id="复赛"><a href="#复赛" class="headerlink" title="复赛"></a>复赛</h2><p>比特派是新的赛道，整个赛制还不太成型吧，别的赛道有打榜的，有测评姬评分评时的，复赛就是答辩，我们直接来两个新题。</p><p>第一题扑克，规则贼像同花顺，就差没写上去了，根据规则对输入的组比较大小。</p><ul><li>不难，一看就是一个考察设计的题目，好说，我那点为数不多的设计模式的知识有用武之地了！然后就憋了一下午啥也没写出来，左想不好，右想扩展性不好。最后顶不住了，就直接类抽象+规则反射加载来写了，感觉设计的还不错？新规则直接加就好了，牌和规则分离了。可惜的是本来还想着把花色大小比较之类的也抽出来，但没设计好，所以，<strong>解耦合了，但没有完全解开</strong>，菜死了。</li></ul><p>第二题 50x50x50x50 种情况下求最优解，我贪心试了比暴力还慢，人傻了，怎么办呢，剪枝吧，咋剪？我写了个奇奇怪怪的方法，调了个奇奇怪怪的参数，哎，好用！虽然方法不行，但写报告我可铆足了劲，各种数学公式，符号化描述，看起来有严谨的数学方法那味了，仔细一推敲啥都不是</p><p><img src="https://i.loli.net/2021/07/07/swcQiNqXFRn9teB.png" alt=""></p><p>答辩就讲讲思路，值的说的是我上午在想不会需要自我介绍吧，准备了一手刚好用上了。</p><h2 id="决赛"><a href="#决赛" class="headerlink" title="决赛"></a>决赛</h2><blockquote><p>吃，码，睡，快乐陪跑 3 天，坐标西安中兴和泰酒店</p></blockquote><p>很早就想参加一次这种全国的线下决赛了，老是馋 <strong>AiDai</strong> 的经历。</p><p>一去发现好多研究生大佬，我一个算法菜鸡瑟瑟发抖。认识了初赛两场全 AC 的大佬（忘加微信了，有幸拍到大佬肩膀和大佬共进餐）</p><p>决赛收获还是颇多的，虽然只拿到了一个 cherry 键盘（京东卖 398 元），但重在过程对吧，学到了就不亏。</p><h3 id="赛"><a href="#赛" class="headerlink" title="赛"></a>赛</h3><p>先说题目，因为没啥说的，依旧是复杂情况的优化问题，没有约束条件的时候很容易，但在约束条件下变成了 NP 问题，还是关于图的，并且数据有多组，奇奇怪怪的方法就不好使了，咋办，我情急之下想到了模拟退火，整个 2 天就围着它转了。</p><p>改改参数，换个初始化，加升温，动态调整概率，动态调整循环次数……</p><p>结果还可以，但比大佬们还是有差距的，海，最后一天听了思路，我可以自豪的说<strong>我当初设想过，我和第一的方法60%是一样的！</strong>，当然这 40% 就是人家比我强的关键了。</p><p>不同组的题目不一样，也听了别的组的第一名的报告，感觉自己还需要努力。</p><h3 id="吃"><a href="#吃" class="headerlink" title="吃"></a>吃</h3><blockquote><p>算法优化不好总不能难为自己，吃饭还是要好好吃的</p></blockquote><ul><li><p>吃夜宵：吃不完<br><img src="https://i.loli.net/2021/07/07/vWKEY4IrFxU3kHm.jpg" alt=""></p></li><li><p>吃自助：营养均衡</p></li></ul><p><img src="https://i.loli.net/2021/07/07/CSQ4MqwBW5VLaHD.jpg"  /></p><p><img src="https://i.loli.net/2021/07/07/aqoBCLtdXyuO4hs.jpg" alt=""></p><p>回学校后我大呼廉价自选的西兰花难吃，我同学都说我口味都变刁钻了。</p><ul><li>吃甜点：重拳出击</li></ul><p><img src="https://i.loli.net/2021/07/07/hGyAb9qVugZnxBT.jpg" alt=""></p><p>要不是不好意思，我能把那边的一盘都吃完。</p><h3 id="睡"><a href="#睡" class="headerlink" title="睡"></a>睡</h3><blockquote><p>11 点半就睡了，早上 6 点多醒，绝对把没写完的代码拖到明天，程序员听了都说养生。</p></blockquote><p>5 星级酒店，房间大，浴室大，每天晚上都能洗热水澡</p><p>房间的隔音效果很好，设施也全，中午也可睡大椅子，想想在学校艰苦的生活就不禁觉得，<strong>要有钱</strong>。</p><p>可惜没照照片，放张百度的：</p><p><img src="https://i.loli.net/2021/07/07/xelgyipUsoq24wM.png" alt=""></p><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>开赛的时候阴差阳错捞了个公仔，说不定就是把我运气用完调不出来参的罪魁祸首。</p><p><img src="https://i.loli.net/2021/07/07/3NOdnT2hWqtcMYi.jpg" alt=""></p><h3 id="归程"><a href="#归程" class="headerlink" title="归程"></a>归程</h3><blockquote><p>人在青岛，刚下飞机</p></blockquote><p>由于考试原因提前离开，真佩服自己心大到考试周来，然后信息论就考的。。一般。。没想象中的好，还是复习差点火候。</p><p>等飞机的时候还看着数据库呢，隔着过道坐的小情侣都笑了。</p><h2 id="鸣谢"><a href="#鸣谢" class="headerlink" title="鸣谢"></a>鸣谢</h2><ul><li>感谢复赛点醒我的好哥们戴老师，决赛和我一个房间的龚同学</li><li>感谢负责人文姐比赛的时候对我的照顾</li><li>感谢掌门人们耐心的听完我的方法</li><li>图源：<a href="https://www.nowcoder.com/activity/challenge2021/index">比赛官网</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;第十一届中兴捧月大赛&quot;&gt;&lt;a href=&quot;#第十一届中兴捧月大赛&quot; class=&quot;headerlink&quot; title=&quot;第十一届中兴捧月大赛&quot;&gt;&lt;/a&gt;第十一届中兴捧月大赛&lt;/h1&gt;&lt;p&gt;菜菜子，比特派，美羊大学，江湖名号 Ctwo&lt;/p&gt;
&lt;p&gt;不太可以具体透露题目，只能随便讲讲了。&lt;del&gt;不会有人是来看题解的吧&lt;/del&gt;&lt;br&gt;</summary>
    
    
    
    <category term="travel" scheme="http://cyx0706.github.io/categories/travel/"/>
    
    
    <category term="travel" scheme="http://cyx0706.github.io/tags/travel/"/>
    
  </entry>
  
  <entry>
    <title>我免费了，暂时的</title>
    <link href="http://cyx0706.github.io/2021/07/07/tempfree/"/>
    <id>http://cyx0706.github.io/2021/07/07/tempfree/</id>
    <published>2021-07-07T07:55:14.000Z</published>
    <updated>2021-07-09T11:21:47.143Z</updated>
    
    <content type="html"><![CDATA[<p><del>上篇写的太负能量了吧</del></p><p>夏令营的面经不想写，也没啥写的，我就没几个夏令营 =_=</p><p>看到一个很好的基于 icarus 的样式，试了一下，有点问题，有空解决了再换上吧。<br><a id="more"></a></p><h1 id="学期总结和暑假计划"><a href="#学期总结和暑假计划" class="headerlink" title="学期总结和暑假计划"></a>学期总结和暑假计划</h1><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>大三的最后一个学期，前半学期没啥紧张感，甚至打了好几个下午的星际，现在想想就离谱，早点准备点东西不好吗。</p><p>后半学期直接 907, 9 点起床，0 点多睡觉。一周干 7 天，先是处理完了校团委的项目，又是社团的一些烦心事，然后数字图像处理大作业，再是导师考核题目，最后期末前一周跑出去打中兴的比赛，感觉做的事情还不少，要是每个学期都这样就好了。</p><p>中兴的比赛会在 zte 那篇里面说，我大学第一次线下比赛，挺开心。数字图像处理大项目也是我在实验室的做的项目，花了很多的时间而没有一个好的效果，现在想想当时没有放弃就很好了，坚持着做到了最后面。成功虽然不让我满意，也不让我的面试官们满意，但我和享受这整个过程。</p><p>数字图像处理这门课我超级喜欢，老师也很好，是我这学期为数不多的认真学习的课程了。</p><p>数据库考的血炸（成绩说不定又要滑了），老惭愧了，毕竟是爱特程序部也是一届站长的。</p><h2 id="假期规划"><a href="#假期规划" class="headerlink" title="假期规划"></a>假期规划</h2><p>依稀记得寒假计划完成的不多，这次要继续努力了（能早返校我效率直接翻一倍！）</p><p>撇开夏令营来计划：</p><ul><li>翻新博客！</li><li>数字图像处理笔记整理成博客</li><li>重学线性代数，概率统计，找一个好的教程/课程，尽可能的站在计算机的角度去理解学习这两门</li><li>复习专业课知识，准备夏令营</li><li>英语！英语！英语！口语！口语！口语！背单词+磨耳朵</li><li>念念不忘的《一个64位操作系统的实现》，《程序员的自我修养》，有时间就再学学吧</li><li>C++，这个必须要点了，至少看完 primer plus 或者我那本很厚的书吧（当然是挑重要的看），有好的练手项目就做做</li><li>画画！锻炼！</li></ul><p>摸了，这个 todo-list 挺多的，具体到每一天要坚持列任务和计划，我最近总是列着列着就忘记了，还有用好 win 的日历，也可以加入计划和安排。</p><p>老规矩，假期结束来验收，有啥会在这里更新</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;del&gt;上篇写的太负能量了吧&lt;/del&gt;&lt;/p&gt;
&lt;p&gt;夏令营的面经不想写，也没啥写的，我就没几个夏令营 =_=&lt;/p&gt;
&lt;p&gt;看到一个很好的基于 icarus 的样式，试了一下，有点问题，有空解决了再换上吧。&lt;br&gt;</summary>
    
    
    
    
    <category term="essay" scheme="http://cyx0706.github.io/tags/essay/"/>
    
  </entry>
  
  <entry>
    <title>black</title>
    <link href="http://cyx0706.github.io/2021/07/05/black/"/>
    <id>http://cyx0706.github.io/2021/07/05/black/</id>
    <published>2021-07-05T14:26:31.000Z</published>
    <updated>2021-07-07T08:31:54.604Z</updated>
    
    <content type="html"><![CDATA[<p>将近半学期没怎么写博客了，最近夏令营频频心态爆炸，这两天刚考完试颓废过去了，感觉自己的心情也没有很好的调整上来。</p><a id="more"></a><p>又一学期结束了，似乎熬出头了？以后再也没有了考试</p><p>突然有一阵空虚感——我以后的课程还学啥呢，反正不是我想选的，又不在乎成绩了。</p><p>我为有一刹那这么想的自己感到羞愧，想想这一学期的工作，自己嘴边上挂着菜，成绩不好，没学上了，报名和准备夏令营的时候也没做到很好记录。</p><blockquote><p>人菜就需要在时间上磨平差距</p></blockquote><p>我信奉的就是这样简单粗暴的规则，大佬抬抬手就完成的，自己做不来那就多砸时间来完成。</p><p>那么在夏令营报名和准备上我和大佬的区别在于绩点和科研竞赛，那么我有做什么去尝试填吗。没有，报名草草完事，没有提前准备一些最基础的问题。</p><p>可是我在学期末这段时间忙于中兴捧月的比赛，数字图像处理的课程设计，导师考核这些事情，兼顾不过来也正常吧。</p><p>那为什么不早点完成了，前面做的那么悠闲结果事情堆积了，怪时间不够？</p><p>可是…可是…</p><p>自我的责备和批判并不能带给我什么实质性的东西，我缺少一个 “壮士一去不复返的” 精神，看着身边的保研边缘同学都拼了老命的学，考出了优秀的成绩，我不禁在想，为什么我做不到呢？</p><p>有时候总会觉得自己人格分裂了，另一半自己在拼命的否定，挑出各种缺点来鞭笞自己去努力改正。剩下的一半在拼命的鼓励自己，还有希望，还能再坚持，你是最棒的。</p><p>说实话，前一段时间海洋哥的事情，毕竟是我的同学，我也有了解，我和他一样，也是怀着和实力或者能力不相符的梦想，都试着努力让自己变得更加优秀。</p><p>谁让这是我自己做出的决定呢！</p><blockquote><p>明天不会更好，只会将一步步的错误积累下来</p></blockquote><p>你要时时刻刻保持优秀，保持努力，不然就会被抛下，我能做什么？咬牙坚持呗。</p><p>不管怎么说，我愿意在我相信的道路上前进。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;将近半学期没怎么写博客了，最近夏令营频频心态爆炸，这两天刚考完试颓废过去了，感觉自己的心情也没有很好的调整上来。&lt;/p&gt;</summary>
    
    
    
    
    <category term="essay" scheme="http://cyx0706.github.io/tags/essay/"/>
    
  </entry>
  
  <entry>
    <title>数字图像处理</title>
    <link href="http://cyx0706.github.io/2021/05/26/dip3/"/>
    <id>http://cyx0706.github.io/2021/05/26/dip3/</id>
    <published>2021-05-26T13:16:50.000Z</published>
    <updated>2021-07-09T11:20:41.219Z</updated>
    
    <content type="html"><![CDATA[<ul><li>2021/5/28: 增加了部分示例代码</li></ul><h1 id="小波变换"><a href="#小波变换" class="headerlink" title="小波变换"></a>小波变换</h1><blockquote><p>小波 (Wavelet) 是时间（空间）频率的局部化分析，它通过伸缩平移运算对信号(函数)逐步进行多尺度细化，最终达到高频处时间细分，低频处频率细分，能自动适应时频信号分析的要求，从而可聚焦到信号的任意细节。有人把小波变换称为“数学显微镜”。</p></blockquote><p>而对于图像处理相关领域，小波变换曾经相当的热门，一些经典的算法如 JPEG-2000 就是基于小波变换原理的。</p><p>要理解小波变换，有一大堆需要铺垫的东西……</p><a id="more"></a><h2 id="多分辨率金字塔（Image-Resolution-Pyramids）"><a href="#多分辨率金字塔（Image-Resolution-Pyramids）" class="headerlink" title="多分辨率金字塔（Image Resolution Pyramids）"></a>多分辨率金字塔（Image Resolution Pyramids）</h2><p>一般情况下，我们要处理的都是一张具有固定分辨率的图片，但有些时候，我们会对同一图像的不同分辨率的子图像进行处理，这些子图像往往是根据原图下采样得到的，如果我们将最大的图像放在底部，最小的放在顶部，就能得到一个金字塔的形状。图片金字塔可以用于图像融合，不过这不是我们在这里需要关注的。</p><div style="display: table-cell"><img src="https://i.loli.net/2021/05/25/aZ1STdIOJhlDk6t.png" alt="image resolution pyramids"  style="text-align:center"/></div><p>下采样的方法有很多，比如<strong>高斯金字塔</strong>：顶部图像中每个像素等于下一层图像中5个像素的高斯加权平均值。这样我们将一个 $M \times N$ 的图像变成了 $\frac{M}{2}\times\frac{N}{2}$ 大小的图像。</p><p>我们可以通过一个大图，通过高斯金字塔生成的方法得到多个小分辨率的图，但是，当我们通过金字塔顶层的图片来试图复原原图的时候，我们复原后的图会很“糊”，因为我们在下采样的时候丢失了图片的一些细节。</p><p>有没有一种变换可以不丢失细节，我们通过金字塔的顶层和一些其他的东西也可以复原出来原图像？</p><p>我们先在一维研究一下这个问题，如下图：</p><p>我们想要通过一组变换将 $x(n)$ 变换 $x’(n)$ 然后再通过一个反变换得到 $\hat{x}(n)$，使得$x(n)=\hat{x}(n)$ ：</p><div style="display: table-cell"><img src="https://i.loli.net/2021/05/25/RvYzmUDK4JgH63b.png" alt="image resolution pyramids"  style="text-align:center"/></div><h2 id="子带编码（Subband-Coding）"><a href="#子带编码（Subband-Coding）" class="headerlink" title="子带编码（Subband Coding）"></a>子带编码（Subband Coding）</h2><p>子带编码一种以信号频谱为依据的编码方法，将信号分解成不同频带分量来去除信号相关性（可以理解成正交分解），再将分量分别进行取样、量化、编码，从而得到一组互不相关的码字合并在一起后进行传输。</p><p>如果我们可以把 $x(n)$ 按照高频和低频分解，在根据一种方法重组，根据子带编码的理论，我们可以无失真的重建 $x(n)$。</p><p><img src="https://i.loli.net/2021/05/25/8MBcUIANXFnwjP7.png" alt="子带编码"></p><p>现在，我们只需要找到一组 $\{ h_0(n),  h_1(n), g_0(n), g_1(n)\}$ 就可以无失真重建了，我们可以将这一组的函数看做是一组一维的滤波器。</p><h3 id="上-下采样（Upsampling-Downsampling）"><a href="#上-下采样（Upsampling-Downsampling）" class="headerlink" title="上/下采样（Upsampling/Downsampling）"></a>上/下采样（Upsampling/Downsampling）</h3><p>我们首先对 $x(n)$ 做 Z 变换：</p><script type="math/tex; mode=display">X(Z) = \sum_{-\infty}^{\infty}x(n)z^{-n}</script><p>在时间域我们做因数为 2 的下采样可以等价到在 Z 域（复频域）的计算：</p><script type="math/tex; mode=display">x_{down}(n)=x(2n) \Leftrightarrow X_{down}(Z)=\frac{1}{2}\left[X\left(z^{1 / 2}\right)+X\left(-z^{1 / 2}\right)\right]</script><p>对应上采样的计算：</p><script type="math/tex; mode=display">x^{\text {up }}(n)=\left\{\begin{array}{cl}x(n / 2) & n=0,2,4, \ldots \\0 & \text { otherwise }\end{array} \Leftrightarrow X^{\text {up }}(Z)=X\left(z^{2}\right)\right.</script><hr><p>我们已经知道了如何上下采样了，现在只需要找到变换的函数了。根据数学公式的一堆推导（详细的参考课本，这里不再写了），我们可以得到关系式：</p><script type="math/tex; mode=display">\begin{array}{l}H_{0}(-z) G_{0}(z)+H_{1}(-z) G_{1}(z)=0 \\H_{0}(z) G_{0}(z)+H_{1}(z) G_{1}(z)=2\end{array}</script><p>满足上面的关系的 H 和 G 函数都可以。</p><p>对于一个图像，我们只需要在行和列上分别应用子带编码就可以得到图像的滤波器了：</p><p><img src="https://i.loli.net/2021/05/25/Bep1I7DqjzNKfSO.png" alt="2D separable filters" style="zoom:80%;" /></p><h2 id="哈尔变换（The-Haar-Transform）"><a href="#哈尔变换（The-Haar-Transform）" class="headerlink" title="哈尔变换（The Haar Transform）"></a>哈尔变换（The Haar Transform）</h2><p>哈尔变换基于<strong>哈尔函数</strong> $h_u(x)$，就是满足上面的一组函数，他们定义在0到1之间。</p><p>为了便于理解，我们先将哈尔变换近似为另一种基于矩阵的变换。函数变成了我们表示为矩阵相乘：$T=AFA^T$，其中 F 是 $N\times N$ 的图像, A 是 $N\times N$ 的变换矩阵，T 是变换的结果。而对于逆变换，有 $F=A^TTA$。</p><p>下面给出生成哈尔变换矩阵 A 的方法：</p><p>对于任意的一个整数 u，它可以被唯一分解为</p><script type="math/tex; mode=display">u = 2^p+q</script><p>p 是 u 中包含的最大的 2 幂次，而 q 是余数，定义哈尔基函数：</p><script type="math/tex; mode=display">h_u(x)=\left\{\begin{matrix} 1, & u=0和0\le x\lt1\\ 2^{\frac{p}{2}}, & u\gt0 和\frac{q}{2^p}\le x \lt \frac{(q+0.5)}{2^p}\\ -2^{\frac{p}{2}}, & u\lt0 和\frac{(q+0.5)}{2^p}\le x \lt \frac{(q+1)}{2^p} \\ 0,& 其他\end{matrix}\right.</script><p>哈尔矩阵的第 i 行包含了 $h_i(z), \quad z=0/N, 1/N, \cdots, (N-1)/N$</p><script type="math/tex; mode=display">\begin{eqnarray} H_N&=&\begin{bmatrix} h_0(\frac{0}{N}) & h_0(\frac{1}{N}) & \cdots & h_0(\frac{N-1}{N})\\ h_1(\frac{0}{N}) & h_1(\frac{1}{N}) & \cdots & h_1(\frac{N-1}{N})\\ \vdots & \vdots & \ddots  & \vdots \\ h_{N-1}(\frac{0}{N}) & h_{N-1}(\frac{1}{N}) & \cdots & h_{N-1}(\frac{N-1}{N}) \end{bmatrix} \\ \\ A_H &=& \frac{1}{\sqrt{N}}H_N\end{eqnarray}</script><p>如一个 4 阶的哈尔变换矩阵</p><script type="math/tex; mode=display">\begin{align}A_4 & = \frac{1}{2}\begin{bmatrix} 1 & 1 & 1 & 1\\ 1 & 1 & -1 & -1\\ \sqrt{2} & -\sqrt{2} & 0 & 0\\ 0 & 0 & \sqrt{2} & -\sqrt{2}\end{bmatrix}\end{align}</script><p>这样我们就构造好了变换：先按照上面的方法用对应的 $A_N$ 计算 T，然后下采样，再用对应 $A_{N/2}$ 计算 T……，要恢复的时候就用逆变换的公式得到 F，再上采样，再逆变换……</p><h2 id="基函数-amp-尺度函数"><a href="#基函数-amp-尺度函数" class="headerlink" title="基函数&amp;尺度函数"></a>基函数&amp;尺度函数</h2><p>我们理解了如何用矩阵去实现上面的子带编码的计算和恢复，但很明显，用矩阵的计算复杂度高，消耗时间，并且对于非正方形的图像需要补齐才能运算，带来了一定的误差。</p><p>尺度函数用于创建函数或图像的一些列逼近，每个逼近的分辨率与其最邻近逼近的分辨率相差 2 倍。并且使用称为<strong>小波</strong>的<strong>补函数</strong>对<strong>相邻逼近之间的差进行编码</strong>。简单来说就是通过尺度函数+小波函数来对频率域的图像进行近似。</p><p>这里开始就进入小波变换的内容了，<strong>离散小波变换（DWT）</strong>使用小波和一个尺度函数，将函数或者图像表示为小波和尺度函数的线性组合。</p><p>我们考虑由实的，平方可积的父尺度函数 $\varphi(x)$ 的所有整数平移和二元缩放组成的基函数集合 $\{\varphi_{j,k}(x)|j,k \in Z \}$，其中：</p><script type="math/tex; mode=display">\varphi_{j,k}(x)=2^{\frac{j}{2}} \varphi(2^jx-k)</script><p>整数 k 决定了平移的结果，尺度 j 决定了缩放（宽度和幅度），若固定 j，我们可以得到一个基函数的空间 $V_j$，增大 j 可以扩大这个空间，让更多的变换后的基函数加入到其中，也就能表示更多更小的细节。</p><h3 id="哈尔尺度函数"><a href="#哈尔尺度函数" class="headerlink" title="哈尔尺度函数"></a>哈尔尺度函数</h3><p>我们还以哈尔为例，考虑高度为 1，宽度为 1 的尺度函数。</p><script type="math/tex; mode=display">\varphi(x)=\left\{\begin{array}{ll}1, & 0 \leq x<1 \\0, & \text { otherwise }\end{array}\right.</script><p>由这个父函数生成的一系列尺度函数：</p><p><img src="https://i.loli.net/2021/05/27/PfpTvzOR72HwodF.png" alt="" style="zoom: 75%;" /></p><p>而一个略微复杂的函数 $f$（e 图），可以用这些同一个尺度空间的尺度函数线性表示。</p><script type="math/tex; mode=display">f(x) = 0.5\varphi_{1,0}(x)+\varphi_{1,1}(x)-0.25\varphi_{1,4}(x)</script><p>上面是在尺度空间 $V_1$ 中的表示，我们同样也可以用 $V_0$ 尺度空间来表示，只需要将上面的尺度空间 $V_1$ 的基函数变换到 $V_0$ 空间即可：</p><script type="math/tex; mode=display">\varphi_{0,k}(x)=\frac{1}{\sqrt{2}}\varphi_{1,2k}(x)+\frac{1}{\sqrt{2}}\varphi_{1,2k+1}(x)</script><p>回忆起 $\varphi_{j,k}(x)$ 的定义，根据上面我们发现的规律，$\varphi(x)$ 可以表示为自身 2 倍分辨率副本的线性组合。（取 j=1）</p><script type="math/tex; mode=display">\varphi_(x)=\sum_{k\in Z}h_{\varphi}(k)\sqrt{2}\varphi(2x-k)</script><p>上式被称为膨胀方程，我们将展开的系数 $\{ h_{\varphi}(k)|k=0,1,2,\cdots \}$ 称为<strong>尺度函数系数</strong>。</p><p>对于哈尔尺度函数，可以计算出他的尺度函数的系数 $\{\varphi(k)|k=0,1 \} = \{ \frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}} \}$：</p><script type="math/tex; mode=display">\begin{eqnarray}&\varphi(x) & = & \frac{1}{\sqrt{2}}\varphi_{1,0}(x)+\frac{1}{\sqrt{2}}\varphi_{1,1}(x) \\& & = & \frac{1}{\sqrt{2}}\left [ \sqrt{2}\varphi(2x) \right]+\frac{1}{\sqrt{2}}\left [ \sqrt{2}\varphi(2x+1) \right]\\\Rightarrow \quad& \varphi(x)&=& \varphi(2x)+\varphi(2x+1)\end{eqnarray}</script><h3 id="小波函数"><a href="#小波函数" class="headerlink" title="小波函数"></a>小波函数</h3><p>我们现在理论化前面的规律，对小波函数进行一般性的描述：</p><p>对于所有的 $j,k\in Z$，存在一个母函数$\psi_{j,k}(x)=2^{\frac{j}{2}} \psi(2^jx-k) $，若令 $W_{j_0}$ 表示由小波函数 $\{\psi_{j_0,k}|k\in Z \}$ 张成的函数空间，则：</p><script type="math/tex; mode=display">V_{j_0+1} = V_{j_0} \oplus W_{j_0}</script><p>$\oplus$ 表示函数空间集合的并集，并且作为$V_{j_0}$ 的基的尺度函数和作为 $W_{j_0}$ 的基的小波函数是正交的：</p><script type="math/tex; mode=display">\psi_{j_0,k}(x), \varphi_{j_0,l}(x)</script><p>尺度函数和小波空间的关系如下图：</p><p><img src="https://i.loli.net/2021/05/27/H1FxNgCw2jBJO7S.png" style="zoom: 50%;" /></p><p>小波函数（类似于前面提到的尺度函数）可以用平移且分辨率加倍后的尺度函数的加权和来表示，我们可以写出：</p><script type="math/tex; mode=display">\psi_{j_0,k}(x)=\sum_{k}h_{\psi}(k)\sqrt{2} \varphi_(2x-k)</script><p>其中 $h_{\psi}(k)$ 被称为小波系数。</p><p>可以证明：</p><script type="math/tex; mode=display">h_{\psi}(k)=(-1)^kh_{\varphi}(1-k)</script><p>对于一个不在函数空间内的一个函数 $f(x)$，可以把它写成小波函数和尺度函数的和 $f(x)=f_{\psi}(x)+f_{\varphi}(x)$</p><p>仍以哈尔小波函数为例，在前面我们计算出了哈尔尺度系数，可以求得对应的小波系数：</p><script type="math/tex; mode=display">\begin{eqnarray}h_{\psi}(0) & = & (-1)^0h_{\varphi}(1-0) =\frac{1}{\sqrt{2}}\\h_{\psi}(1) & = & (-1)^1h_{\varphi}(1-1) =-\frac{1}{\sqrt{2}}\end{eqnarray}</script><p>这两个系数对应 $A_H$ 矩阵的第二行，代入小波函数的表达式可得 $\psi(x)=\varphi(2x)-\varphi(2x-1)$，于是哈尔母小波函数是：</p><script type="math/tex; mode=display">\psi(x)=\left\{\begin{matrix} 1, & 0\le x \lt 0.5&\\ -1, & 0.5\le x \lt 1& \\ 0, & 其他&\end{matrix}\right.</script><h3 id="二维小波变换"><a href="#二维小波变换" class="headerlink" title="二维小波变换"></a>二维小波变换</h3><p>一维小波变换很容易扩展到二维，在二维情况下，需要 1 个尺度函数和 3 个二维小波$\psi^H(x,y), \psi^V(x,y), \psi^D(x,y)$，每个二维小波都是两个一维的积。排除产生了 1 维结果的积之后，剩下的 4 个积产生可分离的尺度函数：</p><script type="math/tex; mode=display">\varphi(x,y)=\varphi(x)\varphi(y)</script><p>和可分离的“方向敏感”小波：</p><script type="math/tex; mode=display">\begin{eqnarray}\psi^H(x,y) & = & \psi(x)\varphi(y)\\ \psi^V(x,y) & = & \varphi(x)\psi(y)\\ \psi^D(x,y) & = & \psi(x)\psi(y)\end{eqnarray}</script><p>V, D, H 分别表示小波度量图像中灰度延行（垂直边缘），对角线，列（水平边缘）的变化，他们分别表示行，对角线，列的高频信息，而 $\varphi(x,y)$ 就是图像的低频信息了。</p><p>课本上二维小波变换的示意图：<br><img src="https://i.loli.net/2021/05/27/Ywzngq5JQkTomyP.png" style="zoom: 67%;" /></p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>Python 的 <strong>PyWavelets</strong> 包提供了小波变换的库。</p><p>官方文档给出的示例代码，对单张图片进行小波变换：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pywt</span><br><span class="line"><span class="keyword">import</span> pywt.data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load image</span></span><br><span class="line">original = pywt.data.camera()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Wavelet transform of image, and plot approximation and details</span></span><br><span class="line">titles = [<span class="string">&#x27;Approximation&#x27;</span>, <span class="string">&#x27; Horizontal detail&#x27;</span>,</span><br><span class="line">          <span class="string">&#x27;Vertical detail&#x27;</span>, <span class="string">&#x27;Diagonal detail&#x27;</span>]</span><br><span class="line">coeffs2 = pywt.dwt2(original, <span class="string">&#x27;bior1.3&#x27;</span>)</span><br><span class="line">LL, (LH, HL, HH) = coeffs2</span><br><span class="line">fig = plt.figure(figsize=(<span class="number">12</span>, <span class="number">3</span>))</span><br><span class="line"><span class="keyword">for</span> i, a <span class="keyword">in</span> <span class="built_in">enumerate</span>([LL, LH, HL, HH]):</span><br><span class="line">    ax = fig.add_subplot(<span class="number">1</span>, <span class="number">4</span>, i + <span class="number">1</span>)</span><br><span class="line">    ax.imshow(a, interpolation=<span class="string">&quot;nearest&quot;</span>, cmap=plt.cm.gray)</span><br><span class="line">    ax.set_title(titles[i], fontsize=<span class="number">10</span>)</span><br><span class="line">    ax.set_xticks([])</span><br><span class="line">    ax.set_yticks([])</span><br><span class="line"></span><br><span class="line">fig.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>LL, LH, HL, HH 分别代表低频信息，水平细节，垂直细节和对角线细节，结果如图：</p><p><img src="https://pywavelets.readthedocs.io/en/latest/_images/camera_approx_detail.png" alt=""></p><h3 id="多阶小波"><a href="#多阶小波" class="headerlink" title="多阶小波"></a>多阶小波</h3><p>如果想要多阶的小波变换，就需要 <code>wavedec2</code> 函数，它的返回值是一个列表 <code>[cAn, (cHn, cVn, cDn), … (cH1, cV1, cD1)]</code> 第一个值是低频信息，第二个元组是从第 n 层到第 1 层 3 个维度的高频信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ......</span></span><br><span class="line">scales = pywt.wavedec2(img, <span class="string">&quot;haar&quot;</span>, level=<span class="number">4</span>)</span><br><span class="line">base = scales[<span class="number">0</span>]</span><br><span class="line">fourth_layer = scales[<span class="number">1</span>]</span><br><span class="line">third_layer = scales[<span class="number">2</span>]</span><br><span class="line">second_layer = scales[<span class="number">3</span>]</span><br><span class="line">first_layer = scales[<span class="number">4</span>]</span><br><span class="line"><span class="comment"># ......</span></span><br></pre></td></tr></table></figure><p>对于反变换，和傅里叶变换时使用的库的函数命名方式很相似，均是以 “ixxx” 开头的函数。如 <code>dwt2</code> 对应 <code>idwt2</code>， <code>wavedec2</code> 对应 <code>waverec2</code>，系数的形式要和使用 <code>wavedec2</code> 得到的结构一致。</p><h3 id="小波去噪"><a href="#小波去噪" class="headerlink" title="小波去噪"></a>小波去噪</h3><h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>学起来的时候还真是反复看了好多边，再加上一些资料，才感觉有一丝丝的理解了……，真的难，本人菜鸡，有问题还请指出。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li>数字图像处理第四版（冈萨雷斯）</li><li><a href="users.rowan.edu/~polikar/WTtutorial.html">The Wavelet Tutorial</a></li><li><a href="https://blog.csdn.net/qq_18343569/article/details/46912929">图像的多分辨率金字塔</a></li><li><a href="https://blog.csdn.net/hhaowang/article/details/102533040">OpenCV教程（25）图像金字塔</a></li><li><a href="https://wenku.baidu.com/view/41ddf4eab52acfc788ebc900.html">小波变换和多分辨率处理（百度文库）</a></li><li><a href="https://baike.baidu.com/item/%E5%B0%8F%E6%B3%A2%E5%88%86%E6%9E%90/1504577?fr=aladdin">小波分析（百度百科）</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;ul&gt;
&lt;li&gt;2021/5/28: 增加了部分示例代码&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;小波变换&quot;&gt;&lt;a href=&quot;#小波变换&quot; class=&quot;headerlink&quot; title=&quot;小波变换&quot;&gt;&lt;/a&gt;小波变换&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;小波 (Wavelet) 是时间（空间）频率的局部化分析，它通过伸缩平移运算对信号(函数)逐步进行多尺度细化，最终达到高频处时间细分，低频处频率细分，能自动适应时频信号分析的要求，从而可聚焦到信号的任意细节。有人把小波变换称为“数学显微镜”。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;而对于图像处理相关领域，小波变换曾经相当的热门，一些经典的算法如 JPEG-2000 就是基于小波变换原理的。&lt;/p&gt;
&lt;p&gt;要理解小波变换，有一大堆需要铺垫的东西……&lt;/p&gt;</summary>
    
    
    
    <category term="Digital Image Processing" scheme="http://cyx0706.github.io/categories/Digital-Image-Processing/"/>
    
    
    <category term="CV" scheme="http://cyx0706.github.io/tags/CV/"/>
    
    <category term="Digital Image Processing" scheme="http://cyx0706.github.io/tags/Digital-Image-Processing/"/>
    
  </entry>
  
  <entry>
    <title>Take Your Time</title>
    <link href="http://cyx0706.github.io/2021/05/25/busyday/"/>
    <id>http://cyx0706.github.io/2021/05/25/busyday/</id>
    <published>2021-05-24T16:57:00.000Z</published>
    <updated>2021-07-07T08:02:49.311Z</updated>
    
    <content type="html"><![CDATA[<p>最近期末考试的安排出来了，学期末要来了，一学期又要结束了，说实话这学期懈怠了好多。</p><p>夏令营在紧张的准备中，联系的导师大多不回复，一度觉得自己要没学上了，加上同学给的压力有点大，好不容易调平心态了，就接着写博客呗…..</p><p>icarus 最近更新了，加入了自定义不同页面的显示，再也不用之前改代码了，是时候翻新一下博客的样式了！图片标题的问题实在是太难看了，顺便如果可以还是直接在页面渲染出来 latex 公式好看，插入图片有点对不齐显得很丑！</p><p>更多的<del>废话</del>放在 more 中了。<br><a id="more"></a></p><ul><li><p>参加了中兴的中兴捧月比赛，很幸运冲进了复赛，苦战 5 天，尽人事了，决赛要有机会我必去陪跑+旅游，想起了我经常出去打比赛的好朋友 AiDai，我也有机会体验一波了。</p></li><li><p>这学期上了数字图像处理这门课，竟然只有5个人。</p><ul><li>问，人去哪里了？答，选微信小程序开发去了。</li><li>问，提前学点CV不香吗？答，分高事少不香吗？</li></ul></li></ul><p>前几天还和学长们聊这个话题，无奈之中还是无奈。大家都想保研，都想不努力就搞高分（我也想啊！可恶）。</p><ul><li>问，你学为啥要学计算机，你要读什么方向的？</li><li>答，收入高啊，热门啊，大家都来了我也来了，研究生目前方向还不清楚。</li><li>问，你为啥不选 xxx （一门挺重要的课）啊？</li><li>答，我这个模块分够了，我选这个还拉我分我为啥要选，xxx（某班第一）也没选。</li></ul><p>我承认自己考试比不过这些人，但我觉得，在考试之外，还有应该更加看重的东西，个人目标，理想，抱负，兴趣难道仅仅是钱多就可以打发的吗？</p><ul><li><p>数字图像处理挺难的（没人选的原因之一？），但一点点来花上时间还是可以弄明白的，数学是个好东西，可惜我会的太少了。</p></li><li><p>Coursera 上吴恩达老师的机器学习课程听着挺香的，力推，难度适中，还有练习题，趁着补了补机器学习的基础。</p></li><li><p>五一时看的漫画《妖精的尾巴》陆陆续续补完了。很向往工会伙伴们的友谊，故事也迎来了圆满的结局，找回来看王道少年漫的感觉，真好。</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近期末考试的安排出来了，学期末要来了，一学期又要结束了，说实话这学期懈怠了好多。&lt;/p&gt;
&lt;p&gt;夏令营在紧张的准备中，联系的导师大多不回复，一度觉得自己要没学上了，加上同学给的压力有点大，好不容易调平心态了，就接着写博客呗…..&lt;/p&gt;
&lt;p&gt;icarus 最近更新了，加入了自定义不同页面的显示，再也不用之前改代码了，是时候翻新一下博客的样式了！图片标题的问题实在是太难看了，顺便如果可以还是直接在页面渲染出来 latex 公式好看，插入图片有点对不齐显得很丑！&lt;/p&gt;
&lt;p&gt;更多的&lt;del&gt;废话&lt;/del&gt;放在 more 中了。&lt;br&gt;</summary>
    
    
    
    
    <category term="essay" scheme="http://cyx0706.github.io/tags/essay/"/>
    
  </entry>
  
  <entry>
    <title>数字图像处理</title>
    <link href="http://cyx0706.github.io/2021/05/08/dip2/"/>
    <id>http://cyx0706.github.io/2021/05/08/dip2/</id>
    <published>2021-05-08T14:01:16.000Z</published>
    <updated>2021-05-25T13:43:13.267Z</updated>
    
    <content type="html"><![CDATA[<h1 id="图像复原与重建"><a href="#图像复原与重建" class="headerlink" title="图像复原与重建"></a>图像复原与重建</h1><p>对于一些周期性噪声，空间滤波器的效果不会很好，而且会比较麻烦。在前面的一篇中，我们提到了，除了空间域，我们还可以在频率域进行滤波。这种变换的基本思想是：在傅里叶变换中，周期噪声在对应于周期干扰的频率处显示为集中突发的能量。方法是通过一个选择性滤波器来分离噪声。<br><a id="more"></a></p><p>带阻滤波器属于选择性滤波，包括后面两种带通滤波器和陷波滤波器，也都是选择性滤波器。它们区别于上一篇里面的滤波器的地方在于，它们只关注处理特定的频带或小频矩形区域。</p><ul><li>如果某个频带中的频率被滤除，则称该频带滤波器为<strong>带阻滤波器</strong></li><li>如果某个频带中的频率被通过，则称该频带滤波器为<strong>带通滤波器</strong></li><li>小频率矩形区域处理中的滤波器称为<strong>陷波滤波器</strong></li></ul><h2 id="带阻滤波器"><a href="#带阻滤波器" class="headerlink" title="带阻滤波器"></a>带阻滤波器</h2><h3 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h3><ul><li><p>理想带阻滤波器（IBRF）</p><script type="math/tex; mode=display">\begin{align*}H(u,v) = \left\{\begin{matrix}0,& C_0-\frac{W}{2} \le D(u,v) \le C_0+\frac{W}{2} \\1,& \quad others\end{matrix}\right.\end{align*}</script></li><li><p>高斯带阻滤波器（GBRF）</p><script type="math/tex; mode=display">H(u,v) = 1 - exp(-\left [ \frac{D^2(u,v)-C_0^2}{D(u,v)W} \right ]^2 )</script></li><li><p>巴特沃斯带阻滤波器（BBRF）</p></li></ul><script type="math/tex; mode=display">H(u,v) = \frac{1}{1+\left [ \frac{D(u,v)W}{D^2(u,v)-C_0^2} \right ]^{2n} }</script><p>其中 $C_0$ 是频带中心，$W$ 是带宽, $D(u,v)$ 是传递函数得到中心到频率矩形中点 $(u,v)$ 的距离。</p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_distance</span>(<span class="params">pa, pb</span>):</span></span><br><span class="line">    <span class="keyword">from</span> math <span class="keyword">import</span> sqrt</span><br><span class="line">    dis = sqrt((pa[<span class="number">0</span>] - pb[<span class="number">0</span>]) ** <span class="number">2</span> + (pa[<span class="number">1</span>] - pb[<span class="number">1</span>]) ** <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> dis</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Filter</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate_filter</span>(<span class="params">cls, d, shape, *args, **kwargs</span>):</span></span><br><span class="line">        <span class="comment"># 这里要反过来 shape 的两个维度</span></span><br><span class="line">        transfer_matrix = np.zeros((shape[<span class="number">0</span>], shape[<span class="number">1</span>]))</span><br><span class="line">        center_point = <span class="built_in">tuple</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x - <span class="number">1</span>) // <span class="number">2</span>, shape))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(transfer_matrix.shape[<span class="number">0</span>]):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(transfer_matrix.shape[<span class="number">1</span>]):</span><br><span class="line">                dist = cal_distance(center_point, (i, j))</span><br><span class="line">                transfer_matrix[i, j] = cls.get_one(d, dist, *args, **kwargs)</span><br><span class="line">        <span class="keyword">return</span> transfer_matrix</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_one</span>(<span class="params">cls, d, dist, *args, **kwargs</span>) -&gt; <span class="built_in">float</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BBRFFilter</span>(<span class="params">Filter</span>):</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_one</span>(<span class="params">cls, d, dist, *args, **kwargs</span>) -&gt; <span class="built_in">float</span>:</span></span><br><span class="line">        w = kwargs[<span class="string">&quot;w&quot;</span>]</span><br><span class="line">        n = kwargs[<span class="string">&quot;n&quot;</span>]</span><br><span class="line">        <span class="keyword">if</span> dist == d:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + ((dist * w) / (dist ** <span class="number">2</span> - d ** <span class="number">2</span>)) ** (<span class="number">2</span> * n))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GBRFFilter</span>(<span class="params">Filter</span>):</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_one</span>(<span class="params">cls, d, dist, *args, **kwargs</span>) -&gt; <span class="built_in">float</span>:</span></span><br><span class="line">        w = kwargs[<span class="string">&quot;w&quot;</span>]</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>-np.exp(-((dist ** <span class="number">2</span> - d ** <span class="number">2</span>) / (d * w)) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IBRFFilter</span>(<span class="params">Filter</span>):</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_one</span>(<span class="params">cls, d, dist, *args, **kwargs</span>) -&gt; <span class="built_in">float</span>:</span></span><br><span class="line">        w = kwargs[<span class="string">&quot;w&quot;</span>]</span><br><span class="line">        <span class="keyword">if</span> d - w / <span class="number">2</span> &lt;= dist &lt;= d + w / <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">abs_threshold_filter</span>(<span class="params">matrix</span>):</span></span><br><span class="line">    matrix = np.<span class="built_in">abs</span>(matrix)</span><br><span class="line">    <span class="keyword">return</span> matrix</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">choice = <span class="string">&quot;material/windmill_noise.png&quot;</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(choice).convert(<span class="string">&quot;L&quot;</span>)</span><br><span class="line">f = np.fft.fft2(img)</span><br><span class="line">f_shift = np.fft.fftshift(f)</span><br><span class="line">choices = &#123;</span><br><span class="line">    <span class="string">&quot;巴特沃斯带阻&quot;</span>: BBRFFilter.generate_filter(<span class="number">15</span>, (img.size[<span class="number">1</span>], img.size[<span class="number">0</span>]), n=<span class="number">3</span>, w=<span class="number">5</span>),</span><br><span class="line">    <span class="string">&quot;高斯带阻&quot;</span>: GBRFFilter.generate_filter(<span class="number">15</span>, (img.size[<span class="number">1</span>], img.size[<span class="number">0</span>]), w=<span class="number">5</span>),</span><br><span class="line">    <span class="string">&quot;理想带阻&quot;</span>: IBRFFilter.generate_filter(<span class="number">15</span>, (img.size[<span class="number">1</span>], img.size[<span class="number">0</span>]), w=<span class="number">5</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> choices.items():</span><br><span class="line">    filter_matrix = v</span><br><span class="line">    transform_img = np.<span class="built_in">abs</span>(np.fft.ifft2(np.fft.ifftshift(f_shift*filter_matrix)))</span><br><span class="line"></span><br><span class="line">    plt.imshow(transform_img, cmap=<span class="string">&quot;gray&quot;</span>)</span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    plt.savefig(<span class="string">&quot;&#123;&#125;.png&quot;</span>.<span class="built_in">format</span>(k))</span><br><span class="line">    plt.clf()</span><br></pre></td></tr></table></figure><p>下面是代码的运行结果，左边的是滤波后的图片，右边的是原图片（带有周期性噪声）。整体来看，高斯滤波效果看起来最差，周期性的黑白条纹没有完全消除。巴特沃斯方法得到的效果看起来更好，基本消除了黑白条纹噪声。</p><ul><li>巴特沃斯带阻滤波</li></ul><div align=center><img src="https://i.loli.net/2021/05/09/7bZqwuHT4CoEUyz.png"></div><ul><li>理想带阻滤波</li></ul><div align=center><img src="https://i.loli.net/2021/05/09/aNOBD85G2doTby9.png"></div><ul><li>高斯带阻滤波</li></ul><div align=center><img src="https://i.loli.net/2021/05/09/8OuvqkD9ENJMhIl.png"></div><h2 id="带通滤波器"><a href="#带通滤波器" class="headerlink" title="带通滤波器"></a>带通滤波器</h2><h2 id="陷波滤波器"><a href="#陷波滤波器" class="headerlink" title="陷波滤波器"></a>陷波滤波器</h2><h2 id="自适应中值法"><a href="#自适应中值法" class="headerlink" title="自适应中值法"></a>自适应中值法</h2>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;图像复原与重建&quot;&gt;&lt;a href=&quot;#图像复原与重建&quot; class=&quot;headerlink&quot; title=&quot;图像复原与重建&quot;&gt;&lt;/a&gt;图像复原与重建&lt;/h1&gt;&lt;p&gt;对于一些周期性噪声，空间滤波器的效果不会很好，而且会比较麻烦。在前面的一篇中，我们提到了，除了空间域，我们还可以在频率域进行滤波。这种变换的基本思想是：在傅里叶变换中，周期噪声在对应于周期干扰的频率处显示为集中突发的能量。方法是通过一个选择性滤波器来分离噪声。&lt;br&gt;</summary>
    
    
    
    <category term="Digital Image Processing" scheme="http://cyx0706.github.io/categories/Digital-Image-Processing/"/>
    
    
    <category term="CV" scheme="http://cyx0706.github.io/tags/CV/"/>
    
    <category term="Digital Image Processing" scheme="http://cyx0706.github.io/tags/Digital-Image-Processing/"/>
    
  </entry>
  
  <entry>
    <title>数字图像处理</title>
    <link href="http://cyx0706.github.io/2021/04/23/dip1/"/>
    <id>http://cyx0706.github.io/2021/04/23/dip1/</id>
    <published>2021-04-23T01:43:40.000Z</published>
    <updated>2021-04-24T04:47:09.399Z</updated>
    
    <content type="html"><![CDATA[<p>2021/4/24: 解决(?)公式渲染问题，卸了重装了渲染器，然后参考配置了<a href="https://github.com/MakerGYT/markdown-it-latex2img">服务器渲染</a></p><h1 id="空间-amp-频率域滤波"><a href="#空间-amp-频率域滤波" class="headerlink" title="空间&amp;频率域滤波"></a>空间&amp;频率域滤波</h1><p>用于记录学习数字图像处理的历程（不完全同步学校课程，不定期摸鱼）</p><h2 id="空间域滤波"><a href="#空间域滤波" class="headerlink" title="空间域滤波"></a>空间域滤波</h2><h3 id="直方图均衡化"><a href="#直方图均衡化" class="headerlink" title="直方图均衡化"></a>直方图均衡化</h3><p>直方图的形状往往与图像的外观有关。</p><ul><li>在暗图像的直方图中，大多数直方图容器集中在灰度级较低的一端（靠近 0）</li><li>亮的图像的直方图中，多数直方图容器集中在灰度级的高端</li><li>低对比度的图像的直方图中，直方图容器基本位于灰度级的中间</li><li>高对比度图像的直方图，容器覆盖了较宽的范围，并且像素的分布是基本均匀的<a id="more"></a>直方图均衡化的数学原理我们暂且不提，假如我们把直方图每个部分出现频率和总统计量做除法，得到一个概率分布函数$p_r(r)$，均衡化变换就是将一个概率分布不规则，不均匀的$p_r(r)$变化为分布均匀的新的函数$p_s(s)$。</li></ul><p>Python 代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分不同的空间来绘图</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&quot;./IMG_2546.JPG&quot;</span>).convert(<span class="string">&quot;L&quot;</span>)  <span class="comment"># 转为灰度图</span></span><br><span class="line">img_array = np.array(img)</span><br><span class="line">x_num, y_num = img.size</span><br><span class="line"><span class="comment"># 查看直方图</span></span><br><span class="line">plt.subplot(<span class="number">121</span>)</span><br><span class="line">plt.xticks([])</span><br><span class="line">plt.yticks([])</span><br><span class="line">plt.hist(img_array.flatten(), <span class="number">256</span>)  <span class="comment"># 变换成 1 维再绘制直方图</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用直方图均衡再查看直方图</span></span><br><span class="line"><span class="comment"># 每个 bins 数组的区间值对应一个 im_hist 数组中的强度值</span></span><br><span class="line">im_hist, bins = np.histogram(img_array.flatten(), <span class="number">256</span>, density=<span class="literal">True</span>)  <span class="comment"># 转化为直方图</span></span><br><span class="line">cdf = im_hist.cumsum()  <span class="comment"># 计算累计分布函数 cumulative distribution function</span></span><br><span class="line"><span class="comment"># cdf(a)=P(x&lt;=a)</span></span><br><span class="line">cdf = <span class="number">255</span> * (cdf / cdf[-<span class="number">1</span>])  <span class="comment"># 归一化到 0-1 之后转化到 0-255</span></span><br><span class="line"><span class="comment"># interp(x, xp, yp)输入原函数的一系列点(xp, yp)使用线性插值方法模拟函数并计算 f(x)</span></span><br><span class="line"><span class="comment"># s = cdf(r)</span></span><br><span class="line">img_array_transformed = np.interp(img_array.flatten(), bins[:<span class="number">256</span>], cdf)</span><br><span class="line"><span class="comment"># 变换回原来的数组结构后转为图片</span></span><br><span class="line">plt.subplot(<span class="number">122</span>)</span><br><span class="line">plt.hist(img_array_transformed, <span class="number">256</span>)</span><br><span class="line">plt.xticks([])</span><br><span class="line">plt.yticks([])</span><br><span class="line">plt.savefig(<span class="string">&quot;exp.png&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br>原直方图和均衡化后的图片的直方图：</p><p><img src="https://i.loli.net/2021/04/15/v5HmMwd6icXlhzS.png" alt="直方图均衡化.png"></p><p>原图和均衡化后的图的对比：</p><p><img src="https://i.loli.net/2021/04/15/A18ByaERU6ZMH5v.png" alt="原图.png"></p><p><img src="https://i.loli.net/2021/04/15/7lEKRXDQhBZ15SW.png" alt="均衡化后.png"></p><p>如果我们对局部直方图均衡化，还可以显示出全局直方图均衡化无法显示的灰度细节。</p><h3 id="低通空间滤波器"><a href="#低通空间滤波器" class="headerlink" title="低通空间滤波器"></a>低通空间滤波器</h3><ul><li>高斯核</li></ul><p><img src="https://i.loli.net/2021/04/15/lAeMogtphx1cRS8.png" alt="3x3高斯核"></p><p>有现成的库可以调用：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image, ImageFilter</span><br><span class="line">img = img.<span class="built_in">filter</span>(ImageFilter.GaussianBlur(radius=<span class="number">2</span>))</span><br></pre></td></tr></table></figure></p><h3 id="高通空间滤波器"><a href="#高通空间滤波器" class="headerlink" title="高通空间滤波器"></a>高通空间滤波器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> signal</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sharpening_f</span>(<span class="params">img_array, kernel, c</span>):</span></span><br><span class="line">    <span class="comment"># 卷积锐化图像</span></span><br><span class="line"></span><br><span class="line">    new_array = signal.convolve2d(img_array, kernel,</span><br><span class="line">                                  boundary=<span class="string">&#x27;symm&#x27;</span>,</span><br><span class="line">                                  mode=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">    new_array[new_array &gt; <span class="number">255</span>] = <span class="number">255</span></span><br><span class="line">    new_array[new_array &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    new_array = img_array + c*new_array</span><br><span class="line">    new_array[new_array &gt; <span class="number">255</span>] = <span class="number">255</span></span><br><span class="line">    new_array[new_array &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> new_array</span><br></pre></td></tr></table></figure><ul><li>Sobel</li></ul><p>横方向的 Sobel 算子，用于提取出水平方向的边界：</p><script type="math/tex; mode=display">\begin{bmatrix} -1 & 0 & 1\\ 2 & 0 & 2\\ -1 & 0 & 1\end{bmatrix}</script><ul><li>拉普拉斯</li></ul><p>拉普拉斯是导数算子，因此会突出图像中急剧的灰度过渡，并不强调缓慢变化的灰度区域，这会使得原图像产生灰色边缘线和其他不连续的特征，因此将原图像与拉普拉斯变换后的图像相加就能够恢复背景特征，并且保留拉普拉斯锐化的效果。</p><p>一种拉普拉斯核如下：</p><script type="math/tex; mode=display">\begin{bmatrix} 1 & 1 & 1\\ 1 & -8 & 1\\ 1 & 1 & 1\end{bmatrix}</script><p>又或者是：</p><script type="math/tex; mode=display">\begin{bmatrix} 0 & 1 & 0\\ 1 & -4 & 1\\ 0 & 1 & 0\end{bmatrix}</script><ul><li>钝化掩蔽</li></ul><p>分为三个步骤：</p><ol><li>模糊原图像</li><li>从原图中减去模糊后的图像（产生的差称为模板）</li><li>将模板与原图像相加</li></ol><p>令 $\bar{f}(x, y)$ 表示模糊后的图像，钝化掩蔽的过程可以用公式表示为：</p><script type="math/tex; mode=display">\left\{\begin{matrix}g_{mask}(x, y) = f(x, y) - \bar{f}(x, y)  \\ g(x, y) = f(x, y)+kg_{mask}(x, y)\end{matrix}\right.</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image, ImageFilter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">image = Image.<span class="built_in">open</span>(<span class="string">&quot;./IMG_2546.JPG&quot;</span>).convert(<span class="string">&quot;L&quot;</span>)</span><br><span class="line">origin_array = np.array(image)</span><br><span class="line">blurred_image = image.<span class="built_in">filter</span>(ImageFilter.BLUR)</span><br><span class="line">blurred_array = np.array(blurred_image)</span><br><span class="line">mask_array = origin_array - blurred_array</span><br><span class="line"></span><br><span class="line">weight_k = <span class="number">2</span></span><br><span class="line">new_array = origin_array + weight_k * mask_array</span><br><span class="line">fig, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">ax1.hist(origin_array.flatten(), <span class="number">256</span>)</span><br><span class="line">ax1.set_title(<span class="string">&quot;Origin&quot;</span>)</span><br><span class="line">ax2.hist(new_array.flatten(), <span class="number">256</span>)</span><br><span class="line">ax2.set_title(<span class="string">&quot;shielding Sharpened&quot;</span>)</span><br><span class="line">plt.savefig(<span class="string">&quot;exp6_sharping2.png&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>原图像：<br><img src="https://i.loli.net/2021/04/24/fOHkQztd3SYwa8c.png" alt="原图"></p><p>增强后的图像：<br><img src="https://i.loli.net/2021/04/24/pXlqekVEhW4NJro.png" alt="钝化掩蔽后的图像"></p><p>可以看到，我们仅仅是把 k 设置为 2，就出现了边界线。当 $k = 1$ 称为钝化掩蔽，$k &gt; 1$ 时被称为高提升滤波，选择 $k &lt; 1$ 可以减少钝化模板的贡献。</p><h2 id="频率域滤波"><a href="#频率域滤波" class="headerlink" title="频率域滤波"></a>频率域滤波</h2><p>在深度学习出来前，频率域滤波一直是数字图像处理的比较热门的研究点（我们老师说的），频率域滤波的功能还是挺强大的，同样关于原理不过多介绍。</p><h3 id="FFT-amp-频谱"><a href="#FFT-amp-频谱" class="headerlink" title="FFT&amp;频谱"></a>FFT&amp;频谱</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fft_transform</span>(<span class="params">in_path, name, out_path=<span class="string">&quot;.&quot;</span></span>):</span></span><br><span class="line">    image = Image.<span class="built_in">open</span>(in_path)</span><br><span class="line">    plt.subplot(<span class="number">221</span>)</span><br><span class="line">    plt.imshow(image)</span><br><span class="line">    <span class="comment"># 关闭坐标轴</span></span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;Origin Image&quot;</span>)</span><br><span class="line">    image = image.convert(<span class="string">&quot;L&quot;</span>)</span><br><span class="line">    plt.subplot(<span class="number">222</span>)</span><br><span class="line">    plt.imshow(image, <span class="string">&quot;gray&quot;</span>)</span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;Gray Image&quot;</span>)</span><br><span class="line">    <span class="comment"># 进行傅立叶变换，并显示结果</span></span><br><span class="line">    fft2 = np.fft.fft2(image)</span><br><span class="line">    <span class="comment"># 取 log</span></span><br><span class="line">    log_fft2 = np.log(<span class="number">1</span> + np.<span class="built_in">abs</span>(fft2))</span><br><span class="line">    <span class="comment"># 将图像变换的原点移动到频域矩形的中心，并显示效果</span></span><br><span class="line">    shift2center = np.fft.fftshift(fft2)</span><br><span class="line">    <span class="comment"># 对中心化后的结果进行对数变换</span></span><br><span class="line">    log_shift2center = np.log(<span class="number">1</span> + np.<span class="built_in">abs</span>(shift2center))</span><br><span class="line">    plt.subplot(<span class="number">223</span>)</span><br><span class="line">    plt.imshow(np.absolute(shift2center), <span class="string">&quot;gray&quot;</span>)</span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;shift2center&quot;</span>)</span><br><span class="line">    plt.subplot(<span class="number">224</span>)</span><br><span class="line">    plt.imshow(log_shift2center, <span class="string">&quot;gray&quot;</span>)</span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;log_shift2center&quot;</span>)</span><br><span class="line">    plt.savefig(os.path.join(out_path, (<span class="string">&quot;fft_transformed_&quot;</span> + name)))</span><br><span class="line">    <span class="comment"># clear all figure</span></span><br><span class="line">    plt.clf()</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/04/24/Ley6mGWhobpKzBi.png" alt="fft_transformed.png"></p><p>需要注意的是要对变换后的图像取 log，不然值太大会无法在正常的灰度级下显示。</p><h3 id="高低通滤波器"><a href="#高低通滤波器" class="headerlink" title="高低通滤波器"></a>高低通滤波器</h3><p>课本上介绍的 3 个高通滤波器：</p><ul><li>理想高通滤波器<script type="math/tex; mode=display">H(u,v)=\left\{\begin{matrix}0, & D(u,v)\le D_0\\1, & D(u,v)\gt D_0\end{matrix}\right.</script></li><li>高斯高通滤波器<script type="math/tex; mode=display">H(u,v)=1-e^{-D^2(u,v)/2D_0^2}</script></li><li>巴德沃斯高通滤波器<script type="math/tex; mode=display">H(u,v)=\frac{1}{1+\left [ D_0/D(u,v) \right ]^{2n}}</script></li></ul><p>其中 $D_0$ 表示截止频率到矩阵中心的距离，$D(u,v)$ 表示频率矩阵中心到矩阵中任意一点的距离。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_distance</span>(<span class="params">pa, pb</span>):</span></span><br><span class="line">    <span class="keyword">from</span> math <span class="keyword">import</span> sqrt</span><br><span class="line">    dis = sqrt((pa[<span class="number">0</span>] - pb[<span class="number">0</span>]) ** <span class="number">2</span> + (pa[<span class="number">1</span>] - pb[<span class="number">1</span>]) ** <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> dis</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Filter</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate_filter</span>(<span class="params">cls, d, shape, *args, **kwargs</span>):</span></span><br><span class="line">        transfer_matrix = np.zeros(shape)</span><br><span class="line">        center_point = <span class="built_in">tuple</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x - <span class="number">1</span>) // <span class="number">2</span>, shape))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(transfer_matrix.shape[<span class="number">0</span>]):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(transfer_matrix.shape[<span class="number">1</span>]):</span><br><span class="line">                dist = cal_distance(center_point, (i, j))</span><br><span class="line">                transfer_matrix[i, j] = cls.get_one(d, dist, *args, **kwargs)</span><br><span class="line">        <span class="keyword">return</span> transfer_matrix</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_one</span>(<span class="params">cls, d, dist, *args, **kwargs</span>) -&gt; <span class="built_in">float</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ILPFLowPassFilter</span>(<span class="params">Filter</span>):</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_one</span>(<span class="params">cls, d, dist, *args, **kwargs</span>) -&gt; <span class="built_in">float</span>:</span></span><br><span class="line">        <span class="keyword">if</span> dist &lt;= d:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ILPFHighPassFilter</span>(<span class="params">Filter</span>):</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_one</span>(<span class="params">cls, d, dist, *args, **kwargs</span>) -&gt; <span class="built_in">float</span>:</span></span><br><span class="line">        <span class="keyword">if</span> dist &lt;= d:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GaussianHighPassFilter</span>(<span class="params">Filter</span>):</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_one</span>(<span class="params">cls, d, dist, *args, **kwargs</span>) -&gt; <span class="built_in">float</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> - np.exp(-(dist ** <span class="number">2</span>) / (<span class="number">2</span> * (d ** <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GaussianLowPassFilter</span>(<span class="params">Filter</span>):</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_one</span>(<span class="params">cls, d, dist, *args, **kwargs</span>) -&gt; <span class="built_in">float</span>:</span></span><br><span class="line">        <span class="keyword">return</span> np.exp(-(dist ** <span class="number">2</span>) / (<span class="number">2</span> * (d ** <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ButterworthFilter</span>(<span class="params">Filter</span>):</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_one</span>(<span class="params">cls, d, dist, *args, **kwargs</span>) -&gt; <span class="built_in">float</span>:</span></span><br><span class="line">        n = kwargs[<span class="string">&quot;n&quot;</span>]</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> / ((<span class="number">1</span> + dist / d) ** (<span class="number">2</span> * n))</span><br></pre></td></tr></table></figure><p>使用滤波器进行滤波：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">choice = <span class="string">&quot;material/1.jpg&quot;</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(choice).convert(<span class="string">&quot;L&quot;</span>)</span><br><span class="line"></span><br><span class="line">sq = <span class="built_in">min</span>(img.size[<span class="number">0</span>], img.size[<span class="number">1</span>])</span><br><span class="line">img = img.resize((sq, sq))</span><br><span class="line">f = np.fft.fft2(img)</span><br><span class="line">f_shift = np.fft.fftshift(f)</span><br><span class="line"><span class="comment"># gauss_filter_matrix = GaussianLowPassFilter.generate_filter(50, img.size)</span></span><br><span class="line">butter_filter_matrix = ButterworthFilter.generate_filter(<span class="number">30</span>, img.size, n=<span class="number">2</span>)</span><br><span class="line">filter_matrix = np.fft.ifftshift(f_shift*butter_filter_matrix)</span><br><span class="line">transform_img = np.<span class="built_in">abs</span>(np.fft.ifft2(filter_matrix))</span><br><span class="line">plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">plt.imshow(transform_img, cmap=<span class="string">&quot;gray&quot;</span>)</span><br><span class="line">plt.savefig(<span class="string">&quot;gauss_filter.png&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h1 id="扩展：人脸磨皮算法"><a href="#扩展：人脸磨皮算法" class="headerlink" title="扩展：人脸磨皮算法"></a>扩展：人脸磨皮算法</h1><h2 id="Bilateral"><a href="#Bilateral" class="headerlink" title="Bilateral"></a>Bilateral</h2><p>双边滤波（Bilateral filter）结合图像的空间邻近度和像素值相似度，同时考虑空域信息和灰度相似性，达到保边去噪的目的。</p><p>它的滤波器核由两个函数生成：空间域核和值域核。</p><p>空间域核是由像素位置欧式距离决定的模板权值，公式：</p><script type="math/tex; mode=display">w_d(i,j,k,l)=exp(-\frac{(i-k)^2+(i-l)^2}{2\delta_d^2})</script><p>其中i,j 代表的是当前坐标点的位置 k，l 为中心坐标点，$\delta_d$ 代表高斯函数的标准差。很明显 $w_d$ 是计算临近点 ij 到中心点的临近程度，因此空间域核是用于衡量空间临近的程度。这代表空间域的高斯函数。</p><p>值域核是由灰度像素值的差值决定模板的权值的：</p><script type="math/tex; mode=display">w_r(i,j,k,l)=exp(-\frac{(f(i,j)-f(k,l))^2}{2\delta_r^2})</script><p>$f(i,j)$代表每个点的灰度像素值，$f(k,l)$代表中点的像素值，$\delta_r$也是值域核下高斯函数的标准差。将两者相乘就能得到双边滤波的模板权值：</p><script type="math/tex; mode=display">w(i,j,k,l)=w_d(i,j,k,l)*w_r(i,j,k,l)=exp(-\frac{(i-k)^2+(i-l)^2}{2\delta_d^2}-\frac{(f(i,j)-f(k,l))^2}{2\delta_r^2})</script><p>化简：</p><script type="math/tex; mode=display">g(i,j)=\frac{\sum_{kl}f(k,l)w(i,j,k,l)}{\sum_{kl}w(i,j,k,l)}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 公式看起来实现起来好麻烦，还好有库</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">file_name = <span class="string">&#x27;./material/4.jpg&#x27;</span></span><br><span class="line">image = cv2.imread(file_name)</span><br><span class="line"></span><br><span class="line">dst = cv2.bilateralFilter(src=image, d=<span class="number">0</span>, sigmaColor=<span class="number">100</span>, sigmaSpace=<span class="number">15</span>)</span><br><span class="line">cv2.imshow(<span class="string">&quot;Source&quot;</span>, image)</span><br><span class="line">cv2.imshow(<span class="string">&quot;Filter&quot;</span>, dst)</span><br><span class="line"></span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/04/21/dm7hzjEcTlV2Qqu.png" alt="bilateral.png"></p><h2 id="表面模糊"><a href="#表面模糊" class="headerlink" title="表面模糊"></a>表面模糊</h2><p>图像的表面模糊处理，其作用是在保留图像边缘的情况下，对图像的表面进行模糊处理。在对人物皮肤处理上，比高斯模糊更有效。（高斯模糊在使人物皮肤光洁的同时，也将一些边缘特征给模糊了）</p><p>在处理手法上，表面模糊也与前面提到的卷积处理手段不同，表面模糊是每一个像素点都有自己的卷积矩阵，而且还是 3 套，用以对应于像素的 R、G、B 分量。</p><p>表面模糊有 2 个参数，即模糊半径 r 和模糊阈值 T，模糊半径确定模糊的范围，而模糊范围确定的是卷积矩阵的大小，模糊矩阵是一个长宽相等的矩阵，长度 $l=2r+1$。</p><p>矩阵的中间元素是当前的像素点，其余的元素按照下面的方法计算：</p><script type="math/tex; mode=display">w_{ij}=1-\frac{\left | I_{ij}-I_0 \right | }{2.5T}</script><p>$I_{ij}$ 是图像值，$I_0$是模板矩阵中心的图像值</p><p>一般来说，会有预处理： $w_{ij}=max(0, w_{ij})$</p><p>根据卷积运算，每个像素通过表面模糊之后的值为：</p><script type="math/tex; mode=display">w_{ij}=\frac{\sum w_{ij}I_{ij}}{\sum w_{ij}}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 表面模糊算法</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sur_blur</span>(<span class="params">origin, threshold, r</span>):</span></span><br><span class="line">    transformed = origin * <span class="number">1.0</span></span><br><span class="line">    row, col = origin.shape</span><br><span class="line">    w_size = r * <span class="number">2</span> + <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(r, row - <span class="number">1</span> - r):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(r, col - <span class="number">1</span> - r):</span><br><span class="line">            iij = origin[i-r: i+r+<span class="number">1</span>, j-r: j+r+<span class="number">1</span>]</span><br><span class="line">            i0 = numpy.ones([w_size, w_size]) * origin[i, j]</span><br><span class="line">            wij = <span class="number">1</span> - <span class="built_in">abs</span>(iij - i0) / (<span class="number">2.5</span> * threshold)</span><br><span class="line">            wij[wij &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">            tmp = iij * wij</span><br><span class="line">            transformed[i, j] = tmp.<span class="built_in">sum</span>() / wij.<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> transformed</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">file_name = <span class="string">&#x27;./material/4.jpg&#x27;</span></span><br><span class="line">img = io.imread(file_name)</span><br><span class="line">img_out = img * <span class="number">1.0</span></span><br><span class="line">boundary = <span class="number">20</span></span><br><span class="line">half_size = <span class="number">10</span></span><br><span class="line">img_out[:, :, <span class="number">0</span>] = sur_blur(img[:, :, <span class="number">0</span>], boundary, half_size)</span><br><span class="line">img_out[:, :, <span class="number">1</span>] = sur_blur(img[:, :, <span class="number">1</span>], boundary, half_size)</span><br><span class="line">img_out[:, :, <span class="number">2</span>] = sur_blur(img[:, :, <span class="number">2</span>], boundary, half_size)</span><br><span class="line"></span><br><span class="line">img_out = img_out / <span class="number">255</span></span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">121</span>)</span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">122</span>)</span><br><span class="line">plt.imshow(img_out)</span><br><span class="line">plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">plt.savefig(<span class="string">&quot;surface_blur.png&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>效果挺好，就是算的有点慢：<br><img src="https://i.loli.net/2021/04/24/6W2wB7e5Jriu9Kb.png" alt="surface_blur.png"></p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul><li>数字图像处理（第四版），冈萨雷斯</li><li><a href="https://zhuanlan.zhihu.com/p/118496347">高斯滤波和双边滤波原理和python实现</a></li><li><a href="https://blog.csdn.net/zb1165048017/article/details/107798789">一个简单好用的磨皮祛斑算法理论和python实现</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;2021/4/24: 解决(?)公式渲染问题，卸了重装了渲染器，然后参考配置了&lt;a href=&quot;https://github.com/MakerGYT/markdown-it-latex2img&quot;&gt;服务器渲染&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;空间-amp-频率域滤波&quot;&gt;&lt;a href=&quot;#空间-amp-频率域滤波&quot; class=&quot;headerlink&quot; title=&quot;空间&amp;amp;频率域滤波&quot;&gt;&lt;/a&gt;空间&amp;amp;频率域滤波&lt;/h1&gt;&lt;p&gt;用于记录学习数字图像处理的历程（不完全同步学校课程，不定期摸鱼）&lt;/p&gt;
&lt;h2 id=&quot;空间域滤波&quot;&gt;&lt;a href=&quot;#空间域滤波&quot; class=&quot;headerlink&quot; title=&quot;空间域滤波&quot;&gt;&lt;/a&gt;空间域滤波&lt;/h2&gt;&lt;h3 id=&quot;直方图均衡化&quot;&gt;&lt;a href=&quot;#直方图均衡化&quot; class=&quot;headerlink&quot; title=&quot;直方图均衡化&quot;&gt;&lt;/a&gt;直方图均衡化&lt;/h3&gt;&lt;p&gt;直方图的形状往往与图像的外观有关。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在暗图像的直方图中，大多数直方图容器集中在灰度级较低的一端（靠近 0）&lt;/li&gt;
&lt;li&gt;亮的图像的直方图中，多数直方图容器集中在灰度级的高端&lt;/li&gt;
&lt;li&gt;低对比度的图像的直方图中，直方图容器基本位于灰度级的中间&lt;/li&gt;
&lt;li&gt;高对比度图像的直方图，容器覆盖了较宽的范围，并且像素的分布是基本均匀的</summary>
    
    
    
    <category term="Digital Image Processing" scheme="http://cyx0706.github.io/categories/Digital-Image-Processing/"/>
    
    
    <category term="CV" scheme="http://cyx0706.github.io/tags/CV/"/>
    
    <category term="Digital Image Processing" scheme="http://cyx0706.github.io/tags/Digital-Image-Processing/"/>
    
  </entry>
  
  <entry>
    <title>winter-2021</title>
    <link href="http://cyx0706.github.io/2021/03/19/winter-2021/"/>
    <id>http://cyx0706.github.io/2021/03/19/winter-2021/</id>
    <published>2021-03-19T03:14:08.000Z</published>
    <updated>2021-03-19T01:31:35.919Z</updated>
    
    <content type="html"><![CDATA[<p><del>诈尸</del></p><p>2021/3/19 开学第三周，挖出来，填一下实际完成的东西。<br><a id="more"></a></p><h1 id="今年"><a href="#今年" class="headerlink" title="今年"></a>今年</h1><p>一回学校就补了部番《机器人笔记》，不愧是我…</p><p>确实很喜欢，之前只是听过 “ROBOTICS;NOTES” 这首超棒的配乐，在看了剧情后，更是感受到了如日出般，绝望中诞生出新的希望的感动和热血。剧情和我很有共鸣，一群热爱某个的事物的人为自己的梦想付出努力的故事，也许主人公们并不完美，但他们都有着对梦想的热情，<del>比博燃</del></p><p>摘一个我喜欢的网易云热评：</p><blockquote><p>或许这架机器人不如屏幕中的完美，也不如屏幕中的强大，但是最后它还是动起来了，真的为了拯救这个世界而动起来了，那还有什么可遗憾的呢？还有比这令人振奋的事情吗？</p></blockquote><p>顺便我找图片的时候还看到了别人写的游记，将来有一天我也想游览各地体会风土人情！</p><p><a href="https://www.jianshu.com/p/148c5363f45c">【拜访全球最浪漫的活火山】鹿儿岛·樱岛15000字超长旅行攻略</a></p><p>说说这学期的事吧，保研基本稳定（虽然排名总体靠近估计的保研线中间），现在就是寻找方向，导师。听我们学长说计算机视觉特别卷，但还是想学……，想读视觉方向目前又担心自己上不了想去的学校，这是目前我最担心的了（自言自语中……），没有啥比赛获奖，成绩也一般，有人会要我吗……</p><p>给想去实验室的同学们一个劝告，早，一定要早去。我之前没有认识到，这学期想去实验室学习，3 周了还没找到自己应该参加什么项目。老师大部分的项目都是有一定积累的或者正在做的，去的早可以先多了解再加入，毕竟自己不在实验室，先让自己熟悉老师的方向和有哪些工作自己可以做的需要花上一定时间。</p><p>ε=(´ο｀*))) 唉，就平凡的人只能多努力了啊。</p><h1 id="去年……"><a href="#去年……" class="headerlink" title="去年……"></a>去年……</h1><p>操作系统考炸了没心情。</p><p>记下没填出来的空，也没啥用</p><ul><li>操作系统是一组：能有效组织和管理四大资源的，合理地对各类作业进行调度和控制的，方便用户使用计算机的软件。</li><li>对于用户，文件系统实现<strong>按名存取</strong></li></ul><p>嗳，就考你背诵，写不上，那你操作系统学了点啥（老师你讲了啥（笑））<strong>！！！以上是内心活动</strong></p><p>来规划一下寒假做啥</p><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ul><li><p>在 2 月 23 左右开学的时候来 check</p></li><li><p>[x] 考完试平安到家（其实这仅仅是个 checkbox 的测试，写的这天没考完也没到家）</p></li></ul><h3 id="ai-x-blending-courses"><a href="#ai-x-blending-courses" class="headerlink" title="ai+x blending courses"></a>ai+x blending courses</h3><ul><li>[x] 课听完</li><li>[x] 每个 Module 整理笔记，写全英的博客</li><li>[] Live Session 整理讲的内容，尽可能反复听，最后整理一篇出来</li></ul><h3 id="CV"><a href="#CV" class="headerlink" title="CV"></a>CV</h3><ul><li>[] 图形学教材老师推荐的整一本看，能看多少算多少吧</li></ul><h3 id="计科"><a href="#计科" class="headerlink" title="计科"></a>计科</h3><ul><li>[] 龙书看起来，算是提前预习，至少看 3-4 章吧</li><li>[x] 自己动手写一个 64 位操作系统接着看，基础部分必须看完 + 内容整理完</li><li>[] 尝试实践自己在程序员的自我修养书中学到的编译链接的知识</li></ul><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><ul><li><p>准备夏令营</p><ul><li>[] 简历</li><li>[] 自我介绍（中+英）</li><li>[] 项目（中+英）</li></ul></li><li><p>[x] 下学期给社团新生讲课的 PPT</p></li><li>[x] 整理数学建模学到的知识</li></ul><h2 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h2><p>假期有啥想写的了（感想啥的）放这里</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;del&gt;诈尸&lt;/del&gt;&lt;/p&gt;
&lt;p&gt;2021/3/19 开学第三周，挖出来，填一下实际完成的东西。&lt;br&gt;</summary>
    
    
    
    
    <category term="essay" scheme="http://cyx0706.github.io/tags/essay/"/>
    
  </entry>
  
  <entry>
    <title>probabilistic-methods</title>
    <link href="http://cyx0706.github.io/2021/03/13/probabilistic-methods/"/>
    <id>http://cyx0706.github.io/2021/03/13/probabilistic-methods/</id>
    <published>2021-03-13T13:46:37.000Z</published>
    <updated>2021-04-24T03:48:46.415Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概率方法"><a href="#概率方法" class="headerlink" title="概率方法"></a>概率方法</h1><h2 id="Monte-Carlo-Simulation"><a href="#Monte-Carlo-Simulation" class="headerlink" title="Monte Carlo Simulation"></a>Monte Carlo Simulation</h2><h3 id="General"><a href="#General" class="headerlink" title="General"></a>General</h3><p>我们需要定量分析我们的模型预测结果的可靠性，而蒙特卡洛方法就是可以预测概率，评估风险的一个方法。当我们需要计算，测量不确定性对模型的影响，模特卡洛方法是首先考虑的。简单来说，蒙特卡洛方法就是用不确定的参数反复评估模型，研究模型输出的不确定性。</p><p>蒙特卡洛方法的主要好处就在于它的鲁棒性和多功能性，不好之处在于它的收敛速度很慢并且计算量很大。<br><a id="more"></a></p><h3 id="Mathematical-Representation"><a href="#Mathematical-Representation" class="headerlink" title="Mathematical Representation"></a>Mathematical Representation</h3><p>我们用服从某个概率分布的随机变量来模型化不确定性，假设我们想要研究随机变量 X 的不确定性是如何影响模型输出 Y 的，先选择一些输入 $[X_1, X_2, \cdots, X_n]$，我们可以得到一组模型 f 的输出：</p><script type="math/tex; mode=display">[Y_1, \cdots, Y_n] = [f(X_1), \cdots, f(X_n)]</script><p>我们可以估计出输出的不确定性的平均值，比如说使用大数定律。根据大数定律（算数平均值依概率收敛于数学期望）。</p><script type="math/tex; mode=display">E[Y] = \frac{1}{N} \sum_{i=1}^{N} f(X_i)</script><p>根据中心极限定理：（当 n 足够大的时候，我们可以把任何一个奇奇怪怪（期望方差要存在）的分布，近似成一个正态分布）$\sum_{i=1}^{N} X_i ~ N(n\mu , n\sigma ^2)$</p><h3 id="probabilistic-forcasting"><a href="#probabilistic-forcasting" class="headerlink" title="probabilistic forcasting"></a>probabilistic forcasting</h3><p>蒙特卡洛方法可以被应用于一个动态模型的概率预测上，如下：</p><script type="math/tex; mode=display">x_{t+1} = f(x_t, \epsilon _t)</script><p>$x_t$ 表示状态，$\epsilon _t$ 表示在 t 时间的噪声。噪声可以表示我们不确定的输入，随机性使我们能够以定量的方式在我们缺乏一些知识下模拟场景并帮我们推理这些这些知识。</p><p>在动态模型上使用蒙特卡洛方法被称为集合预测（ensemble forecasting），这涉及模拟系统的许多独立轨迹（trajectory），以便对系统行为进行预测。每个轨迹就是一个集合成员（ensemble member）</p><p>假设我们想要得到再时间 $t=1$ 时的预测值，用 M 条轨迹描述初始条件 $x_0$，则从 1 到 M，有：</p><script type="math/tex; mode=display">x_1^{(i)}=f(x_0, \epsilon _0^{(i)})</script><p>这样我们就可以预测将来某个时间系统状态的均值和方差，并计算系统可能处于不安全区域的概率或风险。</p><h3 id="Simulating-Rare-Event"><a href="#Simulating-Rare-Event" class="headerlink" title="Simulating Rare Event"></a>Simulating Rare Event</h3><p>蒙特卡洛方法也可以用于模拟罕见的事件的发生：</p><ul><li>地震</li><li>洋流转变</li><li>极端天气</li><li>股票市场崩溃</li></ul><p>但问题是效率太差，想要得到一个比较小的蒙特卡洛观测误差值比较困难，但我们可以通过修改蒙特卡洛方法来提升效率，比如说重要性采样（Importance Sampling）。</p><p>我们假设 Y 是根据概率分布 $p(y)$ 分布的随机变量，我们想要估计 Y 超过某个特定的阈值 a 的概率，一个简单的蒙特卡洛估算方法如下：</p><script type="math/tex; mode=display">\hat{\rho } = \frac{1}{N} \sum_{i=1}^{N} 1_{Y>a} (Y^i) Y^i \sim  p(y)\\1_{Y>a}(y) = \left\{\begin{matrix} 1 & y>a\\ 0 & y<a\end{matrix}\right.</script><p>这样就有一个问题，对于概率很小很小的事件，我们模拟使用的 N 会非常的大，造成计算开销很大。</p><p>重要性采样使用了另一种分布——偏差分布（biasing distribution） $q(y)$，修改上面的模型为：</p><script type="math/tex; mode=display">\hat{\rho }_{IS} = \frac{1}{N} \sum_{i=1}^{N} 1_{Y>a} (\tilde{Y}^i) \frac{p(\tilde{Y}^i)}{q(\tilde{Y}^i)} \\\tilde{Y}^i \sim  q(y)</script><h2 id="Sensitivity-Analysis"><a href="#Sensitivity-Analysis" class="headerlink" title="Sensitivity Analysis"></a>Sensitivity Analysis</h2><p>当我们有这样一个式子：</p><script type="math/tex; mode=display">Y = f(X_1, \vdots, X_n)</script><p>Y 的值取决于一系列的 X，如果我们想要知道那个输入对结果的影响更大，即输入的敏感度（Sensitivity ），就需要敏感度分析（Sensitivity analysis），通过这种方法，我们可以降低投资决策中的不确定性，或者在预测值时忽略一些不重要的输入。</p><p>敏感性分析有以下方法：</p><h3 id="Local-sensitivity-analysis"><a href="#Local-sensitivity-analysis" class="headerlink" title="Local sensitivity analysis"></a>Local sensitivity analysis</h3><p>敏感度系数定义为：</p><script type="math/tex; mode=display">s_i = \frac{\partial y}{\partial x_i}</script><p>对于每个输入 $x_i$，计算 $s_i$ 来衡量。</p><p>问题是，我们应该用那个位置的点 x 来计算偏导？如果模型是一个非线性模型，在不同 x 处得到敏感度系数会有很大的差异。这个方法还忽略了几点：如果 x 是非常不确定变化的变量呢？如果 x 的分布范围很广泛，它可能会比窄的分布范围对输出的 Y 有更大的影响。</p><h3 id="Averaged-derivative"><a href="#Averaged-derivative" class="headerlink" title="Averaged derivative"></a>Averaged derivative</h3><p>对局部敏感度在输入上求积分来表示敏感度：</p><script type="math/tex; mode=display">\int_{D} (\frac{\partial y}{\partial x_i})^2 dx_i</script><p>但依旧有问题，如周期函数，会导致结果不准确。</p><h3 id="Variance-based-global-sensitivity-analysis"><a href="#Variance-based-global-sensitivity-analysis" class="headerlink" title="Variance based global sensitivity analysis"></a>Variance based global sensitivity analysis</h3><p>就像我们使用蒙特卡洛方法来通过 x 的分布估测 y 的分布，我们通过输出 y 的方差值，该方差来自于每个输入的不确定性。我们将对 y 的方差的影响看成一个饼，不同的变量组合对结果的影响可以从饼中划分出来一部分。</p><p><img src="https://i.loli.net/2021/03/17/SCqK2LY57zThjH9.png" alt="对Y影响的饼图"></p><p>为什么这样是正确的呢？我们来看背后的数学推导。</p><p>我们假设输出 Y 受到 $X_1$ 和 $X_2$ 的影响，它们以不同的形式影响这 Y 值，我们现在想要知道它们哪个对 Y 的影响更大，基于方差的全局敏感性分析给出的方法是：如果 $X_1$ 对 Y 的方差值贡献的更多，那么 Y 对 $X_1$ 更加敏感。</p><script type="math/tex; mode=display">Var[Y]=Var_{x_1}[\Bbb E_{x_2}[Y|X_1] ] + \Bbb E_{x_1}[Var_{x_2}[Y|X_1] ]</script><p>上面的式子是方差的一个性质，$\Bbb E_{x_2} [Y| X_1]$ 项代表固定 x 下的条件均值，直观来看，这是 Y 在给定 x 下的最好的观察值，我们可以用它来代表 Y 由于 $X_1$ 而产生的方差。后面的部分就是 Y 在 非 $X_1$ 影响下产生的方差，影响它的可能包括 $X_1 X_2$</p><p>我们在以 $X_2$ 为计算对象来写另外一个相似的计算公式：</p><script type="math/tex; mode=display">Var[Y]=Var_{x_2}[\Bbb E_{x_1}[Y|X_2] ] + \Bbb E_{x_2}[Var_{x_1}[Y|X_2] ]</script><p>仅仅由 $X_1$ 对方差的影响的占比为</p><script type="math/tex; mode=display">s_1 = \frac{Var_{x_1}[\Bbb E_{x_2}[Y|X_1] ]}{Var[Y]}</script><p>由包含 $X_1$ 的项（只含$X_1$和交叉项）对方差的贡献为：</p><script type="math/tex; mode=display">s_{T_1} = \frac{\Bbb E_{x_2}[Var_{x_1}[Y|X_2] ]}{Var[Y]}</script><p>当有两个以上变量的时候，上面两个公式需要归一化。</p><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;概率方法&quot;&gt;&lt;a href=&quot;#概率方法&quot; class=&quot;headerlink&quot; title=&quot;概率方法&quot;&gt;&lt;/a&gt;概率方法&lt;/h1&gt;&lt;h2 id=&quot;Monte-Carlo-Simulation&quot;&gt;&lt;a href=&quot;#Monte-Carlo-Simulation&quot; class=&quot;headerlink&quot; title=&quot;Monte Carlo Simulation&quot;&gt;&lt;/a&gt;Monte Carlo Simulation&lt;/h2&gt;&lt;h3 id=&quot;General&quot;&gt;&lt;a href=&quot;#General&quot; class=&quot;headerlink&quot; title=&quot;General&quot;&gt;&lt;/a&gt;General&lt;/h3&gt;&lt;p&gt;我们需要定量分析我们的模型预测结果的可靠性，而蒙特卡洛方法就是可以预测概率，评估风险的一个方法。当我们需要计算，测量不确定性对模型的影响，模特卡洛方法是首先考虑的。简单来说，蒙特卡洛方法就是用不确定的参数反复评估模型，研究模型输出的不确定性。&lt;/p&gt;
&lt;p&gt;蒙特卡洛方法的主要好处就在于它的鲁棒性和多功能性，不好之处在于它的收敛速度很慢并且计算量很大。&lt;br&gt;</summary>
    
    
    
    <category term="AI" scheme="http://cyx0706.github.io/categories/AI/"/>
    
    
    <category term="AI+X Blending" scheme="http://cyx0706.github.io/tags/AI-X-Blending/"/>
    
  </entry>
  
  <entry>
    <title>from-optimization-to-ml</title>
    <link href="http://cyx0706.github.io/2021/03/11/from-optimization-to-ml/"/>
    <id>http://cyx0706.github.io/2021/03/11/from-optimization-to-ml/</id>
    <published>2021-03-11T12:42:33.000Z</published>
    <updated>2021-03-11T11:21:39.865Z</updated>
    
    <content type="html"><![CDATA[<h1 id="From-Optimization-To-Machine-Learning"><a href="#From-Optimization-To-Machine-Learning" class="headerlink" title="From Optimization To Machine Learning"></a>From Optimization To Machine Learning</h1><p>ps：数学公式好多渲染出奇怪的问题，我佛了，看看换个渲染引擎</p><h2 id="Regession-Problems"><a href="#Regession-Problems" class="headerlink" title="Regession Problems"></a>Regession Problems</h2><h3 id="General"><a href="#General" class="headerlink" title="General"></a>General</h3><p>先补充回顾一下<a href="https://zhuanlan.zhihu.com/p/26884695">范数</a></p><p>我们前面包括后面见到的 $||x ||_2$ 就是 l2 范数，表示向量或者矩阵的元素的平方和： </p><script type="math/tex; mode=display">|| x ||_2  =  \sqrt { \sum_i {x_i}^2 }</script><ul><li><p>回归方法是建立模型常用的一种策略，当我们有数据但没有建立起具体的模型或者是想要一个便于计算的模型时，回归模型是一种不错的选择。它可以从多个预测变量中得到一个连续的预测值。</p><a id="more"></a><p>我们假设有 M 个输入的点 $z_m$，对应输出 $y_m$，构成二元对$\{(z_m, y_m)\}_{m=1}^M$，我们的目标是训练一个函数 $y=f(z)$。</p></li><li><p>虽然插值(interpolant)可以很好和精巧的找到这样的 f，但实际现实中的数据往往包含噪声，对于这样的点，插值模型会很大程度上受到数据的影响，模型的普适性很差。更好的方法是用 N 个基础函数去表示 f：</p></li></ul><script type="math/tex; mode=display">f(x)=\sum_{n=1}^{N}c_n \psi_n(x)</script><p>系数 $c_n$ 可以告诉我们对应的函数（可以理解为函数提取出来的特征）在预测数值时的重要性。$\psi_n(x)$ 是我们认为设置的，我们要训练的就是系数 $c_n$，老方法，把它转换为一个优化问题：</p><script type="math/tex; mode=display">\min_{c_1, \cdots, c_N} \ \sum_{m=1}^{N}(y_m-\sum_{n=1} \psi_n (z_m) * c_n)^2</script><p>写成矩阵的形式：</p><script type="math/tex; mode=display">X = \begin{bmatrix}\psi_1(x_1)  & \cdots & \psi_N(x_1) \\\\\vdots  &  & \vdots \\\\\psi_1(x_m)  & \cdots & \psi_N(x_m)\end{bmatrix}\Y=\begin{bmatrix}y_1 \\\\\vdots \\\\y_M\end{bmatrix}, c=\left [ c_1, \cdots, c_N \right ]^T</script><p>这样，我们就可以将原来的优化问题化为标准形式：</p><script type="math/tex; mode=display">\min_{c} ||Y-Xc ||_2^2</script><p>线性回归的优化问题可以用解方程来解决，避免了迭代方法：</p><script type="math/tex; mode=display">c=(X^T X)^{-1} (X^T Y)</script><p>实际上，对于监督学习的神经网络就是非线性的回归模型。</p><h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><p>线性回归（Linear regression）指回归模型的系数是线性的，模型的输入并没有必要是线性的。对于线性最小二乘问题，我们可以直接求出最小值点，对于一些有高维度系数的问题，有特殊的数值方法去寻找解。</p><ul><li>当数据点的个数比要学习的参数多时，方程的个数大于未知数的个数（M &gt; N），最小二乘问题是<strong>系统超定（overdetermined）</strong>的，反之是未确定的（underdetermined）。</li><li>对于超定的最小二乘问题通常有特殊解，而未确定系统有无穷多个系数可以使损失函数最小，虽然他们得到的目标函数的值是一样的，但在预测未知点的时候有很大的不同，这个问题被称为<strong>不适定问题（ill-posed problem）</strong></li><li>要解决不适定问题，一种方法是平衡要估计的参数和数据点个数，另一种方法是<strong>正则化（regularization）</strong>，正则化在原目标函数上增加了惩罚项：</li></ul><script type="math/tex; mode=display">\min_{c} || Y-Xc || _2^2 + \lambda R ( c )</script><p>$\lambda$ 是正则化系数，$ R ( c ) $ 是正则化项。$ R ( c ) $ 有很多种选择，比如：</p><ul><li>L2 正则化：Tikhonov regularization (Ridge regression)  $ R( c )={||c||}_{2}^{2} = { \sum_n^{N} c_n^2} $</li><li>L1 正则化：LASSO (sparse regression)  $ R ( c )= ||c|| _1= \sum_n^N  \left | c_n \right | $</li></ul><p>为什么增加惩罚函数会有效？</p><ul><li>当你的参数比数据点还多的时候，你会希望一些参数的值为 0，而我们以 L1 形式为例，它就有这样的魔法，让一些不是很重要的参数变成 0。结合下面的图，我们给出从图像角度的理解：</li></ul><p><img src="https://i.loli.net/2021/03/10/5IAedkUa9Q6HOMg.png" alt="L1 Regularization" style="zoom:50%;" /></p><p>我们绘制出 L1 惩罚函数的图像，横坐标和纵坐标是 c1 和 c2 满足 $\left | c_1 \right | + \left | c_2 \right | = k$，他们的曲线构成了一圈一圈的正方形。接着我们画出 $Xc=Y$ 达成最小值的解 c1 和 c2 的图像，有很多的点，他们构成一条直线，直线必然与 $\left | c_1 \right | + \left | c_2 \right | = k$ 有个交点，这取决于 k 值，但要确保我们的目标函数为极小值，我们也要让 k 尽量的小，也就是尽量靠内侧的正方形，在图中表现就是交点，同样也是与 c1 轴的交点，此时 c1 = k, c2 = 0 $R(c)$ 就选择了最重要的系数 c1。</p><h3 id="Python-中的线性回归"><a href="#Python-中的线性回归" class="headerlink" title="Python 中的线性回归"></a>Python 中的线性回归</h3><p><a href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html">官方文档给出的例子</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create linear regression object</span></span><br><span class="line">regr = linear_model.LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model using the training sets</span></span><br><span class="line">regr.fit(diabetes_X_train, diabetes_y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make predictions using the testing set</span></span><br><span class="line">diabetes_y_pred = regr.predict(diabetes_X_test)</span><br></pre></td></tr></table></figure></p><h2 id="Classification-Problems"><a href="#Classification-Problems" class="headerlink" title="Classification Problems"></a>Classification Problems</h2><ul><li>分类问题是针对于离散的点或者不连续的输出的方法，用线性回归模型去建立分类器可能并不好，回归模型不会被限制为具有离散的输出，并且可以在范围外进行预测。</li></ul><h3 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h3><p>逻辑回归将线性模型转变为逻辑函数的形式$logistic(s) = \frac{e^{s} } {1+e^s } $：</p><script type="math/tex; mode=display">f(x; c) = \frac { e ^ {c^T x} } { 1 + e ^ {c^T x}}</script><p>其中 $c^T$ 表示模型中的参数 $\left [ c_1, \cdots, c_n \right ]$。逻辑函数的特征是：输出范围为$[0, 1]$，对于小的输入，输出接近 0；对于大的输入，输出接近 1。<br>这样我们要解决的优化问题如下：</p><script type="math/tex; mode=display">\max_{c} \prod_{m:y_m=1} f(x_m;c) \ \prod_{m:y_m=0}1 - f(x_m;c)</script><p>$\prod_{m:y_m=1} f(x_m;c)$ 代表所有输入是$x_m$，输出$y_m=1$的点。目标函数力图让$y_m=1$时我们函数的预测值尽可能的接近 1，当$y_m=0$时尽可能预测值为 0，这样 $1-f(x)$ 最大。在统计学中，常常使用极大似然估计法来求解，即找到一组参数，使得在这组参数下，我们的数据的似然度（概率）最大。为了方便求解，我们通常求对数：</p><script type="math/tex; mode=display">\sum (\ln(f(x_m;c)) + \ln(1-f(x_m;c)))</script><ul><li>逻辑回归不仅可以做二分类问题，不仅可以预测出来类别，还能得到预测的概率，在逻辑回归模型中，我们最大化似然函数和最小化损失函数实际上是等价的。</li></ul><h3 id="Multi-valued-logistic-regression"><a href="#Multi-valued-logistic-regression" class="headerlink" title="Multi-valued logistic regression"></a>Multi-valued logistic regression</h3><p>例子：待完成</p><h2 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h2><p>回归，分类问题，参数估计都需要解优化问题，它们有一个一般的形式：</p><script type="math/tex; mode=display">\min_{c} \sum_{i=1}^{M}\ell (y_i, f(x_i; c))</script><p>l 是损失函数，表示预测值和实际值的误差，而在每个数据点误差的和常常被称为经验风险（empirical risk）。对于回归问题，$\ell$ 是平方误差 $\ell (y_i, f(x_i; c))=(f(x;c)-y)^2$，对于逻辑回归，$\ell (y_i, f(x_i; c))=log(1+e^{-yf(x;c)})$。</p><p>当我们的数据集很大的时候，迭代方法每次都要对所有的点计算 M 大小的梯度值就会导致很耗时，随机（stochastic）梯度下降（SGD）通过随机选择一个数据集的子集去计算梯度，并用这个值来近似整体的梯度，一个简单的方法就是每次迭代选择一个点：</p><ul><li>随机从 ${1, \cdots, M}$ 选择 j 点</li><li>用这个点计算每一步梯度的值：$(x_j, y_j): c_{n+1}=c_n- \alpha \nabla_c \ell(y_j, f(x_j;c_n))$</li></ul><p>通过每个样本都重复迭代，如果数据有1万条，就迭代1万次，每次用一个点计算来获取最优解，而梯度下降迭代一次要用到全部的样本。优点当然是训练的速度快，缺点是噪声较多，每次迭代不一定朝着整体最优化的方向前进。</p><h2 id="Mini-Batch-Gradient-Descent"><a href="#Mini-Batch-Gradient-Descent" class="headerlink" title="Mini-Batch Gradient Descent"></a>Mini-Batch Gradient Descent</h2><p>在权衡随机梯度下降和批量梯度下降方法后，便有了小批量梯度下降法（MBGD），每次迭代的时候用样本的子集，样本的一部分来代替全部的样本计算梯度。比如说有 1000 个样本，我们选择 10 个一组，这样就有了100 个 mini-batch，重复 100 次计算，每次有 $ c_{n+1}=c_n- \frac{1}{10} \alpha \nabla_c \ell(y_j, f(x_j;c_n)); j=1..100; x_j = [x_j^1, …, x_j^{10}]$</p><h2 id="Assessing-Model-Fit"><a href="#Assessing-Model-Fit" class="headerlink" title="Assessing Model Fit"></a>Assessing Model Fit</h2><ul><li>当我们训练的模型对于训练集的一些不重要的特征关注的太多了后，在新的数据到来时表现的结果很差，这就是<strong>过拟合（Over-fitting）</strong>，我们需要一种验证误差的方法，它可以帮助我们决定什么时候停止训练来避免过拟合。</li></ul><p>对于回归模型，很经典的方法是计算确定系数$R^2$</p><script type="math/tex; mode=display">R^2 = 1 - \frac{ || Xc-Y  ||_2^2 }{ || \bar{y}-Y   ||_2^2 }</script><p>$\bar{y}$表示输出的 y 值的平均值，$R^2$ 越接近 1 说明模型很好的表现了数据的变化，如果数据集有很多的噪声，可能会导致 $\bar{y}$ 很小，但有可能模型依旧有很好的适应性和预测能力</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;From-Optimization-To-Machine-Learning&quot;&gt;&lt;a href=&quot;#From-Optimization-To-Machine-Learning&quot; class=&quot;headerlink&quot; title=&quot;From Optimization To Machine Learning&quot;&gt;&lt;/a&gt;From Optimization To Machine Learning&lt;/h1&gt;&lt;p&gt;ps：数学公式好多渲染出奇怪的问题，我佛了，看看换个渲染引擎&lt;/p&gt;
&lt;h2 id=&quot;Regession-Problems&quot;&gt;&lt;a href=&quot;#Regession-Problems&quot; class=&quot;headerlink&quot; title=&quot;Regession Problems&quot;&gt;&lt;/a&gt;Regession Problems&lt;/h2&gt;&lt;h3 id=&quot;General&quot;&gt;&lt;a href=&quot;#General&quot; class=&quot;headerlink&quot; title=&quot;General&quot;&gt;&lt;/a&gt;General&lt;/h3&gt;&lt;p&gt;先补充回顾一下&lt;a href=&quot;https://zhuanlan.zhihu.com/p/26884695&quot;&gt;范数&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我们前面包括后面见到的 $||x ||_2$ 就是 l2 范数，表示向量或者矩阵的元素的平方和： &lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
|| x ||_2  =  \sqrt { \sum_i {x_i}^2 }&lt;/script&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;回归方法是建立模型常用的一种策略，当我们有数据但没有建立起具体的模型或者是想要一个便于计算的模型时，回归模型是一种不错的选择。它可以从多个预测变量中得到一个连续的预测值。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://cyx0706.github.io/categories/AI/"/>
    
    
    <category term="AI+X Blending" scheme="http://cyx0706.github.io/tags/AI-X-Blending/"/>
    
  </entry>
  
  <entry>
    <title>aix</title>
    <link href="http://cyx0706.github.io/2021/03/04/optimization/"/>
    <id>http://cyx0706.github.io/2021/03/04/optimization/</id>
    <published>2021-03-04T09:48:49.000Z</published>
    <updated>2021-04-24T03:48:10.771Z</updated>
    
    <content type="html"><![CDATA[<p>2021/3/9 补充了数学层面的牛顿公式理解和在求解最小二乘问题中的应用</p><h1 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h1><p>一个优化问题包含三个部分：</p><ul><li>优化的参数</li><li>损失函数</li><li>约束（可选）</li></ul><p>优化问题实际上就是寻找参数来使得在约束的范围内损失函数最小。<br><a id="more"></a></p><h2 id="LEAST-SQUARES-PROBLEMS"><a href="#LEAST-SQUARES-PROBLEMS" class="headerlink" title="LEAST SQUARES PROBLEMS"></a>LEAST SQUARES PROBLEMS</h2><p>最小二乘问题是很经典的优化问题。</p><p>考虑这样一个线性方程组：</p><script type="math/tex; mode=display">Mx=b</script><p>我们将解一个线性方程组变化为一个无约束的最小值问题：</p><script type="math/tex; mode=display">x^* = arg \ min \left \| Mx-b \right \| _2^2</script><p>这就是一个线性的最小二乘问题。虽然线性方程组仅在某些条件下具有解决方案，如条件不能约束过度，但最小值问题在过度约束下依旧是有解的，并且可以直接计算等式来获得：</p><script type="math/tex; mode=display">x^* = (M ^ \top M)^{-1} M^ \top b</script><p>我们通常记 $(M^ \top M)^{-1} M^\top$ Moore-Penrose 逆矩阵，也通常将其简记为 $M^\dagger$ （M dagger 还真就是匕首）</p><h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><p>迭代方法包括 3 个主要部分：</p><ul><li>解的初始猜想</li><li>每一步的迭代方法</li><li>收敛的标准</li></ul><p>梯度下降是一种求最优解的迭代方法，它设置了初始猜想值为$x_0$，每一步都向下降最快的方向前进一步（负梯度），当当前的 x 的梯度为 0 或者接近 0 时，算法停止。</p><ol><li>初始值$k=0$，假设初始最优解为$x_0$</li><li>更新 x: <script type="math/tex; mode=display">x_{k+1}=x_k- \alpha \nabla J(x_k)</script></li><li>if $\left |\nabla J(x_{k+1})  \right | &lt; \tau $ 停止，$x_{k+1}$为最优解，否则令$k=k+1$，重复第二步。<br>其中，$J(x)$ 是损失函数，$\nabla J(x_k)$是它的梯度，一般来说，梯度可以定义为一个函数的全部偏导数构成的向量（这一点与偏导数与方向导数不同，两者都为标量）。$\alpha$ 是步长，$\tau$ 是一个定义的允许的误差值。</li></ol><ul><li>梯度下降得到的是局部最优解，一个局部最小值（local minimum），而不一定是全局的最小值（global minimum）</li><li>梯度下降的性能和表现取决于它的步长，如果步长太大，会导致无法收敛；而小的步长会增加迭代的次数，增加开销。</li><li>梯度下降需要我们能够写出来损失函数和计算损失函数的梯度，有的时候我们无法找到一个合适的损失函数去评估或者无法计算其梯度。</li></ul><h2 id="Newton’s-Method"><a href="#Newton’s-Method" class="headerlink" title="Newton’s Method"></a>Newton’s Method</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>牛顿法同样也是一种迭代方法。它和梯度下降很相似。即我们在寻找$\nabla J(x)=0$的时候采用 Newton-Raphson 寻根法。</p><ol><li>初始值$k=0$，假设初始最优解为$x_0$</li><li>更新 x: <script type="math/tex; mode=display">x_{k+1}=x_k- \alpha_k [H(x_k)]^{-1} \nabla J(x_k)</script></li><li>if $\left |\nabla J(x_{k+1})  \right | &lt; \tau $ 停止，$x_{k+1}$为最优解，否则令$k=k+1$，重复第二步。<br>其中，$J(x)$ 是损失函数，$\nabla J(x_k)$是它的梯度，$\alpha$ 是步长，$\tau$ 是一个定义的允许的误差值。$H(x_k)$ 是 海森矩阵（<strong>Hessian Matrix</strong>），是一个多元函数的二阶偏导数构成的方阵，描述了函数的局部曲率。</li></ol><p>用公式来描写的话就像下面这样：</p><script type="math/tex; mode=display">\nabla^2 J(x)=H(x)=\begin{bmatrix}\frac{\partial^2 J}{\partial x_1^2}  & \cdots  & \frac{\partial^2 J}{\partial x_1 \partial x_n} \\\frac{\partial^2 J}{\partial x_1 \partial x_2}  & \cdots  & \frac{\partial^2 J}{\partial x_2 \partial x_n} \\\vdots  & \vdots & \vdots \\\frac{\partial^2 J}{\partial x_1 \partial x_n}  & \cdots  & \frac{\partial^2 J}{\partial x_n^2} \\\end{bmatrix}</script><ul><li>牛顿方法是一个二阶方法，它的每次计算都要计算二阶偏导。</li><li>同样由于是迭代法，依旧只能找到局部最优解，也同样很依赖 $\alpha_j$ 的选择。但总体来看，牛顿方法可以用更少次数的迭代就使得结果收敛，但每一步的计算量很大。</li><li>沿用牛顿的方法的思路，有另外一种优化算法被称作<strong>拟牛顿法（Quasi-Newton Method）</strong>，但这些方法采用梯度信息来近似海森矩阵，所以比传统的牛顿法更加有效。</li></ul><h3 id="数学层面理解"><a href="#数学层面理解" class="headerlink" title="数学层面理解"></a>数学层面理解</h3><p>牛顿公式：</p><script type="math/tex; mode=display">x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)}</script><p>若要求原函数的极小值，即在$f’(x)$上寻找根，这样就可以应用牛顿方法：</p><script type="math/tex; mode=display">x_{k+1} = x_k - \frac{f ^ {(1)} (x_k)}{f ^{(2)} (x_k)}</script><p>在高维空间，将一阶导换成梯度，二阶导换成海森矩阵就得到了前面的式子。</p><h3 id="例子：两者的比较"><a href="#例子：两者的比较" class="headerlink" title="例子：两者的比较"></a>例子：两者的比较</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 博客优化问题部分对应的例子: 梯度下降法和牛顿方法的比较</span></span><br><span class="line"><span class="comment"># 部分代码参考: https://zhuanlan.zhihu.com/p/92359902</span></span><br><span class="line"><span class="comment"># f(x) = x^T * Q * x</span></span><br><span class="line"><span class="comment"># Q = [[q1, 0], [0, q2]]</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">q1 = <span class="number">1</span></span><br><span class="line">q2 = <span class="number">10</span></span><br><span class="line">Q = [</span><br><span class="line">    [q1, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, q2]</span><br><span class="line">]</span><br><span class="line">Q = np.array(Q)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (x.T.dot(Q)).dot(x)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">g</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">2</span> * x.T.dot(Q)).T</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">H</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * Q</span><br><span class="line">alpha = <span class="number">0.1</span>  <span class="comment"># 步长</span></span><br><span class="line">x0 = np.array([<span class="number">10</span>, <span class="number">1</span>])</span><br><span class="line">steps = <span class="number">30</span></span><br><span class="line">gd_results = []</span><br><span class="line">newton_results = []</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span>(<span class="params">f_gd, x, lr, max_steps, precision=<span class="number">0.0001</span>, decay=<span class="number">0.005</span></span>):</span></span><br><span class="line">    <span class="comment"># 学习率的变化公式用: lr = lr/(1+decay*i)</span></span><br><span class="line">    current_x = x</span><br><span class="line">    gd_results.append(current_x)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_steps):</span><br><span class="line">        lr = lr / (<span class="number">1</span> + decay * i)  <span class="comment"># 调整步长</span></span><br><span class="line">        gd = f_gd(current_x)</span><br><span class="line">        <span class="keyword">if</span> np.linalg.norm(gd) &lt; precision:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            current_x = current_x - lr * gd</span><br><span class="line">            gd_results.append(current_x)</span><br><span class="line">    <span class="keyword">return</span> current_x</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">newtons_method</span>(<span class="params">f_gd, f_H, lr, x, max_steps, precision=<span class="number">0.0001</span>, decay=<span class="number">0.005</span></span>):</span></span><br><span class="line">    newton_results.append(x)</span><br><span class="line">    hessian = f_H(x)</span><br><span class="line">    hessian_reverse = np.linalg.inv(hessian)</span><br><span class="line">    H_G = np.matmul(hessian_reverse, f_gd(x))</span><br><span class="line">    current_x = x - lr * H_G</span><br><span class="line">    newton_results.append(current_x)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, max_steps):</span><br><span class="line">        lr = lr / (<span class="number">1</span> + decay * i)</span><br><span class="line">        gd = f_gd(current_x)</span><br><span class="line">        <span class="keyword">if</span> np.linalg.norm(gd) &lt; precision:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            hessian_reverse = np.linalg.inv(hessian)</span><br><span class="line">            H_G = np.matmul(hessian_reverse, gd)</span><br><span class="line">            current_x = current_x - lr * H_G</span><br><span class="line">            newton_results.append(current_x)</span><br><span class="line">    <span class="keyword">return</span> current_x</span><br><span class="line"><span class="built_in">print</span>(gradient_descent(g, x0, alpha, steps))</span><br><span class="line"><span class="built_in">print</span>(newtons_method(g, H, alpha, x0, steps))</span><br><span class="line">gd_results = np.array(gd_results)</span><br><span class="line">newton_results = np.array(newton_results)</span><br><span class="line"><span class="comment"># 生成网格图</span></span><br><span class="line">X1, X2 = np.meshgrid(np.linspace(-<span class="number">10</span>, <span class="number">10</span>, <span class="number">100</span>), np.linspace(-<span class="number">10</span>, <span class="number">10</span>, <span class="number">100</span>))</span><br><span class="line">obj_results = np.zeros((<span class="number">100</span>, <span class="number">100</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        obj_results[i][j] = f(np.array(([X1[i][j], X2[i][j]])))</span><br><span class="line"><span class="comment"># cmap可以调用cm颜色库中的填充颜色</span></span><br><span class="line">pic_color = plt.contourf(X1, X2, obj_results, <span class="number">20</span>, cmap=<span class="string">&quot;Reds&quot;</span>)</span><br><span class="line"><span class="comment"># 绘制等高线</span></span><br><span class="line">pic = plt.contour(X1, X2, obj_results, <span class="number">20</span>, linewidths=<span class="number">1.5</span>)</span><br><span class="line"><span class="comment"># plt.clabel(pic)</span></span><br><span class="line">plt.colorbar(pic_color)  <span class="comment"># ticks 参数可以省略</span></span><br><span class="line">plt.plot(<span class="built_in">list</span>(gd_results[:, <span class="number">0</span>]), <span class="built_in">list</span>(gd_results[:, <span class="number">1</span>]), <span class="string">&quot;-or&quot;</span>, linewidth=<span class="number">2</span>, label=<span class="string">&quot;Gradient Descent&quot;</span>)</span><br><span class="line">plt.plot(<span class="built_in">list</span>(newton_results[:, <span class="number">0</span>]), <span class="built_in">list</span>(newton_results[:, <span class="number">1</span>]), <span class="string">&quot;-ob&quot;</span>, linewidth=<span class="number">2</span>, label=<span class="string">&quot;Newton&#x27;s Method&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;x1&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;x2&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"><span class="comment"># plt.savefig(&quot;minimum_searching.png&quot;)</span></span><br><span class="line">plt.savefig(<span class="string">&quot;minimum_searching2.png&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>在没有步长衰减的情况下，得到的图像如下：<br><img src="https://i.loli.net/2021/03/07/UWvIx2entByKNjb.png" alt="minimum_searching.png"></p><p>牛顿法显然更好，完全避免了左右横跳，因为牛顿法能看的更远，牛顿法有函数的二阶导数的信息，不仅知道哪一点可以下降的最快，而且还知道从这个点走一步后得到的下一个点的梯度的大小。这就解释了为什么牛顿法会避免”之”字形，因为之字形当前的梯度虽然比较大，但是下一步的梯度不大，而如果顺着”山脊”走，可能当前的梯度没有”之字形”的梯度大，但是下一步可能的梯度会更大一点。</p><p>如果加入步长衰减，适当增加最大步数，得到结果如下：<br><img src="https://i.loli.net/2021/03/07/jQymA91rpLVYaH5.png" alt="minimum_searching2.png"></p><p>明显的梯度下降方法准确度上升很多，不再是在最小值点左右横跳无法收敛了。但相比之下对牛顿方法没有太大的影响。</p><h2 id="Nonlinear-Least-Squares"><a href="#Nonlinear-Least-Squares" class="headerlink" title="Nonlinear Least Squares"></a>Nonlinear Least Squares</h2><h3 id="概念-1"><a href="#概念-1" class="headerlink" title="概念"></a>概念</h3><p>假设我们有模型 f，输入 z 和 x 参数。$y = f(z, x) x \in \mathbb{R}^n$，这样一个模型既可以是一个线性函数，也可以是一个非线性函数：</p><ul><li>如：f 是多项式函数，x 是 f 的系数。</li><li>如：z 是初始时刻在一个导热棒上温度的分布情况，y 是最终最大温度值。f 涉及模拟热传导方程来计算 y 值，x 可以被定义为在这个物体上的热扩散率。</li></ul><p>我们要对这个函数做参数的优化，为了找到最好的最适合当前数据集的 x，我们先收集在 m 个输入下的 m 个点 $(x_1, y_1), (x_2, y_2), \cdots, (x_m, y_m)$， 用这些点去寻找最优的 x</p><p>对于这个优化问题，损失函数怎么定义？对于每个函数，我们考察预测值和实际值的误差：$r_i(x)=f(z_i, x)-y_i$，把所有的误差相加得到我们的误差函数（也就是优化的目标函数）</p><script type="math/tex; mode=display">J(x) = \sum_{i=1}^{m} r_i(x)^2 = \sum_{i=1}^{m} (f(z_i, x)-y_i)^2</script><p>这个问题被称为非线性最小二乘问题，由于目标函数是一个非线性函数。解这个问题当然还是可以使用迭代方法，相当于求解如下优化问题：</p><script type="math/tex; mode=display">\min_{x \in \mathbb{R}^d} \sum_{i=1}^{m}(f(z_i, x)-y_i)^2</script><p>这个函数也被称为<strong>经验风险最小化</strong>函数。对于梯度下降方法，我们假设初始值是 x_0，步长为 $\alpha$，根据我们前面学的迭代公式：</p><script type="math/tex; mode=display">x_{n+1}=x_n-\alpha \nabla J(x_n)</script><p>上面目标函数的梯度公式为：</p><script type="math/tex; mode=display">\nabla J(x_n) = \sum_{i=1}^{m}2(f(z_i, x_n)-y_i)\nabla_x f(z_i, x_n)</script><p>其中 $\nabla_x f(z_i, x_n)$ 是 f 在 x 方向上的梯度：</p><script type="math/tex; mode=display">\nabla_x f(z_i, x_n)=\left [ \frac{\partial f}{\partial x_i}(z_i, x_n), \cdots , \frac{\partial f}{\partial x_d}(z_i, x_n) \right ]^T</script><p>为什么我们不直接对原函数采用梯度下降等方法来寻找最小值？ 因为$\nabla f(x)$ 并不能确保我们找到了局部最小值，梯度为0既可能是最小值，也有可能是最大值，我们无法根据一阶倒数或者偏导来确定。</p><h2 id="Applied-in-Least-Squares"><a href="#Applied-in-Least-Squares" class="headerlink" title="Applied in Least-Squares?"></a>Applied in Least-Squares?</h2><h3 id="Gradient-Descent-1"><a href="#Gradient-Descent-1" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><p>如果采用梯度下降方法去优化最小二乘问题的目标函数：$f(x) = \left | Mx-b \right | _2^2$</p><p>代入梯度下降公式，得到的递推式如下：</p><script type="math/tex; mode=display">x_{k+1}=x_k- 2 \alpha M^T (Mx_k-b)</script><h3 id="Newton’s-Method-1"><a href="#Newton’s-Method-1" class="headerlink" title="Newton’s Method"></a>Newton’s Method</h3><p>如果采用牛顿方法，令$\alpha = 1$</p><script type="math/tex; mode=display">x_{k+1}=x_k- (\nabla^2 f(x_k))^{-1} \nabla f(x_k)</script><p>我们前面知道，最小二乘问题的最优解为 $x^* = (M ^ \top M)^{-1} M^ \top b$</p><p>又计算得出 $\nabla f(x_k)=2M^T (Mx_k-b)$，$\nabla^2 f(x_k)=2M^TM$，代入上式可得：</p><script type="math/tex; mode=display">x_{k+1}=x_k-\frac{1}{2}(M^T M)^{-1} * 2M^T (Mx_k-b) = (M ^ \top M)^{-1} M^ \top b</script><p>一步到位直接求得极值点。这是从数学推导角度来看的，如果从二阶导方面来理解，牛顿方法包含二阶倒数的近似值来进行迭代计算，而最小二乘问题的目标函数一个二阶的，所以精确的用一步可以找到解。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;2021/3/9 补充了数学层面的牛顿公式理解和在求解最小二乘问题中的应用&lt;/p&gt;
&lt;h1 id=&quot;Optimization&quot;&gt;&lt;a href=&quot;#Optimization&quot; class=&quot;headerlink&quot; title=&quot;Optimization&quot;&gt;&lt;/a&gt;Optimization&lt;/h1&gt;&lt;p&gt;一个优化问题包含三个部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;优化的参数&lt;/li&gt;
&lt;li&gt;损失函数&lt;/li&gt;
&lt;li&gt;约束（可选）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;优化问题实际上就是寻找参数来使得在约束的范围内损失函数最小。&lt;br&gt;</summary>
    
    
    
    <category term="AI" scheme="http://cyx0706.github.io/categories/AI/"/>
    
    
    <category term="AI+X Blending" scheme="http://cyx0706.github.io/tags/AI-X-Blending/"/>
    
  </entry>
  
  <entry>
    <title>aix</title>
    <link href="http://cyx0706.github.io/2021/03/02/pde/"/>
    <id>http://cyx0706.github.io/2021/03/02/pde/</id>
    <published>2021-03-02T03:16:16.000Z</published>
    <updated>2021-03-04T09:44:31.075Z</updated>
    
    <content type="html"><![CDATA[<h1 id="偏微分方程"><a href="#偏微分方程" class="headerlink" title="偏微分方程"></a>偏微分方程</h1><ul><li><del>学的时候记得笔记都是英语的加上又懒得翻译就只好拖到了现在</del></li></ul><h2 id="General"><a href="#General" class="headerlink" title="General"></a>General</h2><ul><li>偏微分方程（PDE, partial differential equation）的解是一个关于时间和空间的方程 $u(x, t)$，我们用 u 代表在特定时间和空间的函数值。<a id="more"></a></li><li>在常微分方程的数值解中，我们是通过将 0-T 划分为若干个 $\Delta t$，通过计算每个 $\Delta t$ 位置的值来计算 $u(t)$。在偏微分方程中，我们构造下面的图形（mesh）来辅助我们的计算：</li></ul><p><img src="https://i.loli.net/2021/03/01/MlCWKkFL3YTDvr2.png" alt="mesh"></p><p>我们用 mesh 网格图上的 u 值来近似计算偏微分的值，我们观察临近的矩形，他们构成偏导的近似值。这样我们将偏微分方程的空间离散化，转换为常微分方程的系统。</p><p>空间离散化的方案(spatial discretization schemes)有：</p><ul><li>有限元法（Finite Element Methods）</li><li>有限差分法（Finite Difference Methods）</li><li>有限体积法（Finite Volume Methods）</li></ul><h2 id="Finite-Difference-Formulas"><a href="#Finite-Difference-Formulas" class="headerlink" title="Finite Difference Formulas"></a>Finite Difference Formulas</h2><p>我们以一个简单的偏微分方程为例：</p><script type="math/tex; mode=display">\frac{\partial u}{\partial t} = k \frac{\partial^2 u}{\partial x^2}</script><p>k 代表导热系数。</p><p>假设网格在空间上的距离为 h，那么我们有:</p><ul><li>一阶偏导的前向差分(Forward difference for first spatial derivative)<script type="math/tex; mode=display">\frac{\partial u}{\partial x} \Big|_{x_i} \approx \frac{u(x_{i+1}, t)-u(x_i, t)}{h}</script></li><li>一阶偏导的向后差分(Backward difference for first spatial derivative)<script type="math/tex; mode=display">\frac{\partial u}{\partial x} \Big|_{x_i} \approx \frac{u(x_{i}, t)-u(x_{i-1}, t)}{h}</script></li><li>一阶偏导的中心差分(Central difference for first spatial derivative)<script type="math/tex; mode=display">\frac{\partial u}{\partial x} \Big|_{x_i} \approx \frac{u(x_{i+1}, t)-u(x_{i-1}, t)}{2h}</script></li><li>二阶偏导的中心差分(Central difference for second spatial derivative)<script type="math/tex; mode=display">\frac{\partial^2 u}{\partial x^2} \Big|_{x_i} \approx \frac{\frac{u(x_{i+1}, t)-u(x_{i}, t)}{h}-\frac{u(x_{i}, t)-u(x_{i-1}, t)}{h}}{h} \\\quad \quad \quad \quad  \approx \frac{u(x_{i+1}, t)-2u(x_{i}, t)+u(x_{i-1}, t)}{h^2}</script></li></ul><h2 id="Discretization-Stencil"><a href="#Discretization-Stencil" class="headerlink" title="Discretization Stencil"></a>Discretization Stencil</h2><ul><li>我们在网格来近似计算空间函数$u(x, t)$偏导（前向差分，中心差分等等）的这种方法叫做<code>stencil</code>，如果用空间内更多更远的点来近似一个点的偏导值，那么一般情况下误差就会变的更小。但对于有些情况却不同，补充的来说，用更多的点近似来减少误差只对光滑的函数有效，对于那些不连续的函数，相邻点之间的关系很小，使用太多的点反而会影响准确度。</li><li>我们用 <code>stencils size</code> 来描述有多少个点被用于近似偏导值。我们的误差和使用的近似方法的阶数成正比关系。我们假设误差为 e，h 为 网格缩小比例，q 是使用方法的阶数，我们有这样的关系：$e \sim h^q$</li><li>需要注意：减小空间网格的大小的同时也要减小每一步的时间$\Delta t$来确保稳定性。</li></ul><h2 id="Solving-PDE"><a href="#Solving-PDE" class="headerlink" title="Solving PDE"></a>Solving PDE</h2><ul><li>解决 PDE 问题就是解决一个初始边界值问题（initial boundary value problem）：$u(x, t)$ 在给定的初始条件和边界条件下是如何变化的。</li></ul><p>我们仍以热传导方程$\frac{\partial u}{\partial t} = k \frac{\partial^2 u}{\partial x^2}$为例。在网格内部的点上，我们可以将求偏导转换为计算一组离散化的点，运用我们上面的二阶中心差分的近似计算：</p><script type="math/tex; mode=display">\frac{\partial u}{\partial t} \Big|_{x_i} = k \frac{u(x_{i+1}, t)-2u(x_{i}, t)+u(x_{i-1}, t)}{h^2}</script><p>我们想象 i 从 0 到 N-1，总共有 N-1 个类似上面的式子，即我们把 t 时间的 x 范围划分为 N-1 块，每一块都可以通过它临近的点来计算。下面把这 N-1 个式子合成在一起：</p><script type="math/tex; mode=display">u(t)=\begin{bmatrix} u(x_1, t)\\ u(x_2, t)\\ \vdots \\ u(x_{N-1}, t)\\\end{bmatrix}</script><p>这样上面的方程就可以写成类似下面的形式：</p><script type="math/tex; mode=display">\frac{du}{dt}=Au+f</script><p>f包含边界条件的信息。A 是一个 $N-1 \times N-1$ 的系数矩阵，当x=1和x=N-1的时候都无法计算，所以设置行列式为全 0，需要 f 来辅助确定值以保证边界条件：</p><script type="math/tex; mode=display">\frac{k}{h^2}\begin{bmatrix}  &0  &0  &0  &\cdots &0 &0 \quad \\  &1  &-2  &1  &\cdots &0 &0 \quad \\  &0  &1  &-2  &1  &\cdots &0 \quad \\  &\vdots  &\vdots  &\vdots  &\vdots &\vdots &0 \quad \\  &0  &0  &\cdots &1  &-2  &1 \quad \\  &0  &0  &0  &\cdots &0 &0 \quad\end{bmatrix}\hspace{-0.5cm}\left.\begin{matrix}  & \\  & \\  & \\  & \end{matrix}\right\}N-1</script><p>这样我们就可以用 ODE 的方法来解决 PDE 问题，比如说我们运用前向欧拉法：</p><script type="math/tex; mode=display">\boldsymbol{u}^{n+1}=\boldsymbol{u}^{n}+\Delta t(A \boldsymbol{u}^n+f)</script><blockquote><p> We discretize the spatial domain into N spaces with N+1 grid points. How many unknown variables are evolved in the resulting system of ODEs?</p></blockquote><p>答案是 <strong>N-1</strong>，$u_0$ 和 $u_{N+1}$ 都是边界点（已知），这样在网格的内部就只有 N-1 个未知的点。</p><p>下面我们考虑计算的误差：PDE 方程的解的精确度分为空间和时间上的。我们取 e 代表误差，$\Delta t $ 代表每一步选择迭代的时间差(time_step)，h 代表相邻 x 之间的距离，也可以看做网格大小(mesh_size)，那么有关系：$e \approx (\Delta t)^p+h^q$。其中 p 和 q 分别是我们选择的 ODE 方法的阶数和偏导数值解方法的阶数。如我们选择 FE (forward eluer) + 中心差分（Central Difference），那么 p 值为 1，q 值为 2。因为它们分别是 1 阶和 2 阶精确度的方法。如果 h 减小一半，那么误差就会变为四分之一。</p><p>需要注意，我们前面也提到了，一个小的 h 需要同时一个更小的 $\Delta t$，否则求解将会变得不再稳定。怎么理解呢，我们仍用热传导方程来理解：当我们观察更小的一段的变化的时候，不同段直接的交互会更加的频繁，我们需要用更小的时间来确保可以更加精确的捕获这些变化。</p><h2 id="Boundary-Conditions"><a href="#Boundary-Conditions" class="headerlink" title="Boundary Conditions"></a>Boundary Conditions</h2><ul><li>Dirichlet conditions<blockquote><p>Dirichlet conditions fix the value of the solution u(x,t) at the boundary. This is like pinning the solution to a certain value at the boundary.</p></blockquote></li></ul><p>即在边界指定函数的分布形式</p><ul><li>Neumann conditions<blockquote><p> Neumann conditions fix the value, not of the solution, but of its spatial derivative $\frac{\partial u}{\partial x}$ at the boundary. This is like fixing the stresses at the boundary.</p></blockquote></li></ul><p>在边界指定外法线方向上的导数的数值</p><ul><li>Robin conditions<blockquote><p>Robin conditions fix the value of a linear combination of the solution itself and its partial derivative at the boundary.</p></blockquote></li></ul><p>可以看做是第1和2条件的组合，要求偏导和函数本身的数值。</p><h2 id="Linear-System"><a href="#Linear-System" class="headerlink" title="Linear System"></a>Linear System</h2><p>对于转换为 ODE 的 PDE 问题，我们也可以用一些隐式的方法求解，如隐式欧拉法</p><script type="math/tex; mode=display">u^{n+1}=u^n+\Delta t (Au^{n+1}+f)</script><p>求解这个方法，移项：</p><script type="math/tex; mode=display">(1-\Delta tA)u^{n+1}=u^n+\Delta tf</script><p>它的每一步相当于求解一个线性系统（linear system）（这不就是线性方程组吗），$M \boldsymbol u=b$<br>这么做的好处在于，这样让我们可以使用更大的 $\Delta t$ 相比较于显式方法（隐式方法的稳定性，参考 ODE 那节）</p><p>求解线性系统有很多的现成方法：</p><ul><li>A 是一般矩阵（general）：高斯消元（Gaussian elimination），$O(N^3)$</li><li>A 是三角矩阵（triangular）：Forward substitution/backward substitution，$O(N^2)$</li><li>A 是三对角线矩阵（tri-diagonal）：托马斯算法（Thomas algorithm），$O(N)$</li></ul><h3 id="Iterative-Algorithms-for-Linear-Systems"><a href="#Iterative-Algorithms-for-Linear-Systems" class="headerlink" title="Iterative Algorithms for Linear Systems"></a>Iterative Algorithms for Linear Systems</h3><p>当线性系统中的 M 是一个很大的矩阵，又不特殊时，用高斯消元太慢了并且太占用空间。这时就需要一个更高效的方法：迭代法再次上线。我们先猜一个初始值 v，通过计算 Mv 和 b 比较，修改 v 来一步步逼近真实解。</p><p>可选的迭代方法有很多，如 Krylov Subspace Method（krylov 子空间算法），conjugate gradients（共轭梯度法），GMRes（广义最小残差法）。</p><p><del>学一下共轭梯度法然后补充在这里</del></p><h2 id="Nonlinear-Systems"><a href="#Nonlinear-Systems" class="headerlink" title="Nonlinear Systems"></a>Nonlinear Systems</h2><p>对于如下非线性形式的 ODEs：</p><script type="math/tex; mode=display">\frac{du}{dt}=f(u)</script><p>使用隐式欧拉法得到迭代公式：</p><script type="math/tex; mode=display">u_{n+1}=u_n+\Delta tf(u_{n+1})</script><p>求解$u_{n+1}$等同于在这样一个方程中寻找根:$R(u_{n+1})=u_{n+1}-\Delta tf(u_{n+1})=0$，常用的计算方法有：二分法，牛顿法</p><h2 id="Sample"><a href="#Sample" class="headerlink" title="Sample"></a>Sample</h2><p>我们考虑污染物在一维空间的传播问题。令 $u(x, t)$ 表示在 t 时间 x 位置处污染物的浓度。u 的变化用对流扩散方程来定义：</p><script type="math/tex; mode=display">\frac{\partial u}{\partial t} = k \frac{\partial^2 u}{\partial x^2} + c\frac{\partial u}{\partial x}</script><p>扩散速率$k = 5 \times 10^{-4} m^2/s$，对流速度$ c= 0.5m/s$<br>当 $t=0$ 时污染物的分布图像为:</p><p><img src="https://i.loli.net/2021/03/04/UomdbXjc4OF9aeR.png" alt="init_ux.png"></p><p>边界条件是第一类边界条件，在边界的时候$u(x，t)$始终等于 0。</p><p>我们采用有限差分法来计算污染物在 $[0, 1]$ 区间内随着时间从 0 变到 1 的变化。首先将偏导全部拆分近似为差分的形式：</p><script type="math/tex; mode=display">\frac{\partial u}{\partial t} \Big|_{x_i} = \frac{k}{h^2}u(x_{i+1}, t) + \frac{-2k+ch}{h^2}u(x_i, t) + \frac{k-ch}{h^2}u(x_{i-1}, t)</script><p>然后用上面的方法转化为求 ODE 问题，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">u0</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="comment"># 高斯函数</span></span><br><span class="line">    mean = <span class="number">0.2</span></span><br><span class="line">    sigma = <span class="number">0.025</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1</span>/<span class="number">160</span>)*np.exp(-<span class="number">1</span> * ((x - mean) ** <span class="number">2</span>) / (<span class="number">2</span> * (sigma ** <span class="number">2</span>))) / (math.sqrt(<span class="number">2</span> * np.pi) * sigma)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># init_ux():</span></span><br><span class="line"><span class="comment"># xs = np.linspace(0, 1, 1000)</span></span><br><span class="line"><span class="comment"># ys = [u0(x) for x in xs]</span></span><br><span class="line"><span class="comment"># plt.plot(xs, ys, &quot;b-&quot;)</span></span><br><span class="line"><span class="comment"># plt.ylabel(r&quot;$u_0(x)$&quot;)</span></span><br><span class="line"><span class="comment"># plt.yticks([round(x, 2) for x in np.linspace(0, 0.1, 10)])</span></span><br><span class="line"><span class="comment"># plt.savefig(&quot;init_ux.png&quot;)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line">sample_num = <span class="number">5</span></span><br><span class="line">nSpace = <span class="number">600</span></span><br><span class="line">nTime = <span class="number">1000</span></span><br><span class="line">xs = np.linspace(<span class="number">0</span>, <span class="number">1</span>, nSpace-<span class="number">1</span>)</span><br><span class="line">k = <span class="number">5e-4</span></span><br><span class="line">c = -<span class="number">0.5</span></span><br><span class="line">dt = <span class="number">1</span>/nTime</span><br><span class="line">h = <span class="number">1</span>/nSpace</span><br><span class="line">A = np.zeros((nSpace-<span class="number">1</span>, nSpace-<span class="number">1</span>))</span><br><span class="line">u = np.zeros((nSpace-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nSpace-<span class="number">1</span>):</span><br><span class="line">    u[i][<span class="number">0</span>] = u0(i*h)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, nSpace-<span class="number">1</span>-<span class="number">1</span>):</span><br><span class="line">    A[i][i-<span class="number">1</span>] = k / (h * h) - c / h</span><br><span class="line">    A[i][i] = (-<span class="number">2</span> * k) / (h * h) + c / h</span><br><span class="line">    A[i][i+<span class="number">1</span>] = k / (h * h)</span><br><span class="line"><span class="comment"># 采用欧拉方法</span></span><br><span class="line">times = <span class="number">0</span></span><br><span class="line">t = <span class="number">0</span></span><br><span class="line">u_record = [u.T.tolist()[<span class="number">0</span>]]</span><br><span class="line"><span class="keyword">while</span> t &lt;= <span class="number">1</span>:</span><br><span class="line">    u = u + dt * A.dot(u)</span><br><span class="line">    t += dt</span><br><span class="line">    times += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 每 200 次迭代记录一次</span></span><br><span class="line">    <span class="keyword">if</span> times == nTime/sample_num:</span><br><span class="line">        u_record.append(u.T.tolist()[<span class="number">0</span>])</span><br><span class="line">        times = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 采用二阶龙格库塔法</span></span><br><span class="line">u = np.zeros((nSpace-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nSpace-<span class="number">1</span>):</span><br><span class="line">    u[i][<span class="number">0</span>] = u0(i*h)</span><br><span class="line">t = <span class="number">0</span></span><br><span class="line">times = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> t &lt;= <span class="number">1</span>:</span><br><span class="line">    k1 = A.dot(u)</span><br><span class="line">    k2 = A.dot(u+k1*dt)</span><br><span class="line">    u = u + <span class="number">0.5</span> * dt * (k1 + k2)</span><br><span class="line">    t += dt</span><br><span class="line">    times += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> times == nTime/sample_num:</span><br><span class="line">        u_record.append(u.T.tolist()[<span class="number">0</span>])</span><br><span class="line">        times = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(u_record))</span><br><span class="line">values_fe = [<span class="built_in">int</span>(i*<span class="number">250</span>/<span class="number">10</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br><span class="line">values_rk2 = [<span class="built_in">int</span>(i*<span class="number">250</span>/<span class="number">10</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br><span class="line">colors_fe = [<span class="string">&quot;#%02x%02x%02x&quot;</span> % (<span class="number">120</span>, <span class="built_in">int</span>(g), <span class="number">200</span>)<span class="keyword">for</span> g <span class="keyword">in</span> values_fe]</span><br><span class="line">colors_rk2 = [<span class="string">&quot;#%02x%02x%02x&quot;</span> % (<span class="number">200</span>, <span class="built_in">int</span>(g), <span class="number">40</span>)<span class="keyword">for</span> g <span class="keyword">in</span> values_rk2]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(sample_num):</span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">        plt.plot(xs, u_record[i], color=<span class="string">&quot;#000080&quot;</span>, linewidth=<span class="number">2</span>, label=<span class="string">&quot;Init u&quot;</span>)</span><br><span class="line">    <span class="keyword">elif</span> i == <span class="number">1</span>:</span><br><span class="line">        plt.plot(xs, u_record[i], color=colors_fe[i], linewidth=<span class="number">2</span>, label=<span class="string">&quot;FE Method&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        plt.plot(xs, u_record[i], color=colors_fe[i], linewidth=<span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(sample_num):</span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">        plt.plot(xs, u_record[sample_num + i], color=colors_rk2[i], linewidth=<span class="number">2</span>, label=<span class="string">&quot;RK2 Method&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        plt.plot(xs, u_record[sample_num + i], color=colors_rk2[i], linewidth=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&quot;Evolution of u&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;x&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;u(x, t)&quot;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;best&quot;</span>)</span><br><span class="line">plt.savefig(<span class="string">&quot;ux.png&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>向后差分 + 显式欧拉 和 向后差分 + 二阶龙格库塔 方法得到的预测结果，颜色深度的变化代表时间的变化，绘制的图形为每隔 200ms 后在 x 上污染物的分布情况。：</p><p><img src="https://i.loli.net/2021/03/04/H8d4zWL9faSciuU.png" alt="ux_FE.png"></p><p>FE 误差为 8% 左右，如果我们使用 RK2 代替 FE 方法，误差减小到 0.8%。（用的黑盒函数测的误差，暂时还没复现解析解）</p><p>可以尝试修改 nSpace 和 nTime 观察效果。当切换方法为向前差分时，很难让结果稳定，对 nSpace 和 nTime 的要求很苛刻。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;偏微分方程&quot;&gt;&lt;a href=&quot;#偏微分方程&quot; class=&quot;headerlink&quot; title=&quot;偏微分方程&quot;&gt;&lt;/a&gt;偏微分方程&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;del&gt;学的时候记得笔记都是英语的加上又懒得翻译就只好拖到了现在&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;General&quot;&gt;&lt;a href=&quot;#General&quot; class=&quot;headerlink&quot; title=&quot;General&quot;&gt;&lt;/a&gt;General&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;偏微分方程（PDE, partial differential equation）的解是一个关于时间和空间的方程 $u(x, t)$，我们用 u 代表在特定时间和空间的函数值。</summary>
    
    
    
    <category term="AI" scheme="http://cyx0706.github.io/categories/AI/"/>
    
    
    <category term="AI+X Blending" scheme="http://cyx0706.github.io/tags/AI-X-Blending/"/>
    
  </entry>
  
  <entry>
    <title>aix</title>
    <link href="http://cyx0706.github.io/2021/01/28/ode/"/>
    <id>http://cyx0706.github.io/2021/01/28/ode/</id>
    <published>2021-01-28T14:21:04.000Z</published>
    <updated>2021-02-23T11:23:53.920Z</updated>
    
    <content type="html"><![CDATA[<p>正式开课了，这里仅是我的学习记录。<br><a id="more"></a></p><h1 id="Module-1"><a href="#Module-1" class="headerlink" title="Module 1"></a>Module 1</h1><h2 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h2><blockquote><p>Now, engineering and science applications may, as you know, have very different demands. Accuracy and reliability may be paramount(=very important). Large data sets might or might not be available. And we might need to use prior physical knowledge with new data-driven insights. We need to really combine physical modeling with what data can tell us.</p></blockquote><p>The computational paradigm change over centuries: </p><ul><li>The experiment based model such as Newton’s Laws of Motion, we just observe carefully and distill the pricinple from thousands of experiments.</li><li>The theory based model. It describe the physical world with a great deal of predictive power, accuary, and generalizability.</li><li>The computational modeling and simulation. Based on the ideas that we use the computer to help us broaden the theory based model.</li><li>The data-driven model. Using the machine learning method which gives us powerful ways of rebuilding models together with making predictions.</li></ul><h1 id="Module-2：常微分方程"><a href="#Module-2：常微分方程" class="headerlink" title="Module 2：常微分方程"></a>Module 2：常微分方程</h1><blockquote><p>they are equations that describe how things vary in time.</p><p>they are equation that describe how certain things vary in space.</p><p>they can be understood as a very deep limit of a recurrent neural network.</p><p>……</p></blockquote><p>常微分方程的一般形式是</p><script type="math/tex; mode=display">\frac{du}{dt} = f(u,t)</script><p>其中 u 代表状态向量(state vector) 而 t 代表时间。</p><h2 id="计算机求解常微分方程"><a href="#计算机求解常微分方程" class="headerlink" title="计算机求解常微分方程"></a>计算机求解常微分方程</h2><p>补充一下泰勒展开:<br>当函数 $f(x)$ 在点 $x_0$ 处可导时，在点 $x_0$ 的邻域内恒有:</p><script type="math/tex; mode=display">f(x)=f(x_0)+f'(x_0)\frac{(x-x_0)^1}{1!}+f''(x_0)\frac{(x-x_0)^2}{2!}+...+f^{(n)}(x_0)\frac{(x-x_0)^n}{n!}</script><p><a href="https://youtu.be/3d6DsjIBzJ4">一个很好的讲解(3bBlue1Brown)</a></p><p>如果我们从本质来看，可以更好的理解泰勒展开</p><p><img src="https://i.loli.net/2021/01/30/9IuzmYCbnXpxcye.png" alt="泰勒展开.png"></p><p>如上图，我们绘制了一个函数 $f(x)$ 的导数的图像，在这个 $\frac{df}{dx}$ 图像中, $f(x)$ 的值可以用 $x$ 和这个曲线围成的面积来计算。我们要求的 $f(x)$ 由三个部分组成：</p><ul><li>$f(a)$</li><li>矩形</li><li>近似面积三角形</li></ul><p>矩形的面积很简单，为长乘宽 $(x-a)\frac{df}{dx}(a)$, 我们取的三角形为图中 $a$ 点导数与 $x$ 构成的区域, 那么三角形的面积就是 </p><script type="math/tex; mode=display">S = 0.5*h*(x-a) = 0.5*f''(a)(x-a)*(x-a) = \frac{1}{2}f''(a)(x-a)^2</script><p>这就是一个二阶的泰勒展开式，如果我们借助3阶甚至更高阶导数来进一步求面积，就会使结果更加的精确，就有了上面的泰勒展开式的形式。</p><h3 id="显式欧拉法-Forward-Euler"><a href="#显式欧拉法-Forward-Euler" class="headerlink" title="显式欧拉法(Forward-Euler)"></a>显式欧拉法(Forward-Euler)</h3><p>显示欧拉法的核心是用 $\frac{f(x_{n+1})-f(x_n)}{x_{n+1}-x_n}$ 来代替 $f’(x_n)$, 这样在一阶泰勒展开式就可以用我们已知的量来计算未知的 $f(x_n)$ 的值。为了保证计算更加的精确，同时我们将从 $x_n$ 到 $x_{n+1}$ 分成若干个 $\Delta t$ , 我们通过步步迭代来求得最终的估计值。迭代的函数如下:</p><script type="math/tex; mode=display">u_{k+1} = u_k + \Delta t f(u_k, t_k) \\ u(t_0)=u_0=f(x_0)\\ u(t_n)=f(x_n)</script><p>其中 $u_k$ 为我们的估计值，而 $t_k$ 就是当前累计迭代的 $t$ 值，由于我们忽略掉了泰勒展开二阶以后的所有量，所以我们的这个方法只有一阶的精度。</p><h3 id="隐式欧拉法-Backward-Euler"><a href="#隐式欧拉法-Backward-Euler" class="headerlink" title="隐式欧拉法(Backward-Euler)"></a>隐式欧拉法(Backward-Euler)</h3><p>和显示欧拉大部分相同，但有些微不同。不同就在于隐式欧拉法选择用 $\frac{f(x_{n+1})-f(x_n)}{x_{n+1}-x_n}$ 代替 $f’(x_{n+1})$, 这样如果对 $f(x_{n+1})$ 在 $x_n$ 处展开，就能得到一个包含未知数的方程的递推公式。</p><script type="math/tex; mode=display">u_{k+1}=u_k+\Delta t f(u_{k+1}, t_{k+1}) \\ u(t_0)=u_0=f(x_0)\\ u(t_n)=f(x_n)</script><p>隐式欧拉也等价于找上述方程的解，如果这个解有解，那么自然近似值也有解，即隐式欧拉可以更加好的确保稳定性，这对于考察一些系统的长期行为有帮助（我们会在后面的代码中说明这一点）</p><h3 id="龙格库塔法-Runge-Kutta"><a href="#龙格库塔法-Runge-Kutta" class="headerlink" title="龙格库塔法(Runge-Kutta)"></a>龙格库塔法(Runge-Kutta)</h3><p>龙格库塔方法是一种高阶的方法，注意这里的高阶并不意味着任何时候它更加精确，只是它在我们减小步长的时候更误差会降低的更小。实际上，以四阶龙格库塔法为例，当步长选的比较大的时候，它的误差甚至比显式欧拉法还大。（我们会在后面的代码中观察到这一现象）</p><p>下面介绍 RK4（4阶龙格库塔）方法，迭代公式如下:</p><script type="math/tex; mode=display">t_{k+\frac{1}{2}}=t_k+\frac{\Delta t}{2}; t_{k+1}=t_k+\Delta t \\S_1=f(u_k, t_k) \\S_2=f(u_k+\frac{\Delta t}{2}S_1, t_{k+\frac{1}{2}}) \\S_3=f(u_k+\frac{\Delta t}{2}S_2, t_{k+\frac{1}{2}}) \\S_4=f(u_k+\Delta t S_3, t_{k+1}) \\u_{k+1}=u_k+\frac{\Delta t}{6}(S_1+2S_2+3S_3+S_4)</script><p>以4阶龙格库塔方法为例，当我们减半$dt$，误差的值将会变为原来的 1/16，同样的低阶方法（如显示欧拉）只能将误差变为原来的 1/2。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>多数无益，我们用几个例子来比较一下这些方法</p><h3 id="FE-vs-BE"><a href="#FE-vs-BE" class="headerlink" title="FE vs BE"></a>FE vs BE</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显式欧拉和隐式欧拉的对比</span></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">llam = <span class="number">1</span></span><br><span class="line">u0 = <span class="number">3</span></span><br><span class="line">t_final = <span class="number">3</span> * math.pi</span><br><span class="line">dt = <span class="number">1e-1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dudt</span>(<span class="params">t, u</span>):</span></span><br><span class="line">    <span class="keyword">return</span> -llam * (u - math.cos(t)) - math.sin(t)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf</span>(<span class="params">t</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (u0 - <span class="number">1</span>) * math.exp(-llam * t) + math.cos(t)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_euler</span>(<span class="params">f, u0, dt, t_final</span>):</span></span><br><span class="line">    us = [u0]</span><br><span class="line">    u = u0</span><br><span class="line">    t = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">int</span>(t_final/dt)):</span><br><span class="line">        u = u + dt * f(t, u)</span><br><span class="line">        us.append(u)</span><br><span class="line">        t += dt</span><br><span class="line">    <span class="keyword">return</span> us</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_euler</span>(<span class="params">f, u0, dt, t_final, lam</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># newton nonlinear solver function</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">newton</span>(<span class="params">t, u, f, dt, lam</span>):</span></span><br><span class="line">        uu = u</span><br><span class="line">        g = <span class="keyword">lambda</span> x: ((x - u) / dt - f(t, x))</span><br><span class="line">        j = <span class="keyword">lambda</span> x: <span class="number">1</span> / dt + lam</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">100</span>):</span><br><span class="line">            u_next = uu - g(uu) / j(uu)</span><br><span class="line">            uu = u_next</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">abs</span>(g(u_next)) &lt;= <span class="number">1e-14</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">return</span> uu</span><br><span class="line"></span><br><span class="line">    us = [u0]</span><br><span class="line">    u = u0</span><br><span class="line">    t = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">int</span>(t_final/dt)):</span><br><span class="line">        u = newton(t+dt, u, f, dt, lam)</span><br><span class="line">        us.append(u)</span><br><span class="line">        t += dt</span><br><span class="line">    <span class="keyword">return</span> us</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">t_true = np.linspace(<span class="number">0</span>, t_final, <span class="number">1000</span>)</span><br><span class="line">u_true = [tf(t) <span class="keyword">for</span> t <span class="keyword">in</span> t_true]</span><br><span class="line">plt.plot(t_true, u_true, <span class="string">&#x27;b&#x27;</span>, label=<span class="string">&quot;True Function&quot;</span>)</span><br><span class="line">t = [i*dt <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">int</span>(t_final/dt)+<span class="number">1</span>)]</span><br><span class="line">uFE = forward_euler(dudt, u0, dt, t_final)</span><br><span class="line">uBE = backward_euler(dudt, u0, dt, t_final, llam)</span><br><span class="line">err_FE = np.mean([<span class="built_in">abs</span>(uFE[i] - u_true[i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(uFE))])</span><br><span class="line">err_BE = np.mean([<span class="built_in">abs</span>(uBE[i] - u_true[i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(uFE))])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Error of Forward Euler: &quot;</span>, err_FE)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Error of Backward Euler: &quot;</span>, err_BE)</span><br><span class="line">plt.plot(t, uFE, <span class="string">&quot;r-&quot;</span>, label=<span class="string">&quot;Forward Euler&quot;</span>)</span><br><span class="line">plt.plot(t, uBE, <span class="string">&quot;g-&quot;</span>, label=<span class="string">&quot;Backward Euler&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.savefig(<span class="string">&quot;./fe_be.png&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>我们前面提到了一般来说，隐式欧拉更加的精确并且稳定（但有例外，尤其是原函数是线性函数的时候，显式欧拉更加精确）。什么是稳定性呢，就是当函数变化的时候，我们的估值计算误差不会发生很大的浮动。</p><p>上面的代码对 cos 函数做了一个小小的修改，以便于稳定性的比较，当 llam 很小的时候（如代码中），我们看到如下图：</p><p><img src="https://i.loli.net/2021/02/01/HnzpPOhVNyGmkI9.png" alt="fe_be.png"></p><p>当我们将 llam 调到 100 的时候，就会发现图像变成了下面的样子：</p><p><img src="https://i.loli.net/2021/02/01/OpF4Pv3VxDEywS9.png" alt="fe_be.png"></p><p>观察误差 <code>Error of Forward Euler:  1.183439944385943e+88; Error of Backward Euler:  0.9107412532059388</code> 明显，显式欧拉法的不稳定性弊端就凸显出来了。</p><p>计算隐式欧拉法的根的时候采用的牛顿法估计根，方法的描述如下图（懒得打了网上找了张图）</p><p><img src="https://i.loli.net/2021/02/01/UxPjy6VREdcl28h.png" alt="牛顿法求近似根.png"></p><h3 id="FE-vs-RK4"><a href="#FE-vs-RK4" class="headerlink" title="FE vs RK4"></a>FE vs RK4</h3><p>RK4 也并非一直都比 FE 更加的精确，看下面的这个例子：</p><p>我们设计下面一个 ODE：</p><script type="math/tex; mode=display">\frac{du(t)}{dt} = -\lambda u(t); u(0)=1</script><p>令$ u(t)=e^{-\lambda t} $, ODE 公式如下，当我们放大参数$\lambda$就可以比较 FE 和 RK4 方法的误差之处，有了真实函数方便我们计算误差，码来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">coff = <span class="number">100</span>  <span class="comment"># 1 10 100 尝试变化</span></span><br><span class="line">u0 = <span class="number">1</span></span><br><span class="line">t_final = <span class="number">1</span></span><br><span class="line">fe_error = []</span><br><span class="line">rk4_error = []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">t, u</span>):</span></span><br><span class="line">    <span class="keyword">return</span> -coff * u</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">true_f</span>(<span class="params">u</span>):</span></span><br><span class="line">    <span class="keyword">return</span> u0 * math.exp(-coff*u)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_euler</span>(<span class="params">f, u0, dt, t_final</span>):</span></span><br><span class="line">    t = <span class="number">0</span></span><br><span class="line">    u = u0</span><br><span class="line">    <span class="keyword">while</span> t &lt; t_final:</span><br><span class="line">        u = u + dt * f(t, u)</span><br><span class="line">        t = t + dt</span><br><span class="line">    <span class="keyword">return</span> u</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rk4</span>(<span class="params">f, u0, dt, t_final</span>):</span></span><br><span class="line">    t = <span class="number">0</span></span><br><span class="line">    u = u0</span><br><span class="line">    <span class="keyword">while</span> t &lt; t_final:</span><br><span class="line">        t_half = t + <span class="number">0.5</span> * dt</span><br><span class="line">        t_next = t + dt</span><br><span class="line">        k1 = dt * f(t, u)</span><br><span class="line">        u1 = u + <span class="number">0.5</span> * k1</span><br><span class="line">        k2 = dt * f(t_half, u1)</span><br><span class="line">        u2 = u + <span class="number">0.5</span> * k2</span><br><span class="line">        k3 = dt * f(t_half, u2)</span><br><span class="line">        u3 = u + k3</span><br><span class="line">        k4 = dt * f(t_next, u3)</span><br><span class="line">        u = u + (<span class="number">1</span>/<span class="number">6</span>) * (k1 + <span class="number">2</span> * (k2 + k3) + k4)</span><br><span class="line">        <span class="comment"># 更新 t</span></span><br><span class="line">        t = t_next</span><br><span class="line">    <span class="keyword">return</span> u</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dt_vec = [<span class="number">1e-4</span>, <span class="number">2e-4</span>, <span class="number">1e-3</span>, <span class="number">2e-3</span>, <span class="number">1e-2</span>, <span class="number">2e-2</span>, <span class="number">1e-1</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(dt_vec)):</span><br><span class="line">    dt = dt_vec[i]</span><br><span class="line">    fe_result = forward_euler(f, u0, dt, t_final)</span><br><span class="line">    rk_result = rk4(f, u0, dt, t_final)</span><br><span class="line">    fe_error.append(<span class="built_in">abs</span>(true_f(t_final) - fe_result))</span><br><span class="line">    rk4_error.append(<span class="built_in">abs</span>(true_f(t_final) - rk_result))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图</span></span><br><span class="line">plt.loglog(dt_vec, fe_error, <span class="string">&quot;r-&quot;</span>, label=<span class="string">&quot;Forward Euler Error&quot;</span>)</span><br><span class="line">plt.loglog(dt_vec, rk4_error, <span class="string">&quot;b&quot;</span>, label=<span class="string">&quot;RK4 Error&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;dt&quot;</span>)</span><br><span class="line"><span class="comment"># y 轴设置反向</span></span><br><span class="line"><span class="comment"># ax = plt.gca()</span></span><br><span class="line"><span class="comment"># ax.invert_yaxis()</span></span><br><span class="line"></span><br><span class="line">plt.ylabel(<span class="string">&quot;error&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.savefig(<span class="string">&quot;fe_rk.png&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>当我们取 $\lambda=10$ 时</p><p><img src="https://i.loli.net/2021/02/23/pSgFEGTDnRZyYhr.png" alt=""></p><p>当我们取 $\lambda=100$ 时</p><p><img src="https://i.loli.net/2021/02/23/aXJEPe17WyzMGhY.png" alt=""></p><p>我们可以明显的感受到，并非高阶方法就能带来更小的误差，高阶只代表随着计算迭代次数增加，误差收敛的快。但这同时也要求有一个很好的 $dt$ 的选择。通常在对精度要求不高的计算中，我们都可以选择 FE 来解决。</p><p>(ps: 实际运算的时候 matlab 和 python 结果有些不同，matlab 更加精确一些，图像看起来也更好看。但图像变换的趋势和结论是一样的)</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;正式开课了，这里仅是我的学习记录。&lt;br&gt;</summary>
    
    
    
    <category term="AI" scheme="http://cyx0706.github.io/categories/AI/"/>
    
    
    <category term="AI+X Blending" scheme="http://cyx0706.github.io/tags/AI-X-Blending/"/>
    
  </entry>
  
  <entry>
    <title>bird-identify</title>
    <link href="http://cyx0706.github.io/2020/12/31/bird-identify/"/>
    <id>http://cyx0706.github.io/2020/12/31/bird-identify/</id>
    <published>2020-12-31T11:37:00.000Z</published>
    <updated>2021-07-07T00:35:20.362Z</updated>
    
    <content type="html"><![CDATA[<h1 id="软件工程炼丹心得与体会"><a href="#软件工程炼丹心得与体会" class="headerlink" title="软件工程炼丹心得与体会"></a>软件工程炼丹心得与体会</h1><p><del>其实就是深度学习入门吧</del></p><p>大三上软工项目：鸟类识别与分享平台，项目传送门<a href="https://github.com/OUC-iBird">iBird</a></p><p>在充分了解了深度学习（指前两周看了老师发的视频）后开始尝试构建鸟类识别模型，这篇博客用于记录自己在学习中的一点点收获。</p><p>ps:基础太差了，感觉好多时候都是在瞎炼。<br><a id="more"></a></p><h2 id="一点点准备"><a href="#一点点准备" class="headerlink" title="一点点准备"></a>一点点准备</h2><ul><li><p><a href="https://cyx0706.github.io/2020/11/20/cuda-disaster/">Pytorch + Cuda</a></p></li><li><p>我的电脑GPU内存不咋够用，所以训练主要放在了 <a href="https://www.google.com/intl/zh-CN/drive/">Colab</a> 上</p></li><li><p>模型训练的数据源于<a href="https://god.yanxishe.com/4?from=god_home_list">AI 研习社-200种鸟类识别分类</a></p></li></ul><h2 id="模型的构建"><a href="#模型的构建" class="headerlink" title="模型的构建"></a>模型的构建</h2><h3 id="细粒度图像识别"><a href="#细粒度图像识别" class="headerlink" title="细粒度图像识别"></a>细粒度图像识别</h3><p>200 种鸟类识别其实是一个细粒度图像识别问题(fine-grained image recognition)</p><p>对于现在的模型，识别出物体的大类别（比如：猫，狗，手机，车）比较容易，但如果要进一步去更细的划分物体的类别和名称，难度就大了很多，在这其中，有一些子类别的差异十分的小，如何区分布他们是比较困难的。</p><p>目前，精细化分类的方法主要有以下两类：</p><ul><li>基于图像重要区域定位的方法：该方法集中探讨如何利用弱监督的信息自动找到图像中有判别力的区域，从而达到精细化分类的目的。</li><li>基于图像精细化特征表达的方法：该方法提出使用高维度的图像特征（如：bilinear vector）对图像信息进行高阶编码，以达到准确分类的目的。</li></ul><p>举我看的论文里面的例子吧：</p><p>在 <a href="http://vis-www.cs.umass.edu/bcnn/">Bilinear CNN Models for Fine-grained Visual Recognition</a> 这篇论文里提到了：</p><blockquote><p>Fine-grained recognition tasks such as identifying the species of a bird …… are quite challenging because the visual differences between the categories are small and can be easily overwhelmed by those caused by factors such as pose, viewpoint, or location of the object in the image.</p></blockquote><p>这里提到了，细粒度识别的一个很大的难度在于”细小的差别会被鸟的姿势，视角，拍摄的位置给掩盖掉”（这里是以鸟为例）</p><blockquote><p>For example, the inter-category variation(类别间的变化) between “Ringed-beak gull” and a “California gull” due to the differences in the pattern on their beaks(喙) is significantly smaller than the inter-category variation on a popular fine-grained recognition dataset for birds.</p></blockquote><p>论文中举了环嘴鸥（Ringed-beak gull）和加州鸥（California gull）在喙上的差别要明显小于细粒度分别的数据集中的差别。</p><p>为了解决这个问题，这篇论文中提出了一个 BCNN 模型来解决，我主要学习的也是这个模型，不过这是后面要说的了。</p><h3 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h3><p>在正式写我们的模型前，要先写好读取数据的方法，数据集就用 AI 研习社上的了，先在本地下一份。</p><p>对于数据，我们交给模型训练的时候，一般都会进行预处理，预处理的方法有很多，最常用的如下：</p><ul><li>平移：一定尺度内平移</li><li>旋转：一定角度内旋转</li><li>翻转：水平或者上下翻转</li><li>裁剪：在原有图像上裁剪一部分</li><li>颜色变化：rgb 颜色空间进行一些变换（亮度对比度等）</li><li>噪声扰动：给图像加入一些人工生产的噪声</li></ul><p>说的高级点好像叫<strong>数据增强</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="comment"># 随机比例缩放</span></span><br><span class="line">transforms.Resize((<span class="number">100</span>, <span class="number">200</span>))</span><br><span class="line"><span class="comment"># 随机位置裁剪</span></span><br><span class="line">transforms.RandomCrop(<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 中心裁剪</span></span><br><span class="line">transforms.CenterCrop(<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 随机垂直水平翻转</span></span><br><span class="line">transforms.RandomVerticalFlip(p=<span class="number">1</span>)</span><br><span class="line">transforms.RandomHorizontalFlip(p=<span class="number">1</span>)   <span class="comment"># p表示概率</span></span><br><span class="line"><span class="comment"># 随机角度旋转</span></span><br><span class="line">transforms.RandomRotation(<span class="number">45</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 色度，亮度，饱和度，对比度</span></span><br><span class="line">transforms.ColorJitter(brightness=<span class="number">1</span>)  <span class="comment"># 亮度</span></span><br><span class="line">transforms.ColorJitter(contrast=<span class="number">1</span>)  <span class="comment"># 对比度</span></span><br><span class="line">transforms.ColorJitter(saturation=<span class="number">0.5</span>)  <span class="comment"># 饱和度</span></span><br><span class="line">transforms.ColorJitter(hue=<span class="number">0.5</span>)  <span class="comment"># 色度</span></span><br></pre></td></tr></table></figure><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Pytorch 提供内置的图片数据集 ImageFolder，它有一个通用的数据加载器，它加载的数据要求以下面的方式组织：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">base_dir = <span class="string">&quot;xxx/xxx&quot;</span></span><br><span class="line"><span class="comment"># data_dir 中的图片这样组织</span></span><br><span class="line"><span class="comment"># data_dir/dog/xxx1.png</span></span><br><span class="line"><span class="comment"># data_dir/dog/xxx2.png</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># data_dir/cat/xxx1.png</span></span><br><span class="line"><span class="comment"># data_dir/cat/xxx2.png</span></span><br><span class="line"></span><br><span class="line">predict_sets = torchvision.datasets.ImageFolder(os.path.join(base_dir, <span class="string">&quot;data_dir&quot;</span>), transform=your_trans)</span><br></pre></td></tr></table></figure><p>这时读入的数据所有在 dog 文件夹下的都被打上了 dog 的标签，同理 cat。简单来说，你要将一类的图片全部放入一个以这个类别命名的文件夹下才能正常的读取。</p><p>这对于我们这个显然不太方面，所以就要自己写数据集的加载方式了</p><blockquote><p>All datasets are subclasses of torch.utils.data.Dataset i.e, they have __getitem__ and __len__ methods implemented. Hence, they can all be passed to a torch. utils.data.DataLoader which can load multiple samples parallelly using torch.multiprocessing workers. </p></blockquote><p>就是要我们实现两个函数<code>__getitem__()</code> 和 <code>__len__()</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FirstDataset</span>(<span class="params">data.Dataset</span>):</span><span class="comment">#需要继承data.Dataset</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment">#在这里初始化</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#1 读取一个数据和标签</span></span><br><span class="line">        <span class="comment">#2 预处理数据（例如 torchvision.transform）</span></span><br><span class="line">        <span class="comment">#3 返回数据对（例如图像和标签）</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 数据集的大小</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>有了这个我们思路就很清晰了，由于我们的标签都在一个 .csv 文件中，里面包括图片名对应的标签号，我们用 Pandas 读入然后分列，在我们的__getitem__() 函数里一次取一个就好了（取第 item 个）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets.folder <span class="keyword">import</span> accimage_loader, pil_loader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">default_loader</span>(<span class="params">path</span>):</span></span><br><span class="line">    <span class="keyword">from</span> torchvision <span class="keyword">import</span> get_image_backend</span><br><span class="line">    <span class="keyword">if</span> get_image_backend() == <span class="string">&#x27;accimage&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> accimage_loader(path)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> pil_loader(path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomDataset</span>(<span class="params">torch.utils.data.Dataset</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data_path, data_label_path, data_transform, data_loader=default_loader</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param data_path: 要读取的文件的路径</span></span><br><span class="line"><span class="string">        :param data_label_path: 标签数据的路径</span></span><br><span class="line"><span class="string">        :param data_transform: 数据变换模式</span></span><br><span class="line"><span class="string">        :param data_loader: 加载方法</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 在 label文件中注意不要加上第一行列名行</span></span><br><span class="line">        df = pd.read_csv(data_label_path, header=<span class="literal">None</span>)</span><br><span class="line">        self.data_loader = data_loader</span><br><span class="line">        self.data_transform = data_transform</span><br><span class="line">        self.data_path = data_path</span><br><span class="line">        </span><br><span class="line">        self.img_names = <span class="built_in">list</span>(df[<span class="number">0</span>])</span><br><span class="line">        self.labels = <span class="built_in">list</span>(df[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.img_names)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 模型训练的时候调用，返回一组图片和标签用于训练</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, item</span>):</span></span><br><span class="line">        img_name = self.img_names[item]</span><br><span class="line">        img_path = os.path.join(self.data_path, img_name)</span><br><span class="line">        label = self.labels[item]</span><br><span class="line">        img = self.data_loader(img_path)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            img = self.data_transform(img)</span><br><span class="line">            <span class="keyword">return</span> img, label-<span class="number">1</span></span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">raise</span> Exception(<span class="string">&quot;cannot transform image: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(img_name))</span><br></pre></td></tr></table></figure><h3 id="训练函数"><a href="#训练函数" class="headerlink" title="训练函数"></a>训练函数</h3><p>Tranier 的写法比较固定，网上有各种各样的，贴一个我找到<del>（自己写不来，但改了一下）</del></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Tuple</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Module</span><br><span class="line"><span class="keyword">from</span> torch.optim.optimizer <span class="keyword">import</span> Optimizer</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torch.optim.lr_scheduler <span class="keyword">import</span> ReduceLROnPlateau</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trainer</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">            self,</span></span></span><br><span class="line"><span class="function"><span class="params">            model: Module,</span></span></span><br><span class="line"><span class="function"><span class="params">            criterion: Module,</span></span></span><br><span class="line"><span class="function"><span class="params">            optimizer: Optimizer,</span></span></span><br><span class="line"><span class="function"><span class="params">            device: torch.device</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        </span><br><span class="line">        <span class="built_in">super</span>(Trainer, self).__init__()</span><br><span class="line">        self.model: Module = model</span><br><span class="line">        self.criterion: Module = criterion</span><br><span class="line">        self.optimizer: Optimizer = optimizer</span><br><span class="line">        self.device: torch.device = device</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self, loader: DataLoader</span>) -&gt; <span class="type">Tuple</span>[<span class="built_in">float</span>, <span class="built_in">float</span>]:</span></span><br><span class="line">        </span><br><span class="line">        total_loss, total_acc = <span class="number">0.0</span>, <span class="number">0.0</span></span><br><span class="line">        self.model.train()</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">with</span> tqdm(<span class="built_in">enumerate</span>(loader), total=<span class="built_in">len</span>(loader), desc=<span class="string">&#x27;Training&#x27;</span>) <span class="keyword">as</span> proc:</span><br><span class="line">                <span class="keyword">for</span> _, (inputs, targets) <span class="keyword">in</span> proc:</span><br><span class="line">                    inputs = inputs.to(self.device)</span><br><span class="line">                    targets = targets.to(self.device)</span><br><span class="line">                    outputs = self.model(inputs)</span><br><span class="line">                    loss = self.criterion(outputs, targets)</span><br><span class="line">                    self.optimizer.zero_grad()</span><br><span class="line">                    loss.backward()</span><br><span class="line">                    self.optimizer.step()</span><br><span class="line">                    _, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">                    total_loss += loss.item()</span><br><span class="line">                    total_acc += (predicted == targets).<span class="built_in">float</span>().<span class="built_in">sum</span>().item() / targets.numel()</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="comment"># 异常情况关闭</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Running Error in training, &quot;</span>, e)</span><br><span class="line">            proc.close()</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>, -<span class="number">1</span></span><br><span class="line">        proc.close()</span><br><span class="line">        <span class="keyword">return</span> total_loss / <span class="built_in">len</span>(loader), <span class="number">100.0</span> * total_acc / <span class="built_in">len</span>(loader)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span>(<span class="params">self, loader: DataLoader</span>) -&gt; <span class="type">Tuple</span>[<span class="built_in">float</span>, <span class="built_in">float</span>]:</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            total_loss, total_acc = <span class="number">0.0</span>, <span class="number">0.0</span></span><br><span class="line">            self.model.<span class="built_in">eval</span>()</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="keyword">with</span> tqdm(<span class="built_in">enumerate</span>(loader), total=<span class="built_in">len</span>(loader), desc=<span class="string">&#x27;Testing &#x27;</span>) <span class="keyword">as</span> proc:</span><br><span class="line">                    <span class="keyword">for</span> _, (inputs, targets) <span class="keyword">in</span> proc:</span><br><span class="line">                        inputs = inputs.to(self.device)</span><br><span class="line">                        targets = targets.to(self.device)</span><br><span class="line">                        outputs = self.model(inputs)</span><br><span class="line">                        loss = self.criterion(outputs, targets)</span><br><span class="line">                        _, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">                        total_loss += loss.item()</span><br><span class="line">                        total_acc += (predicted == targets).<span class="built_in">float</span>().<span class="built_in">sum</span>().item() / targets.numel()</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                proc.close()</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Running Error in validating,&quot;</span>, e)</span><br><span class="line">                <span class="keyword">return</span> -<span class="number">1</span>, -<span class="number">1</span></span><br><span class="line">            proc.close()</span><br><span class="line">        <span class="keyword">return</span> total_loss / <span class="built_in">len</span>(loader), <span class="number">100.0</span> * total_acc / <span class="built_in">len</span>(loader)</span><br></pre></td></tr></table></figure><ul><li><p>比较喜欢这个写法，tqdm 是一个 Python 的进度条库，它有个问题是如果代码异常结束，它有时不会被停止，这样在第二次运行时会无法刷新输出窗口，导致看上去就不是一个进度条了，而是进度条每更新一次就打印出来一个新的，原来的还在。我们在套一个 try-catch 在异常的时候正确的关闭这个进度条进程就好了，用 close() 函数。</p></li><li><p>model.eval() 和 model.train()这两个必须要搞明白</p><ul><li>model.train() 会启用 BatchNormalization 和 Dropout 而 model.eval() 不启用 BatchNormalization 和 Dropout。</li><li>否则的话，有输入数据，即使不训练，它也会改变权值。这是 model 中含有 batch normalization 层和 dropout所带来的的性质。</li></ul></li></ul><p><img src="https://i.loli.net/2020/12/31/riJ1FKxGcNa3MXA.png" alt="dropout.png"></p><ul><li>想象一下，如果被删除的神经元是唯一促成正确结果的神经元。一旦我们不激活它，其他神经元就需要学习如何在没有这些神经元的情况下保持准确。这种 dropout 提高了最终测试的性能。但它对训练期间的性能产生了负面影响，因为网络是不全的。</li></ul><h3 id="数据集加载"><a href="#数据集加载" class="headerlink" title="数据集加载"></a>数据集加载</h3><p>用 <code>torch.utils.data.DataLoader</code> 就可以，需要注意的是 Windows 下需要将 num_workers 设置为 0。</p><p>dataloader 一次性创建 num_worker 个 worker，他们负责将数据提前读入好内存。num_worker 设置得大，好处是寻 batch 速度快，因为下一轮迭代的 batch 很可能在前面几轮的迭代时已经加载好了。坏处是内存开销大，也加重了 CPU 的负担。num_workers 的经验设置看自己的 CPU 和 RAM 吧，如果 CPU 处理强，内存大，就可以设置得更大些。如果 num_worker 设为 0，意味着每一轮迭代时，dataloader 不再有自主加载数据到 RAM 这一步骤（没有worker了），而是在RAM 中找 batch，找不到时再加载相应的 batch。这样当然是速度慢。</p><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>我鸟类识别的模型实现了两个（<del>还有一个出问题了先不管他</del>）</p><h4 id="BCNN"><a href="#BCNN" class="headerlink" title="BCNN"></a>BCNN</h4><p><a href="http://vis-www.cs.umass.edu/bcnn/">Bilinear CNN Models for Fine-grained Visual Recognition</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BilinearModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Load model with pretrained weights and initialise new layers.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_classes: <span class="built_in">int</span> = <span class="number">200</span>, pretrained=<span class="literal">True</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Load pretrained model, set new layers with specified number of layers.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(BilinearModel, self).__init__()</span><br><span class="line">        model: nn.Module = models.vgg16(pretrained)</span><br><span class="line">        self.features: nn.Module = nn.Sequential(*<span class="built_in">list</span>(model.features)[:-<span class="number">1</span>])</span><br><span class="line">        self.classifier: nn.Module = nn.Linear(<span class="number">512</span> ** <span class="number">2</span>, num_classes)</span><br><span class="line">        self.dropout: nn.Module = nn.Dropout(<span class="number">0.5</span>)</span><br><span class="line">        nn.init.kaiming_normal_(self.classifier.weight.data)</span><br><span class="line">        <span class="keyword">if</span> self.classifier.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            nn.init.constant_(self.classifier.bias.data, val=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @overrides</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs: torch.Tensor</span>) -&gt; torch.Tensor:</span></span><br><span class="line">        outputs: torch.Tensor = self.features(inputs)</span><br><span class="line">        outputs = outputs.view(-<span class="number">1</span>, <span class="number">512</span>, <span class="number">28</span> ** <span class="number">2</span>)</span><br><span class="line">        outputs = self.dropout(outputs)</span><br><span class="line">        outputs = torch.bmm(outputs, outputs.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>))      <span class="comment"># bilinear product</span></span><br><span class="line">        outputs = torch.div(outputs, <span class="number">28</span> ** <span class="number">2</span>)                       <span class="comment"># normalize</span></span><br><span class="line">        outputs = outputs.view(-<span class="number">1</span>, <span class="number">512</span> ** <span class="number">2</span>) </span><br><span class="line">        outputs = torch.sign(outputs) * torch.sqrt(outputs + <span class="number">1e-5</span>)  <span class="comment"># signed square root normalization</span></span><br><span class="line">        outputs = nn.functional.normalize(outputs, p=<span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">        outputs = self.dropout(outputs)</span><br><span class="line">        outputs = self.classifier(outputs)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><p>论文中原本推荐使用两个不同的模型来提取特征值然后使用一个双线性函数来进一步处理提取的特征值，后来又有人指出，使用同源的模型也可以得到不错的效果，所以我就尝试使用了 VGG 作为提取层，然后将处理好的结果使用一个全连接层对应 200 种鸟类。最后正确率在 75% 左右。</p><h4 id="EfficientNet-With-Attention"><a href="#EfficientNet-With-Attention" class="headerlink" title="EfficientNet With Attention"></a>EfficientNet With Attention</h4><p>Attention机制还没咋看的（有空再补了），看别人这么用我也就瞎几把组合了一下。</p><p><a href="https://github.com/lukemelas/EfficientNet-PyTorch">Pytorch 实现的 EfficientNet</a></p><p><a href="https://arxiv.org/abs/1905.11946">论文在此</a></p><p>这个我看懂了（震声！），论文对现有模型提出了反思：如果只是增加模型的深度（有多少层）（depth），宽度（每一层的参数数）（width），还有图像的解析度（输入的大小）（resolution）其中之一对模型的提升不完全而且有时还会导致准确率下降。Google 的研究员们发现当按照一个比率（ratio）同时提升这 3 个值，会让模型更好的提高准确度，也变得更加精简。它通过（经验？）发现这样的原则:</p><script type="math/tex; mode=display">depth: d = \alpha^\phi  \\width: w = \beta^\phi \\resolution: r = \gamma^\phi \\s.t. \quad  \alpha * \beta^2 * \gamma^2 \approx 2 \\\quad \quad \quad \quad \quad \alpha \geqslant 1, \beta \geqslant 1,\gamma \geqslant 1 \\</script><p>按照这个原则，Google 提出了 EfficientNet 系列，非常精简并且准确率高的模型。。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> efficientnet_pytorch <span class="keyword">import</span> EfficientNet</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> lr_scheduler</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv3x3</span>(<span class="params">in_planes, out_planes, stride=<span class="number">1</span></span>):</span></span><br><span class="line">    <span class="comment"># &quot;3x3 convolution with padding&quot;</span></span><br><span class="line">    <span class="keyword">return</span> nn.Conv2d(in_planes, out_planes, kernel_size=<span class="number">3</span>, stride=stride,</span><br><span class="line">                     padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChannelAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_planes, ratio=<span class="number">16</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ChannelAttention, self).__init__()</span><br><span class="line">        self.avg_pool = nn.AdaptiveAvgPool2d(<span class="number">1</span>)  <span class="comment"># 压缩空间</span></span><br><span class="line">        self.max_pool = nn.AdaptiveMaxPool2d(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.fc1 = nn.Conv2d(in_planes, in_planes // <span class="number">16</span>, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.relu1 = nn.ReLU()</span><br><span class="line">        self.fc2 = nn.Conv2d(in_planes // <span class="number">16</span>, in_planes, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))</span><br><span class="line">        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))</span><br><span class="line">        out = avg_out + max_out  <span class="comment"># [b, C, 1, 1]</span></span><br><span class="line">        <span class="keyword">return</span> self.sigmoid(out)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpatialAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, kernel_size=<span class="number">7</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SpatialAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> kernel_size <span class="keyword">in</span> (<span class="number">3</span>, <span class="number">7</span>), <span class="string">&#x27;kernel size must be 3 or 7&#x27;</span></span><br><span class="line">        padding = <span class="number">3</span> <span class="keyword">if</span> kernel_size == <span class="number">7</span> <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">2</span>, <span class="number">1</span>, kernel_size, padding=padding, bias=<span class="literal">False</span>)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        avg_out = torch.mean(x, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)  <span class="comment"># 压缩通道</span></span><br><span class="line">        max_out, _ = torch.<span class="built_in">max</span>(x, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)   <span class="comment"># 压缩通道</span></span><br><span class="line">        x = torch.cat([avg_out, max_out], dim=<span class="number">1</span>)  <span class="comment"># [b, 1, h, w]</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        <span class="keyword">return</span> self.sigmoid(x)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EfficientNetWithAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_classes: <span class="built_in">int</span> = <span class="number">200</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EfficientNetWithAttention, self).__init__()</span><br><span class="line">        self.eff_model = EfficientNet.from_pretrained(<span class="string">&quot;efficientnet-b7&quot;</span>)</span><br><span class="line">        self._avg_pooling = nn.AdaptiveAvgPool2d(output_size=<span class="number">1</span>)</span><br><span class="line">        self._dropout = nn.Dropout(p=<span class="number">0.5</span>, inplace=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(in_features=<span class="number">2560</span>, out_features=num_classes, bias=<span class="literal">True</span>)</span><br><span class="line">        self.ca_head = ChannelAttention(<span class="number">64</span>)</span><br><span class="line">        self.sa = SpatialAttention()</span><br><span class="line">        self.ca_tail = ChannelAttention(<span class="number">2560</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.eff_model.extract_features(x)</span><br><span class="line">        <span class="comment"># 最后一层加入 Attention 机制</span></span><br><span class="line">        x = self.ca_tail(x) * x</span><br><span class="line">        x = self.sa(x) * x</span><br><span class="line">        x = self._avg_pooling(x)</span><br><span class="line">        <span class="keyword">if</span> self.eff_model._global_params.include_top:</span><br><span class="line">            x = x.flatten(start_dim=<span class="number">1</span>)</span><br><span class="line">            x = self._dropout(x)</span><br><span class="line">            x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>最后准确率在 81% 左右，大小仅仅需要 200+MB，比前一个小多了！</p><h3 id="学习率调整函数"><a href="#学习率调整函数" class="headerlink" title="学习率调整函数"></a>学习率调整函数</h3><p>一般来说，我们希望在训练初期学习率大一些，使得网络收敛迅速，在训练后期学习率小一些，使得网络更好的收敛到最优解。</p><h4 id="固定步长衰减"><a href="#固定步长衰减" class="headerlink" title="固定步长衰减"></a>固定步长衰减</h4><ul><li>使用 <code>torch.optim.lr_scheduler.StepLR</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">optimizer_StepLR = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line">StepLR = torch.optim.lr_scheduler.StepLR(optimizer_StepLR, step_size=step_size, gamma=<span class="number">0.65</span>)</span><br></pre></td></tr></table></figure><p>其中gamma参数表示衰减的程度，step_size参数表示每隔多少个step进行一次学习率调整</p><h4 id="ReduceLROnPlateau"><a href="#ReduceLROnPlateau" class="headerlink" title="ReduceLROnPlateau"></a>ReduceLROnPlateau</h4><ul><li>使用 <code>torch.optim.lr_scheduler.ReduceLROnPlateau</code></li></ul><p>他可以基于训练中的某些测量值对学习率进行动态下降。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=<span class="string">&#x27;min&#x27;</span>, factor=<span class="number">0.1</span>, patience=<span class="number">10</span>,</span><br><span class="line"> verbose=<span class="literal">False</span>, threshold=<span class="number">0.0001</span>, threshold_mode=<span class="string">&#x27;rel&#x27;</span>, cooldown=<span class="number">0</span>, min_lr=<span class="number">0</span>, eps=<span class="number">1e-08</span>)</span><br></pre></td></tr></table></figure><ul><li>mode 可选择 min 或者 max ，min 表示当监控量停止下降的时候，学习率将减小，max 表示当监控量停止上升的时候，学习率将减小。</li><li>factor 学习率每次降低多少。new_lr = old_lr * factor</li><li>min_lr,学习率的下限</li></ul><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>感觉软工这个项目确实学到了点深度学习和人工智能的东西，但又说不上来（还是太菜了）。2020 年要结束了，今年的工作绝不拖到明年做！先这样子了，忙去复习期末了，等有空了还会捡起来接着做的！<del>数学不好感觉学不明白……</del></p><p>下一步大概是尝试异元的 BCNN，一个用 EfficientNet 和另一个用 EfficientNet + Attention。还有就是提取特征后使用 SVM 或者一些拟合函数来训练，希望能突破85% 的准确率吧。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://cloud.tencent.com/developer/article/1347752">腾讯云：细粒度图像识别概述</a></li><li><a href="https://blog.csdn.net/mind_programmonkey/article/details/104354525">实战200类鸟类细粒度图像分类</a>|这个我虽然没看懂代码，但学习了点方法</li><li><a href="http://vis-www.cs.umass.edu/bcnn/">Bilinear CNN Models for Fine-grained Visual Recognition</a></li><li><a href="https://github.com/dasguptar/bcnn.pytorch/tree/master/bcnn">dasguptar/bcnn.pytorch</a></li><li><a href="https://github.com/lukemelas/EfficientNet-PyTorch/">lukemelas/EfficientNet-PyTorch </a></li><li><a href="https://zhuanlan.zhihu.com/p/93624972">pytorch必须掌握的的4种学习率衰减策略</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;软件工程炼丹心得与体会&quot;&gt;&lt;a href=&quot;#软件工程炼丹心得与体会&quot; class=&quot;headerlink&quot; title=&quot;软件工程炼丹心得与体会&quot;&gt;&lt;/a&gt;软件工程炼丹心得与体会&lt;/h1&gt;&lt;p&gt;&lt;del&gt;其实就是深度学习入门吧&lt;/del&gt;&lt;/p&gt;
&lt;p&gt;大三上软工项目：鸟类识别与分享平台，项目传送门&lt;a href=&quot;https://github.com/OUC-iBird&quot;&gt;iBird&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在充分了解了深度学习（指前两周看了老师发的视频）后开始尝试构建鸟类识别模型，这篇博客用于记录自己在学习中的一点点收获。&lt;/p&gt;
&lt;p&gt;ps:基础太差了，感觉好多时候都是在瞎炼。&lt;br&gt;</summary>
    
    
    
    <category term="AI" scheme="http://cyx0706.github.io/categories/AI/"/>
    
    
    <category term="Deep Learning" scheme="http://cyx0706.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>pre-learn-of-aix-week-2/3</title>
    <link href="http://cyx0706.github.io/2020/12/26/pre-learn-of-aix-2/"/>
    <id>http://cyx0706.github.io/2020/12/26/pre-learn-of-aix-2/</id>
    <published>2020-12-26T07:45:50.000Z</published>
    <updated>2021-01-16T03:38:01.212Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Content"><a href="#Content" class="headerlink" title="Content"></a>Content</h1><p>In this blog, I’ll combine pre learning 2 and 3.<br><a id="more"></a></p><h2 id="Interviews-with-AI-Leaders"><a href="#Interviews-with-AI-Leaders" class="headerlink" title="Interviews with AI Leaders"></a>Interviews with AI Leaders</h2><p>// watching…..</p><h2 id="Learning-Python"><a href="#Learning-Python" class="headerlink" title="Learning Python"></a>Learning Python</h2><h3 id="Complex"><a href="#Complex" class="headerlink" title="Complex"></a>Complex</h3><p>Python actually has complex numbers as a primitive data type. There are two ways to make a complex number: </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="built_in">complex</span>(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span> <span class="built_in">type</span>(a)</span><br><span class="line">a = <span class="number">1</span>+<span class="number">2j</span></span><br><span class="line"><span class="built_in">print</span> <span class="built_in">type</span>(a)</span><br><span class="line"><span class="built_in">print</span> a.imag</span><br><span class="line"><span class="built_in">print</span> a.real</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">result: </span></span><br><span class="line"><span class="string">&lt;type &#x27;complex&#x27;&gt;</span></span><br><span class="line"><span class="string">&lt;type &#x27;complex&#x27;&gt;</span></span><br><span class="line"><span class="string">2.0</span></span><br><span class="line"><span class="string">1.0</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>ps: the textbook mentioned something interesting:</p><blockquote><p>You’re probably used to using i for (-1)0.5. Just to confuse you, we’re going to use j instead. Why? Because to an electrical engineer, i stands for current, and there’s no arguing</p></blockquote><h3 id="Funtional-Style"><a href="#Funtional-Style" class="headerlink" title="Funtional Style"></a>Funtional Style</h3><p>What is Funtional Style? In the prelearning material, it is defined as:</p><blockquote><p>In the functional programming style, one tries to use a very small set of primitives and means of combination. We’ll see that recursion is a very powerful primitive, which could allow us to dispense with all other looping constructs (while and for) and results in code with a certain beauty. </p><p>Another element of functional programming style is the idea of functions as first-class objects. That means that we can treat functions or procedures in much the same way we treat numbers or strings in our programs: we can pass them as arguments to other procedures and return them as the results from procedures. This will let us capture important common patterns of abstraction and will also be an important element of object-oriented programming.</p></blockquote><p>A exmple of ancient algorithm: </p><p>It is an iterative algorithm: it starts with a guess about the square root, and repeatedly asks whether the guess is good enough. It’s good enough if, when we square the guess, we are close to the number, x, that we’re trying to take the square root of. If the guess isn’t good enough, we need to try to improve it. We do that by making a new guess, which is the average of the original <code>guess</code> and <code>x / guess</code>. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sqrt</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">goodEnough</span>(<span class="params">guess</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">abs</span>(x-square(guess)) &lt; <span class="number">.0001</span></span><br><span class="line">    guess = <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> goodEnough(guess):</span><br><span class="line">        guess=average(guess,x/guess)</span><br><span class="line">    <span class="keyword">return</span> guess</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sqrt(<span class="number">2</span>)</span><br><span class="line"><span class="number">1.4142156862745097</span> </span><br></pre></td></tr></table></figure><h3 id="Extra"><a href="#Extra" class="headerlink" title="Extra:"></a>Extra:</h3><ol><li><p>inline function in function. We called it closure in a more professional way.</p><ul><li>I once write a blog about this, <a href="https://cyx0706.github.io/2020/05/24/python-closure/#more">see here</a></li></ul></li><li><p><em>arg, *</em>kwargs</p><ul><li>we called it <strong>Variable Argument</strong>, <em>arg means positional arguments and **kwargs is keyword arguments. We’ve learn it the first day we use Python, but we have overlooked the most important symbol——\</em>(star).</li><li>the star symbol has two functions: pack and unpack, a sample:</li></ul></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">star_wrapper</span>(<span class="params">a, b, *c</span>):</span></span><br><span class="line">    <span class="built_in">print</span> c</span><br><span class="line">    <span class="built_in">print</span> <span class="built_in">type</span>(c)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">star_wrapper(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="comment"># (3, 4, 5)</span></span><br><span class="line"><span class="comment"># &lt;type &#x27;tuple&#x27;&gt;</span></span><br><span class="line"></span><br><span class="line">star_wrapper(*[<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"><span class="comment"># (4, 5, 6)</span></span><br><span class="line"><span class="comment"># &lt;type &#x27;tuple&#x27;&gt;</span></span><br></pre></td></tr></table></figure><p>When convey the arguments to a function, * allocate the paramters to the arguments in function in sequence. When the interpretor found a formal parameter with *, it will pack the unallocated actual parameter and convey it as a tuple to the formal parameter. Alike *, the double star(**) pack the keyword into a dictionary.</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Content&quot;&gt;&lt;a href=&quot;#Content&quot; class=&quot;headerlink&quot; title=&quot;Content&quot;&gt;&lt;/a&gt;Content&lt;/h1&gt;&lt;p&gt;In this blog, I’ll combine pre learning 2 and 3.&lt;br&gt;</summary>
    
    
    
    <category term="AI" scheme="http://cyx0706.github.io/categories/AI/"/>
    
    
    <category term="AI+X Blending" scheme="http://cyx0706.github.io/tags/AI-X-Blending/"/>
    
  </entry>
  
  <entry>
    <title>pre-learn-of-aix-week-1</title>
    <link href="http://cyx0706.github.io/2020/12/18/pre-learn-of-aix-1/"/>
    <id>http://cyx0706.github.io/2020/12/18/pre-learn-of-aix-1/</id>
    <published>2020-12-18T00:13:58.000Z</published>
    <updated>2020-12-26T07:46:26.197Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>This is the pre-learning record of my ai+x blending courses, there will be 5 weeks before the Base SPOC session starts, and each week, we will get a learning material in order to help us get ready for the upcoming study, here we start the first week’s pre-learning.<br><a id="more"></a></p><h2 id="Interviews-with-AI-Leaders"><a href="#Interviews-with-AI-Leaders" class="headerlink" title="Interviews with AI Leaders"></a>Interviews with AI Leaders</h2><p>There is a link of a video interview with a leader in the field of Machine Learning and AI.</p><blockquote><p>Video: Tomaso Poggio<br>Abstarct: Tomaso is a professor at MIT and is the director of the Center for Brains, Minds, and Machines. Cited over 100,000 times, his work has had a profound impact of intelligence, in both biological neural networks and artificial ones.</p></blockquote><p>So, what indeed he said in the video?</p><p>I record one of the most impressive question:</p><blockquote><p>how far we can get in creating intelligent systems without understanding the biological or understanding how the human creates intelligence.</p><p>It is a difficult problem, we are able to fly without using to much our knowledge about how birds fly. it was important I guess to know that you could have things heavier than air being able to fly like birds</p><p>in the case of intelligence I think that it’s a bit of a bet right now. …… the question is you can ask people do you think we’ll get there without any knowledge about the human brain.</p></blockquote><h2 id="Exercise-of-Python"><a href="#Exercise-of-Python" class="headerlink" title="Exercise of Python"></a>Exercise of Python</h2><p>There are exercise designed for we to learn the basics of Python, help us dignose the level of programming ability.</p><p>The exercise….too easy</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;--<span class="number">4</span></span><br><span class="line"><span class="number">4</span></span><br></pre></td></tr></table></figure><h2 id="Python-Documentation"><a href="#Python-Documentation" class="headerlink" title="Python Documentation"></a>Python Documentation</h2><ul><li>recommands <strong>“Think Python: How to Think Like a Computer Scientist”</strong> in the mail, I’ll borrow one from the school library.</li></ul><h2 id="Software"><a href="#Software" class="headerlink" title="Software"></a>Software</h2><p>we’ll be using the Python 2.6.x, so why not the Python3? Well, in any case, I need to prepare the environment.</p><p>It is easy for us the create a Python 2.6.x environment using the virtual environment, I’m so familiar with this, so I’d like to explore something interesting otherwise I will have nothing worth to write this blog.</p><h3 id="About-some-problems"><a href="#About-some-problems" class="headerlink" title="About some problems"></a>About some problems</h3><blockquote><p>ImportError: cannot import name _remove_dead_weakref</p></blockquote><p>I suggest you see the discussion in StackOverFlow <a href="https://stackoverflow.com/questions/49166873/pip-python-importerror-cannot-import-name-remove-dead-weakref">pip-python-importerror-cannot-import-name-remove-dead-weakref</a></p><p>in short, just click the <code>pythonw.exe</code> in the python2 floder and repair it!</p><h3 id="About-the-virtual-environment"><a href="#About-the-virtual-environment" class="headerlink" title="About the virtual environment"></a>About the virtual environment</h3><p>It is a magic to use different kind of type of Python. Thanks to the <code>PATH</code> which tells the Python interpreter which version and where is the Python.exe we use.</p><p>When we call the Python interpreter or run the python script xxx.py, shell will search the catalog listed in <code>PATH</code> until it found a python object matched for this command.</p><p><code>virtualenv</code> is a package for creating and managing the virtual environment of Python, it use tricks to achieve:</p><ul><li>change the <code>PATH</code> of current running Shell</li><li>change the runtime path stored in <code>sys.path</code></li></ul><p>to have a clear recognition of this, you can use the <code>python -m site</code> in different environment, you will found that almost everything in <code>sys.path</code> replaced by the path in the venv except the stardard libraries.</p><p>How we find the package in the “.py” file? The basis repo provide us a package “site”</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> site</span><br><span class="line">site.getsitepackages()</span><br><span class="line"><span class="comment"># your site-package path and base of python path</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;p&gt;This is the pre-learning record of my ai+x blending courses, there will be 5 weeks before the Base SPOC session starts, and each week, we will get a learning material in order to help us get ready for the upcoming study, here we start the first week’s pre-learning.&lt;br&gt;</summary>
    
    
    
    <category term="AI" scheme="http://cyx0706.github.io/categories/AI/"/>
    
    
    <category term="AI+X Blending" scheme="http://cyx0706.github.io/tags/AI-X-Blending/"/>
    
  </entry>
  
  <entry>
    <title>summary-of-2020-autumn</title>
    <link href="http://cyx0706.github.io/2020/12/12/summary-of-2020-autumn/"/>
    <id>http://cyx0706.github.io/2020/12/12/summary-of-2020-autumn/</id>
    <published>2020-12-12T12:12:18.000Z</published>
    <updated>2021-01-16T03:15:09.661Z</updated>
    
    <content type="html"><![CDATA[<p>封面：恭喜记录的地平线第三季定档2021，狂喜 :laughing:</p><p>发出来才发现不支持 emoji，回来看看能不能配一下吧</p><p>2020/12/12 更新，增加了 emoji 支持，<a href="https://hasaik.com/posts/9b280ea3.html">参考博客</a></p><p><a href="https://www.webfx.com/tools/emoji-cheat-sheet/">markdown emoji</a></p><a id="more"></a><h1 id="2020-年秋学期总结"><a href="#2020-年秋学期总结" class="headerlink" title="2020 年秋学期总结"></a>2020 年秋学期总结</h1><p>2020 年要结束了，2020 秋季学期也要结束了，没啥好说的，总结一下过去的这个学期吧</p><p>什么？你问为啥没有上半年？上半年在家摸了，荒废了没啥好说的</p><h2 id="看了啥书"><a href="#看了啥书" class="headerlink" title="看了啥书"></a>看了啥书</h2><ul><li>看完了《程序员的自我修养》</li><li>马上看完《浪潮之巅》了（思修课无聊都是那段时间看的）</li><li>开始看《一个64位操作系统的设计与实现》，正在学习如何写一个操作系统（还停留在进程创建那里，看的是真的慢）</li><li><del>Accel World，实教……</del></li></ul><h2 id="干了点啥"><a href="#干了点啥" class="headerlink" title="干了点啥"></a>干了点啥</h2><ul><li>各种专业课实验（这个其实不算）</li><li>软工课设，鸟类识别分享交流平台项目，个人负责 CV 模型，从此开始接触炼丹……，个人还是挺喜欢的，学习了 RestNet， VGG， BCNN， Inception， EfficientNet 等一些模型，但大部分都是看了看就直接用别人写好的了，自己其实学到的不多，后来做出来第一版模型后开始返回来读论文，感觉自己离 CV 方向的大门还差点，研究生是打算做人工智能，不一定是 CV，但多学点也不差么，毕竟有些是共通的（<del>其实根本没学明白，哪里来谈共同性？</del>）。客串了前端组（项目前期就我在干前端的活！），为了应用学习了 Vue，超级不系统，基本上是到处 copy，后来好了一点，开始自己写一些代码，自己封装组件，慢慢的也开始熟悉了一点用 Vue 做项目。</li><li>小学期国赛准备期间学了很多数学模型和 Python 的科学计算</li><li>LeetCode 刷题之旅开始了！漫漫长路，但起码坚持打双周晚周赛吧！<del>算法菜再不学就没救了</del></li><li><del>打通了 Rance 10</del></li></ul><h2 id="打算接下来（学期末到寒假）干啥"><a href="#打算接下来（学期末到寒假）干啥" class="headerlink" title="打算接下来（学期末到寒假）干啥"></a>打算接下来（学期末到寒假）干啥</h2><ul><li>冲分，求保研，拼命考试</li><li>假期复习《程序员的自我修养》，起码画思维导图吧，之前还想自己实现点东西的，这个看时间了</li><li>《一个64位操作系统的设计与实现》接着看，不着急，但每一大章后做个总结，代码一定要自己照着写然后跑一下</li><li>计网自顶而下，和期末考试复习一起食用吧</li><li>整理 Python 科学计算的代码，以及自己实现的黄书里面的一些算法和调库的例子，备战美赛</li><li>接着刷题，频率最好提高，每天刷不现实，假期能隔几天刷一上午一下午那种感觉就可以</li><li>英语听力，磨耳朵，多听吧</li><li>为下学期编译原理打打基础吧，可以翻出来 CSAPP 看看了</li><li>入坑 Rance 系列！！！（不是）</li></ul><h2 id="一点感想？"><a href="#一点感想？" class="headerlink" title="一点感想？"></a>一点感想？</h2><p>接触 CV 感受到了自己有多菜了，终于明白了那些本科都做这个研究的人有多强，尤其是看论文的时候深感自己的数学和英语多差了，海，自己耗子尾汁吧，要补了。但我自己计划也就这学期这门课的时间多学一点了，毕竟不是我当前的目标，保了大四有的是时间学这些，现在就打基础+拼分就好了</p><p>:expressionless: :expressionless: :expressionless: :expressionless: :expressionless:</p><p>总之就是 T5（大三上）努力拉高成绩，T6 结束后直接走夏令营线，T7 保研，然后用空出来大量时间点研究生的技能就好了，希望 A 结局能稳。</p><blockquote><p>你写完了年末总结，感觉自己充满了决心！ :fist:</p></blockquote>]]></content>
    
    
    <summary type="html">&lt;p&gt;封面：恭喜记录的地平线第三季定档2021，狂喜 :laughing:&lt;/p&gt;
&lt;p&gt;发出来才发现不支持 emoji，回来看看能不能配一下吧&lt;/p&gt;
&lt;p&gt;2020/12/12 更新，增加了 emoji 支持，&lt;a href=&quot;https://hasaik.com/posts/9b280ea3.html&quot;&gt;参考博客&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.webfx.com/tools/emoji-cheat-sheet/&quot;&gt;markdown emoji&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    
    <category term="essay" scheme="http://cyx0706.github.io/tags/essay/"/>
    
  </entry>
  
  <entry>
    <title>mini-os-2</title>
    <link href="http://cyx0706.github.io/2020/11/29/mini-os-2/"/>
    <id>http://cyx0706.github.io/2020/11/29/mini-os-2/</id>
    <published>2020-11-29T10:33:13.000Z</published>
    <updated>2020-12-12T13:20:33.218Z</updated>
    
    <content type="html"><![CDATA[<p>这是第四章的补充知识</p><p>尚未完成啊啊啊</p><a id="more"></a><h1 id="GNU-C-内嵌汇编语言"><a href="#GNU-C-内嵌汇编语言" class="headerlink" title="GNU C 内嵌汇编语言"></a>GNU C 内嵌汇编语言</h1><ul><li>__asm__ 关键字：用于声明这行代码是一个内嵌汇编的表达式</li><li>__volatile__ 关键字：其作用是告诉编译器此行代码不能被编译器优化，编译时保持代码原状。否则经过编译器优化后，汇编语句很可能被修改以至于不能达到预期的效果。</li></ul><h2 id="内嵌汇编表达式"><a href="#内嵌汇编表达式" class="headerlink" title="内嵌汇编表达式"></a>内嵌汇编表达式</h2><p>GNU C 语言的内嵌汇编表达式由四个部分组成，它们之间使用 “:” 号分隔，其完整的格式为：<code>指令部分:输出部分:输入部分:损坏部分</code></p><h3 id="指令部分"><a href="#指令部分" class="headerlink" title="指令部分"></a>指令部分</h3><p>汇编代码本身，其书写格式与 AT&amp;T 汇编语言程序的格式基本相同，但有些微变化：</p><ul><li>当指令表达式中存在多条汇编代码的时候，可以全部书写在一对双引号中；亦可将汇编代码放在多对双引号中。如果将所有指令编写放在同一双引号中，那么相邻两条指令之间必须使用分号（;）或者换行符（\n）分隔，如果使用了换行符那么通常还会跟一个制表符（\t）。当汇编代码引用寄存器时，必须在寄存器名前再加上一个 % 符，以表示对寄存器的引用，例如代码 <code>&quot;movl $0x10, %%eax&quot;</code></li></ul><h3 id="输出部分"><a href="#输出部分" class="headerlink" title="输出部分"></a>输出部分</h3><p>记录指令的部分的输出信息，其格式为：<strong>“输出操作约束”(输出表达式),输出操作约束”(输出表达式),</strong> …。格式中的输出操作约束和输出表达式成对出现，每部分之间用逗号（,）分割。</p><ul><li>输出表达式部分主要负责保存指令部分的执行结果，通常情况下输出表达式是一个变量。</li><li>双引号内的部分，被称为<strong>输出操作约束</strong>，输出约束必须用等号和加号修饰，等号表示这是一个纯粹的输出的标识，加号表示既用于输出也用于输入。不论加号还是等号，都只能用在输出部分，不能用在输入部分。</li></ul><h3 id="输入部分"><a href="#输入部分" class="headerlink" title="输入部分"></a>输入部分</h3><p>记录指令部分的输入信息，其格式 <strong>“输出操作约束”(输出表达式),输出操作约束”(输出表达式),</strong> …。同样成对出现，但输入操作约束中不允许使用等号和加号，因此输入部分都是只读。</p><h3 id="损坏部分"><a href="#损坏部分" class="headerlink" title="损坏部分"></a>损坏部分</h3><p>描述了在指令部分执行的过程中，将被修改的寄存器，内存空间或标志寄存器，并且这些修改部分并未在输出部分和输入部分出现过，格式为 <strong>“损坏描述”, “损坏描述”,</strong>…，如果要声明多个寄存器，则需要逗号隔开。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;这是第四章的补充知识&lt;/p&gt;
&lt;p&gt;尚未完成啊啊啊&lt;/p&gt;</summary>
    
    
    
    <category term="ComputerOrganization" scheme="http://cyx0706.github.io/categories/ComputerOrganization/"/>
    
    
    <category term="OperatingSystems" scheme="http://cyx0706.github.io/tags/OperatingSystems/"/>
    
  </entry>
  
  <entry>
    <title>complain</title>
    <link href="http://cyx0706.github.io/2020/11/28/complain/"/>
    <id>http://cyx0706.github.io/2020/11/28/complain/</id>
    <published>2020-11-28T15:40:26.000Z</published>
    <updated>2020-11-28T15:44:22.668Z</updated>
    
    <content type="html"><![CDATA[<h2 id="￥"><a href="#￥" class="headerlink" title="@#￥@#%@%"></a>@#￥@#%@%</h2><p>记于2020年11月27日某个很坑的课实验做不出来</p><p>（28日再看自己写的好羞耻啊）</p><a id="more"></a><p>心情很差，随便写点东西吧</p><p>先讲讲自己，我不知道什么时候开始对自己失望，我还记得大一的时候，我满怀的热情，考入了转专业直通车慧与班，能学自己感兴趣的计算机专业。大一加入了爱特工作室，学习的虽然是怎么写网站，后端，sql这些在某些人眼里”廉价”的东西，但对于什么都不懂得我来说，说实话，很cool，很有趣，让我在枯燥的c语言（大一上专业课）之外有了愿意付出时间学习研究的东西，那个时候的真的很好，每周都有各种各样的bug，但历经千辛万苦解决后看着成功运行的功能，很高兴。就这样在爱特度过了大一的一年。绩点并不高（在全年级里）</p><p>时间来到了大二，我发现身边的人都在学算法（很后悔当时没有加入他们），我就觉得，为了打比赛去学这些，太功利了，我在看着没人愿意看的汇编，链接，库。让我印象很深的是:计基的实验，用的是csapp的炸弹实验和覆盖栈指针返回位置攻击实验（忘了叫啥名字了），我做了很久终于解决后很高兴，想要和我的同学们交流思路的时候，我得到的只是冷冰冰的回复:这种实验网上照着做不就行了吗。我沉默了，确实，像我这种”吃力不讨好”的事没人（有可能是在我身边我不认识吧）愿意做。当我看书学习到自闭，看不懂的时候，身边没人和我交流，大一的时候，还和社团的别人讨论，现在都没有了，大家都在各忙各的。大二下，在家度过，学数学建模，很多时候就觉得自己很累，但还是咬着牙坚持吧。</p><p>大二，绩点没啥大变化。然后我排名就掉了很多。</p><p>老师学长口中”保研”的压力开始出现，我估摸着算了一下，现在踩着边。我有点慌了，我有些科目考的确实不好，甚至好好学了还没划划水的考的好，我承认自己菜，复习不到位。大三好好努力吧。<br>大二暑期小学期，国赛没打好，下次再努力吧。我选了web框架这门课（上了几次课我发现这门课根本不讲框架），少了好多熟人，我就很有疑问。问了问，他们告诉我:这门课给分低，不选。实话实说，我觉得不同老师给分差别很大，我也体验到了，但我一直以来都觉得无所谓，从此时，我开始觉得:绩点重要!最明显的是这次选课吧，按照大一的我，肯定就:这个课有意思，选。选不到好老师，没啥，目的是排另一门课。现在选门课问三问四，非xxx老师不选，无非:给分高吗，事多吗。选课的时候我还觉得没啥，身边的不少人也都在这样做。但晚上躺在床上胡思乱想的时候，就很难受，短短2年自己已经变了这么多。功利心直线上升，学习就是为了保研，保不了就炸了这种病态的想法开始蚕食我自己。我们抛开那些所谓的”课上学不到东西，水水就行了，全靠课下自学所以选个水的老师”这种言论，我个人在好多课上都能学到很多东西。当我要选一门”很坑的课t”的时候，竟然有人说”他怎么敢”这种话。（事实上这门课确实坑，实验环境都仿佛是上个世纪的东西，据说给分超低） 但实话实说，学到了很多东西。<br>我庆幸我还有自己最后的倔强:大三我在学写操作系统（这门专业课讲的太哲学了），我还想在大三下好好系统学编译原理（据说课上几乎啥也没交）。</p><p>我觉得，如果你按照学校安排的课程学几乎啥也学不会。所以就多学别的啊，但看着别人分考的越来越高（我菜，两面兼顾的我身边也有，比我小一级），我悠闲的心态也变了。大学真的和我想的不一样了，用你们的话说”内卷”，我也身处其中，不得不卷了，拼高分，拼光鲜的奖。但每每有些时候，我都觉得，自己已经不是自己了，可能长大了吧，小孩子那种幼稚的想法:学到东西就是好，分尽力考已经不现实了，你要用尽全力考高分（包括争取一些非能力的因素如老师），进好的研究生学校。<br>逻辑写的有点乱，没理没据，就是自己菜，还考不好的无奈的说辞。</p><p>希望你们不要像我一样，自闭，多去交流吧，计算机那么多东西，网上总有人和你有同样的爱好，兴趣，愿你能找到更多的知音。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;￥&quot;&gt;&lt;a href=&quot;#￥&quot; class=&quot;headerlink&quot; title=&quot;@#￥@#%@%&quot;&gt;&lt;/a&gt;@#￥@#%@%&lt;/h2&gt;&lt;p&gt;记于2020年11月27日某个很坑的课实验做不出来&lt;/p&gt;
&lt;p&gt;（28日再看自己写的好羞耻啊）&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>cuda 安装受难记</title>
    <link href="http://cyx0706.github.io/2020/11/20/cuda-disaster/"/>
    <id>http://cyx0706.github.io/2020/11/20/cuda-disaster/</id>
    <published>2020-11-20T01:08:25.000Z</published>
    <updated>2020-11-22T02:38:59.434Z</updated>
    
    <content type="html"><![CDATA[<ul><li>2020/11/22 更新了解决下载过慢的问题</li></ul><h1 id="如何无痛安装-cuda-Anaconda-pytroch-环境"><a href="#如何无痛安装-cuda-Anaconda-pytroch-环境" class="headerlink" title="如何无痛安装 cuda + Anaconda + pytroch 环境"></a>如何无痛安装 cuda + Anaconda + pytroch 环境</h1><p>!! 重要声明，这仅仅只是我的痛苦安装过程的一个踩坑记录，如果可以帮助你无痛安装，那自然是极好的了。<br><a id="more"></a></p><h2 id="Anaconda"><a href="#Anaconda" class="headerlink" title="Anaconda"></a>Anaconda</h2><p>Anaconda 是一个python 的发行版，包括了python 和很多常见的软件库，和一个包管理器 conda</p><p>但它巨大，囊括了1000+的 Python 库，如果你不需要，强烈建议你安装 mini 版本</p><p>我没有遇到很多的问题，但建议你看这个，我们的重点不在这里：</p><p><a href="https://zhuanlan.zhihu.com/p/75717350">https://zhuanlan.zhihu.com/p/75717350</a></p><h2 id="cuda"><a href="#cuda" class="headerlink" title="cuda"></a>cuda</h2><p>Anaconda 安装完毕后，不要急着安装 pytorch，先去安装 CUDA Toolkit！！！</p><p>在这些之前，你需要看看你的 GPU 驱动的版本，查看的方法很简单：</p><p>找到你的电脑的 NVIDIA 控制面板，打开，左下角点击系统信息，它看起来就像是这样：</p><p><img src="https://i.loli.net/2020/11/22/qXM3VebopnB7GjI.png" alt="找到系统信息"></p><p><img src="https://i.loli.net/2020/11/22/taCQ8iznsBjXKcd.png" alt="查看 driver 的版本"></p><p>然后去官网找到你的 driver 匹配的版本 <a href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#abstract">Release note</a>，</p><p>找好之后去下载 <a href="https://developer.nvidia.com/cuda-toolkit-archive">cuda downloads</a></p><p>然后安装，可以自定义安装，我们只选择 Development，Runtime，Sample，Document 这四个组件即可。可以修改路径，如果不想安在C盘的话就改改。</p><p>安装完成后还需要配置系统环境变量，在<strong>系统环境变量那里和 PATH 同级</strong>的地方添加下面的环境变量：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CUDA_PATH &#x3D; D:\NVIDIA GPU Computing Toolkit\CUDA\v10</span><br><span class="line">CUDA_PATH_V10_0 &#x3D; D:\NVIDIA GPU Computing Toolkit\CUDA\v10</span><br><span class="line"></span><br><span class="line">CUDA_SDK_PATH &#x3D; D:\NVIDIA GPU Computing Toolkit\sample</span><br><span class="line">CUDA_LIB_PATH &#x3D; %CUDA_PATH%\lib\x64</span><br><span class="line">CUDA_BIN_PATH &#x3D; %CUDA_PATH%\bin</span><br><span class="line">CUDA_SDK_BIN_PATH &#x3D; %CUDA_SDK_PATH%\bin\win64</span><br><span class="line">CUDA_SDK_LIB_PATH &#x3D; %CUDA_SDK_PATH%\common\lib\x64</span><br></pre></td></tr></table></figure><p>在<strong>用户环境变量的 PATH 里</strong>添加下面的部分，如果你是文本编辑记得加上分号。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%CUDA_LIB_PATH%</span><br><span class="line">%CUDA_BIN_PATH%</span><br><span class="line">%CUDA_SDK_BIN_PATH%</span><br><span class="line">%CUDA_SDK_LIB_PATH%</span><br></pre></td></tr></table></figure><p>重启电脑进入你的 cuda 安装路径，<code>\extras\demo_suite</code> 路径，执行 deviceQuery.exe 和 bandwidthTest.exe 程序，会有一堆输出，如果显示了 <code>Result = PASS</code>，证明你安装成功了。这时你的任务管理器会多一栏 GPU。</p><div align="center"><img src=https://i.loli.net/2020/11/22/5SVJI86HiQjGlZc.png width="80%" height="50%" /></div><h3 id="Nvidia-显示设置不可用"><a href="#Nvidia-显示设置不可用" class="headerlink" title="Nvidia 显示设置不可用"></a>Nvidia 显示设置不可用</h3><p>自己平常明明可以正常打开控制面板，但今天我就突然打不开了……，看了看网上说的，大多数是让你更新一下驱动，好吧，那么我们更新一下吧。</p><p>右键右下角窗口里面的 NVIDIA 图标，点 NVIDIA GeForce experience，竟然需要登录，好吧，就注册一个账号然后登录……，然后就无法注册。我人傻了，只好去官网上重新下一个 <a href="https://www.nvidia.cn/geforce/drivers/">https://www.nvidia.cn/geforce/drivers/</a> 下载自动更新驱动程序。</p><div align="center"><img src=https://i.loli.net/2020/11/22/q5o7Ze6EF43tD8V.png width="80%" height="50%" /></div><p>之后就正常的安装，然后注册登录，它会自动提示你更新驱动。（我从2017年的驱动更新到了最新的…），还有一个注意的是不知道为啥它死活不能用QQ登录，会卡在认证的页面上。</p><h2 id="pytorch"><a href="#pytorch" class="headerlink" title="pytorch"></a>pytorch</h2><p>在官网选择你的系统配置：</p><p><a href="https://pytorch.org/get-started/locally/">install</a></p><p><img src="https://i.loli.net/2020/11/22/eK65hnij2sMDcxO.png" alt="QQ截图20201122101112.png"></p><p>用下面的命令一下安装好，新建一个 py 文件测试一下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Using gpu: %s&quot;</span> % torch.cuda.is_available())</span><br></pre></td></tr></table></figure><p>显示 “Using gpu: xxx” 针不戳。</p><p>是不是很简单？才怪！</p><h3 id="下载速度太慢"><a href="#下载速度太慢" class="headerlink" title="下载速度太慢"></a>下载速度太慢</h3><p>如果下载速度太慢，建议添加清华的源</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;free&#x2F;</span><br><span class="line">conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;main&#x2F;</span><br><span class="line">conda config --set show_channel_urls yes</span><br><span class="line"># 添加第三方 conda 源</span><br><span class="line">conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud&#x2F;conda-forge&#x2F;</span><br><span class="line">conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud&#x2F;msys2&#x2F;</span><br><span class="line">conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud&#x2F;bioconda&#x2F;</span><br><span class="line">conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud&#x2F;menpo&#x2F;</span><br><span class="line">conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud&#x2F;pytorch&#x2F;</span><br></pre></td></tr></table></figure><p>这时执行安装就不要再加 <code>-c</code> 了</p><p><code>conda install pytorch==1.2.0 torchvision==0.4.0 cudatoolkit=10.0</code></p><h3 id="CUDA-版本太低"><a href="#CUDA-版本太低" class="headerlink" title="CUDA 版本太低"></a>CUDA 版本太低</h3><p>这里面并没有 CUDA10.0 这个版本，但是我自作聪明的修改 cudatoolkit=10.0 也正常安装了，不错，然后运行一下，提示 CUDA 版本太低…..，我严重怀疑是版本不匹配的问题，cuda 是不想再装一遍了，只好重装一个低版本的 pytorch。</p><p>学老实点，去早期版本找匹配吧 <a href="https://pytorch.org/get-started/previous-versions/">previous versions</a></p><p>找到一条适合你的版本的命令，比如说我的 <code>conda install pytorch==1.2.0 torchvision==0.4.0 cudatoolkit=10.0 -c pytorch</code></p><p>然后安装就好了，记得先卸掉你装过的 cudatoolkit。打开 Anaconda Prompt，输入 <code>conda uninstall cudatoolkit</code> 即可。</p><h3 id="Solving-environment-failed-with-initial-frozen-solve……"><a href="#Solving-environment-failed-with-initial-frozen-solve……" class="headerlink" title="Solving environment: failed with initial frozen solve……."></a>Solving environment: failed with initial frozen solve…….</h3><p>安装了很久还没成功，事情似乎没有那么简单，看了报错，<strong>Solving environment: failed with initial frozen solve. Retrying with flexible solve.</strong></p><p>具体什么原因我还没搞明白，可能是依赖的问题。遇事不决先升级，<del>不行就重装</del>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda update -n base conda</span><br><span class="line">conda update --all</span><br></pre></td></tr></table></figure><p>还是不行，只好去百度，试了不少法子，放一个我最终成功解决问题的方法：<a href="https://github.com/conda/conda/issues/9367">issue#93676</a></p><p>具体来说就是起一个虚拟环境然后再重新安装，有人说这是 Anaconda3 的一个最新版的问题，我也不懂也别乱说了，就照着别人的方法试就完事了:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda create -n your_virtual_environment_name(I use the pytorch)</span><br><span class="line">conda activate pytorch</span><br><span class="line">conda install pytorch&#x3D;&#x3D;1.2.0 torchvision&#x3D;&#x3D;0.4.0 cudatoolkit&#x3D;10.0 -c pytorch</span><br></pre></td></tr></table></figure><p>具体点来看是用了一个旧一些的版本的 conda 的库（可能），这么说前面更新还是不应该更的了吗……，不过这发生在你装一个老版本的库时，我觉得把cuda更新到最新，装个最新的 pytorch 不香吗</p><h2 id="写在后面"><a href="#写在后面" class="headerlink" title="写在后面"></a>写在后面</h2><p>每次安环境都是最痛苦的，希望人没事。后续一些问题我会跟进增加内容</p>]]></content>
    
    
    <summary type="html">&lt;ul&gt;
&lt;li&gt;2020/11/22 更新了解决下载过慢的问题&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;如何无痛安装-cuda-Anaconda-pytroch-环境&quot;&gt;&lt;a href=&quot;#如何无痛安装-cuda-Anaconda-pytroch-环境&quot; class=&quot;headerlink&quot; title=&quot;如何无痛安装 cuda + Anaconda + pytroch 环境&quot;&gt;&lt;/a&gt;如何无痛安装 cuda + Anaconda + pytroch 环境&lt;/h1&gt;&lt;p&gt;!! 重要声明，这仅仅只是我的痛苦安装过程的一个踩坑记录，如果可以帮助你无痛安装，那自然是极好的了。&lt;br&gt;</summary>
    
    
    
    <category term="AI" scheme="http://cyx0706.github.io/categories/AI/"/>
    
    
    <category term="cuda" scheme="http://cyx0706.github.io/tags/cuda/"/>
    
  </entry>
  
</feed>
