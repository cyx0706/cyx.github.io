<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>deep-learning-intro - Ctwo&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Ctwo&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Ctwo&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="深度学习中的数学基础软工作业 Week 2  概率是基础 支持向量机涉及很多数学基础 梯度下降是神经网络的共同基础"><meta property="og:type" content="blog"><meta property="og:title" content="deep-learning-intro"><meta property="og:url" content="http://cyx0706.github.io/2020/10/24/deep-learning-intro/"><meta property="og:site_name" content="Ctwo&#039;s Blog"><meta property="og:description" content="深度学习中的数学基础软工作业 Week 2  概率是基础 支持向量机涉及很多数学基础 梯度下降是神经网络的共同基础"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://cyx0706.github.io/gallery/thumbnails/deep-learning.jpg"><meta property="article:published_time" content="2020-10-24T06:50:43.000Z"><meta property="article:modified_time" content="2021-02-27T15:05:59.448Z"><meta property="article:author" content="Ctwo"><meta property="article:tag" content="Deep Learning"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/gallery/thumbnails/deep-learning.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://cyx0706.github.io/2020/10/24/deep-learning-intro/"},"headline":"deep-learning-intro","image":["http://cyx0706.github.io/gallery/thumbnails/deep-learning.jpg"],"datePublished":"2020-10-24T06:50:43.000Z","dateModified":"2021-02-27T15:05:59.448Z","author":{"@type":"Person","name":"Ctwo"},"publisher":{"@type":"Organization","name":"Ctwo's Blog","logo":{"@type":"ImageObject","url":"http://cyx0706.github.io/img/logo.svg"}},"description":"深度学习中的数学基础软工作业 Week 2  概率是基础 支持向量机涉及很多数学基础 梯度下降是神经网络的共同基础"}</script><link rel="canonical" href="http://cyx0706.github.io/2020/10/24/deep-learning-intro/"><link rel="alternate" href="/atom.xml" title="Ctwo&#039;s Blog" type="application/atom+xml"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/5.13.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/railscasts.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.2.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Ctwo&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="/gallery/thumbnails/deep-learning.jpg" alt="deep-learning-intro"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-10-24T06:50:43.000Z" title="2020-10-24T06:50:43.000Z">2020-10-24</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-02-27T15:05:59.448Z" title="2021-02-27T15:05:59.448Z">2021-02-27</time></span><span class="level-item"><a class="link-muted" href="/categories/AI/">AI</a></span></div></div><h1 class="title is-3 is-size-4-mobile">deep-learning-intro</h1><div class="content"><h1 id="深度学习中的数学基础"><a href="#深度学习中的数学基础" class="headerlink" title="深度学习中的数学基础"></a>深度学习中的数学基础</h1><p><del>软工作业 Week 2</del></p>
<ul>
<li>概率是基础</li>
<li>支持向量机涉及很多数学基础</li>
<li>梯度下降是神经网络的共同基础<a id="more"></a>
<h2 id="矩阵线性变换"><a href="#矩阵线性变换" class="headerlink" title="矩阵线性变换"></a>矩阵线性变换</h2></li>
</ul>
<p>对于给定的矩阵 A，假设其特征值为 $\lambda$，特征向量为x，则他们之间的关系如下:</p>
<script type="math/tex; mode=display">
Ax = \lambda x</script><div align=center><img width=80% src="https://i.loli.net/2020/10/23/lgupfzZInUJeLdF.png" ></div>

<p>矩阵 A 对于 A 的特征向量做线性变换的时候，方向不再发生变化。</p>
<ul>
<li>从线性变换的角度，矩阵相乘对原始向量同时施加方向变化和尺度变化。</li>
<li>对于有些特殊的向量，矩阵的作用只有尺度变化而没有方向变化，这类向量就是特征向量，变化系数就是特征值</li>
</ul>
<h2 id="矩阵的秩"><a href="#矩阵的秩" class="headerlink" title="矩阵的秩"></a>矩阵的秩</h2><ul>
<li><p>线性方程组的角度：</p>
<ul>
<li>度量矩阵行列之间的相关性</li>
<li>如果矩阵的各行或列是线性无关的，矩阵就是满秩的，也就是说秩等于行数</li>
</ul>
</li>
<li><p>数据点分布的角度：</p>
<ul>
<li>表示数据需要的最小的基数量</li>
<li>数据分布模式越容易被捕捉，即需要的基越少，秩就越小</li>
<li>数据冗余度越大，需要的基就越小，秩越小</li>
<li>若矩阵表达的是结构化信息，如图像，用户-物品表等，个行之间存在一定相关性，一般是低秩的</li>
</ul>
</li>
</ul>
<h2 id="低秩近似"><a href="#低秩近似" class="headerlink" title="低秩近似"></a>低秩近似</h2><p>较大奇异值包含了矩阵的主要信息，只保留前r个较大奇异值及其对应的特征向量，一般 r 取 d 的十分之一就可以保留足够信息，可以实现数据从 <code>n x d</code> 维降维到 <code>n x r + r x r + r x d</code></p>
<p>矩阵低秩近似:</p>
<script type="math/tex; mode=display">
A_{m \times n} = U_{m \times n} \textstyle \sum_{n \times n}^{} V_{k \times n}^T \quad rank(A)=r
\\
\hat{A}_{m \times n} = U_{m \times n} \textstyle \sum_{n \times n}^{} V_{k \times n}^T \quad rank(\hat A)=r</script><p>图像去噪：数据矩阵X一般同事包含结构信息和噪声</p>
<p>矩阵分解为两个矩阵的相加，一个是低秩的（结构信息造成行或列间线性相关）另一个是稀疏的（噪声是稀疏的），很多问题中都会有这样类似的优化方法，写成函数如下:</p>
<script type="math/tex; mode=display">
\min_{A,E} \,rank(A) + \lambda \left \| E \right \|_0
\\
s.t.\, X = A+E</script><h2 id="概率函数形式统一"><a href="#概率函数形式统一" class="headerlink" title="概率函数形式统一"></a>概率函数形式统一</h2><p>同一个模型可以用概率形式和函数形式来表示</p>
<p><img src="https://i.loli.net/2020/10/23/bRlFCvdGeALtNyz.png" alt=""></p>
<p>指数族：对常见分布标准化的形式；广义线性模型：线性回归，逻辑回归都是广义线性回归</p>
<p>指数族决定是高斯分布，那么广义线性模型就是线性回归模型。</p>
<p>例子：对于逻辑回归模型，假定的概率分布是伯努利分布，根据伯努利分布的定义，其概率质量函数 PMF 为：</p>
<script type="math/tex; mode=display">
P(Y=n)=
\begin{cases}
 1-p \quad n=0\\p \quad n=1
\end{cases}</script><p>似然函数可以写成：$L(\theta)=\prod_{i=1}^{m}P(y=1|x_i)^{y_i}P(y=0|x_i)^{1-y_i}$</p>
<p>常规操作——取对数，然后求最大化的对数似然</p>
<script type="math/tex; mode=display">
max \;lnL(\theta)\longrightarrow min \;-lnL(\theta)=\sum_{i=1}^{m}-y_i ln P(y=1|x_i)-(1-y_i)ln(1-P(y=1|x_i))</script><p>这个公式的每个加数子项表示单个样本的对数损失：若yi=1，P(y=1|x)越大损失越小；yi=0，P(y=1|x) 越小损失越小。</p>
<p>这些样本的损失，实际上和逻辑回归的经验风险损失$J(\theta)$很相近，逻辑回归也可以直接采用对数损失函数通过经验风险最小化求参，而经验风险最小化策略与极大似然策略优化得到的模型参数是一致的。</p>
<h2 id="策略设计"><a href="#策略设计" class="headerlink" title="策略设计"></a>策略设计</h2><p>衡量训练误差与泛化误差的差异：计算学习理论，它得出来一个结论——训练误差与泛化误差的差异和训练样本样本量与模型的复杂程度有关，样本量越大误差越小，模型越复杂差异越大</p>
<p>PAC 理论给出了实际训练学习器的目标</p>
<ul>
<li>从<strong>合理数量</strong>的训练数据中通过<strong>合理计算量</strong>学习到<strong>可靠的知识（置信度高）</strong></li>
</ul>
<h3 id="奥卡姆剃刀原理"><a href="#奥卡姆剃刀原理" class="headerlink" title="奥卡姆剃刀原理"></a>奥卡姆剃刀原理</h3><p>如无必要，勿增实体：简单有效原理（说白了，如果你有两个方案，选择最简单的，多出来的东西并非有益，反而会带来麻烦）</p>
<p>如果多种模型能够同等程度地符合一个问题的观测结果，那么应该选择其中使用假设最少的<strong>最简单的模型</strong></p>
<h3 id="过拟合-amp-欠拟合"><a href="#过拟合-amp-欠拟合" class="headerlink" title="过拟合&amp;欠拟合"></a>过拟合&amp;欠拟合</h3><p>欠拟合：训练集的一般性质没有被学习器学好（训练误差大）</p>
<ul>
<li>提高模型复杂度<ul>
<li>决策树：拓展分支</li>
<li>神经网络：增加训练轮数</li>
</ul>
</li>
</ul>
<p>过拟合：学习器把训练集特点当做样本一般的特点（训练误差小，测试误差大）</p>
<ul>
<li>降低模型复杂度<ul>
<li>优化目标加正则项</li>
<li>决策树：剪枝</li>
<li>神经网络：early stop, dropout</li>
<li>数据增广（扩大数据训练集）：如计算机视觉可以将图像旋转，缩放，剪切；自然语言处理中可以将同义词替换；语音识别中可以加入随机噪声。</li>
</ul>
</li>
</ul>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>BP 神经网络和损失函数：</p>
<p>如果设定$S(x)=\frac{1}{1+e^-x}$，损失函数</p>
<script type="math/tex; mode=display">
L=\frac{1}{2}\sum_{k=1}^{N}D(f(x_k), h(x_k)) = \frac{1}{2}\sum_{k=1}^{N}\sum_{i=1}^{c}(f_i(x_k)-x_{k_i}^{(\mathfrak{h})})^2=\sum_{k=1}^{N}E_k</script><p>令 $E_k=\sum_{i=1}^{c}(f_i(x_k)-x_{k_i}^{(\mathfrak{h})})^2$ 为每个样本的平方损失代价</p>
<p>待优化参数：权重 $\omega_{ij}^{(t)}$</p>
<p>待优化算法：梯度下降，需要计算代价函数L和梯度 $\frac{\partial L}{\partial \omega_{ij}^{(t)}}$</p>
<p>更新公式：$\omega_{ij}^{(t)}\leftarrow \omega_{ij}^{(t)}- \eta\frac{\partial E_k}{\partial \omega_{ij}^{(t)}}$</p>
<h3 id="平方损失"><a href="#平方损失" class="headerlink" title="平方损失"></a>平方损失</h3><p>$\frac{\partial L}{\partial w_i}=2(a-y)\cdot a(1-a)\cdot x_i$ 计算困难，采用交叉熵（对数损失函数）</p>
<p>$L(a,y)=-yloga-(1-y)log(1-a)$ 巴拉巴拉……听不懂了这里，改天再改改</p>
<h2 id="概率学派-amp-贝叶斯学派"><a href="#概率学派-amp-贝叶斯学派" class="headerlink" title="概率学派&amp;贝叶斯学派"></a>概率学派&amp;贝叶斯学派</h2><ul>
<li>频率学派：关注可独立重复的随机试验中单个事件发生的频率，假设概率是客观存在的并且是固定的。这样得出的模型参数唯一，需要从有限的观测数据中估计参数值。</li>
<li>贝叶斯学派：关注随机事件的”可信程度”，在数据之上加入假设，数据作用是对初始的假设做出修正，使观察者对概率的主观认识（先验）更接近客观实际（观测）；模型参数本身是随机变量，需要顾及参数的整个概率分布。</li>
</ul>
<p><img src="https://i.loli.net/2020/10/23/Y3I4Nk9m6TiVzrJ.png" alt=""></p>
<h1 id="深度学习概述"><a href="#深度学习概述" class="headerlink" title="深度学习概述"></a>深度学习概述</h1><h2 id="Yule-Simpson-悖论"><a href="#Yule-Simpson-悖论" class="headerlink" title="Yule-Simpson 悖论"></a>Yule-Simpson 悖论</h2><p>相关性并不可靠，会因为一些混杂因素而改变。</p>
<p>因果性=相关性+忽略的因素</p>
<h2 id="卷积神经网络（Convolutional-Neural-Network）"><a href="#卷积神经网络（Convolutional-Neural-Network）" class="headerlink" title="卷积神经网络（Convolutional Neural Network）"></a>卷积神经网络（Convolutional Neural Network）</h2><p>全连接网络处理图像的问题：</p>
<ul>
<li>参数太多：权重矩阵的参数太多 -&gt; 过拟合</li>
</ul>
<p>卷积神经网络的解决方式：</p>
<ul>
<li>局部关联，参数共享</li>
</ul>
<h3 id="卷积神经网络的应用"><a href="#卷积神经网络的应用" class="headerlink" title="卷积神经网络的应用"></a>卷积神经网络的应用</h3><ul>
<li>分类</li>
<li>检索</li>
<li>检测</li>
<li>分割</li>
<li>人脸识别，人脸验证</li>
<li>表情识别，图像生成，图像风格转化，自动驾驶</li>
<li>……</li>
</ul>
<h3 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h3><p>一维卷积经常用在信号处理中，用于计算信号的延迟累积。假设一个信号发生器在时刻 t 发出一个信号xi，其信息衰减率为 fk，即在 k-1 个时间步长后，信息衰减为原来的 fk 倍。</p>
<p>我们设 f1=1,f2=0.5,f3=0.25，在 t 时刻收到的信号 yt 为当前时刻产生的信息和以前时刻延迟信息的叠加：</p>
<script type="math/tex; mode=display">
yt=1\times x_i+\frac{1}{2} \times x_{i-1} + \frac{1}{4} \times x_{i-2}
=\sum_{3}^{k=1}f_k \cdot x_{i-k+1}</script><p>此处的 $f=[f_1,f_2,f_3]$ 被称为滤波器（filter）或者叫卷积核（convolutional kernel），假设滤波器 f 的长度为 m，它和一个信号序列 $x=[x_1,x_2,x_3,…]$ 的卷积记为 $y_t = \sum_{m}^{k=1}f_k \cdot x_{i-k+1}$</p>
<p>那么卷积是什么?</p>
<blockquote>
<p>convolution is an operation on two functions of a real-valued argument.</p>
<p>卷积是对两个实变函数的一种数据操作</p>
</blockquote>
<p><img src="https://i.loli.net/2020/10/24/6q5QpEhKezkFAyN.png" alt="卷积示例"></p>
<p>卷积核的输出等于卷积核的与输入对应位置的内积（逐个对应相乘再相加），粉色位置的值应为4（=1x1+1x0+1x1+0x0+1x1+1x0+0x1+0x0+1x1），步长为1，每次向右移动1格，这样特征图每一行就有3个数值了。</p>
<p>但有的时候，我们的输入无法被指定的步长分割好，这时候往往加入 padding，即在最外圈补一定数量的0</p>
<p><img src="https://i.loli.net/2020/10/24/FfOeG3LtNc85QxP.png" alt="卷积示例"></p>
<p>输出的特征图的大小：</p>
<ul>
<li>(N(input_scale)-F(filter_scale))/stride(step)+1</li>
<li>(N+padding*2-F)/stride+1</li>
</ul>
<p>加入深度：会直接影响输出的层数，有几层特征图的输出同样也有几层，(相当于使用不同的卷积核处理的次数?)。</p>
<h3 id="池化（Pooling）"><a href="#池化（Pooling）" class="headerlink" title="池化（Pooling）"></a>池化（Pooling）</h3><ul>
<li>保留了主要特征的同时减少参数和计算量，防止过拟合来提高模型的泛化能力</li>
<li>一般处于卷积层和卷积层之间，全连接层与全连接层之间</li>
</ul>
<p>常用的有最大值池化和平均值池化：</p>
<p><img src="https://i.loli.net/2020/10/24/dQq1XRB4SlhG7Ex.png" alt="池化示例"></p>
<p>分类识别任务的时候，最大值池化比平均值池化效果更好。</p>
<h3 id="全连接（Fully-Connecting）"><a href="#全连接（Fully-Connecting）" class="headerlink" title="全连接（Fully Connecting）"></a>全连接（Fully Connecting）</h3><p>全连接层（FC layer）</p>
<ul>
<li>两层之间的所有神经元都有权重链接</li>
<li>通常全连接层在卷积神经网络尾部</li>
<li>全连接层参数量通常很大</li>
</ul>
<h2 id="各种深度学习的模型概述"><a href="#各种深度学习的模型概述" class="headerlink" title="各种深度学习的模型概述"></a>各种深度学习的模型概述</h2><h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p>ReLU 激活函数的优点:</p>
<ul>
<li>解决了梯度消失的问题</li>
<li>计算的速度很快，只需要判断输入是否大于0</li>
<li>收敛速度远快于 sigmod</li>
</ul>
<p>使用 dropout 来防止过拟合: 训练的时候随机关闭一些神经元，测试时整合所有的神经元。</p>
<h3 id="ZFNet"><a href="#ZFNet" class="headerlink" title="ZFNet"></a>ZFNet</h3><p>网络结构和 AlexNet 相同，将卷积层1中的感受野大小由11x11改为7x7，步长改为2，同时修改了3，4，5层滤波器的个数。</p>
<h3 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h3><p>加深了 AlexNet 的层数，由8层变为了16，19层</p>
<h3 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h3><ul>
<li>网络包含22个带参数的层，参数量大概是Alexnet的1/12。</li>
<li>使用不同的卷积核来增加特征的多样性</li>
<li>使用辅助分类器来解决模型深度过高导致过拟合</li>
</ul>
<h3 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h3><p>残差学习网络（deep residual learning network）</p>
<p><img src="https://i.loli.net/2020/10/24/w3srFN8clip9Y4S.png" alt="课件"></p>
<p>运用了残差的思想：去掉相同的主体部分，从而突出微小的变化，可以被用来训练非常深的网络。</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>deep-learning-intro</p><p><a href="http://cyx0706.github.io/2020/10/24/deep-learning-intro/">http://cyx0706.github.io/2020/10/24/deep-learning-intro/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Ctwo</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2020-10-24</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2021-02-27</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Deep-Learning/">Deep Learning</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2020/10/25/feint/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">feint</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2020/10/20/python-web-3/"><span class="level-item">Python网络编程——TCP</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "18acb159494240e183af2f4ce3beb225",
            repo: "cyx0706.github.io",
            owner: "cyx0706",
            clientID: "4d458c0d13a2c2157dbd",
            clientSecret: "99a8159ca4a12d236f888be149168b92808a4e29",
            admin: ["cyx0706"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 20,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            language: "zh-CN",
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#深度学习中的数学基础"><span class="level-left"><span class="level-item">1</span><span class="level-item">深度学习中的数学基础</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#矩阵线性变换"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">矩阵线性变换</span></span></a></li><li><a class="level is-mobile" href="#矩阵的秩"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">矩阵的秩</span></span></a></li><li><a class="level is-mobile" href="#低秩近似"><span class="level-left"><span class="level-item">1.3</span><span class="level-item">低秩近似</span></span></a></li><li><a class="level is-mobile" href="#概率函数形式统一"><span class="level-left"><span class="level-item">1.4</span><span class="level-item">概率函数形式统一</span></span></a></li><li><a class="level is-mobile" href="#策略设计"><span class="level-left"><span class="level-item">1.5</span><span class="level-item">策略设计</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#奥卡姆剃刀原理"><span class="level-left"><span class="level-item">1.5.1</span><span class="level-item">奥卡姆剃刀原理</span></span></a></li><li><a class="level is-mobile" href="#过拟合-amp-欠拟合"><span class="level-left"><span class="level-item">1.5.2</span><span class="level-item">过拟合&amp;欠拟合</span></span></a></li></ul></li><li><a class="level is-mobile" href="#损失函数"><span class="level-left"><span class="level-item">1.6</span><span class="level-item">损失函数</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#平方损失"><span class="level-left"><span class="level-item">1.6.1</span><span class="level-item">平方损失</span></span></a></li></ul></li><li><a class="level is-mobile" href="#概率学派-amp-贝叶斯学派"><span class="level-left"><span class="level-item">1.7</span><span class="level-item">概率学派&amp;贝叶斯学派</span></span></a></li></ul></li><li><a class="level is-mobile" href="#深度学习概述"><span class="level-left"><span class="level-item">2</span><span class="level-item">深度学习概述</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Yule-Simpson-悖论"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">Yule-Simpson 悖论</span></span></a></li><li><a class="level is-mobile" href="#卷积神经网络（Convolutional-Neural-Network）"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">卷积神经网络（Convolutional Neural Network）</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#卷积神经网络的应用"><span class="level-left"><span class="level-item">2.2.1</span><span class="level-item">卷积神经网络的应用</span></span></a></li><li><a class="level is-mobile" href="#卷积"><span class="level-left"><span class="level-item">2.2.2</span><span class="level-item">卷积</span></span></a></li><li><a class="level is-mobile" href="#池化（Pooling）"><span class="level-left"><span class="level-item">2.2.3</span><span class="level-item">池化（Pooling）</span></span></a></li><li><a class="level is-mobile" href="#全连接（Fully-Connecting）"><span class="level-left"><span class="level-item">2.2.4</span><span class="level-item">全连接（Fully Connecting）</span></span></a></li></ul></li><li><a class="level is-mobile" href="#各种深度学习的模型概述"><span class="level-left"><span class="level-item">2.3</span><span class="level-item">各种深度学习的模型概述</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#AlexNet"><span class="level-left"><span class="level-item">2.3.1</span><span class="level-item">AlexNet</span></span></a></li><li><a class="level is-mobile" href="#ZFNet"><span class="level-left"><span class="level-item">2.3.2</span><span class="level-item">ZFNet</span></span></a></li><li><a class="level is-mobile" href="#VGG"><span class="level-left"><span class="level-item">2.3.3</span><span class="level-item">VGG</span></span></a></li><li><a class="level is-mobile" href="#GoogleNet"><span class="level-left"><span class="level-item">2.3.4</span><span class="level-item">GoogleNet</span></span></a></li><li><a class="level is-mobile" href="#ResNet"><span class="level-left"><span class="level-item">2.3.5</span><span class="level-item">ResNet</span></span></a></li></ul></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Ctwo&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2021 Ctwo</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" async></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>